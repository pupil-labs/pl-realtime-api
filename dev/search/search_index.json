{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pupil Labs Real-Time Python API Client","text":"<p>This is the documentation for the Pupil Labs Real-Time Python client, which allows you to stream data from Neon and Pupil Invisible devices in real time, as well as remote control them.</p> <p>Head to the Getting Started section to start learning how to use this client!</p> <p>Real-time access to eye tracking data and control over the generating devices is critical for many applications, from gaze-driven interaction tools to tightly timed experiments. Neon and Pupil Invisible devices enable such applications via their Real-Time Network API.</p> <p>The Pupil Labs Real-Time Python client provides a high-level and very easy-to-use interface to this API, allowing developers and researchers to quickly build applications.</p> <p>Check out the Cookbook section for a list of example applications that use this library.</p> <p>Not a programmer?</p> <p>If writing code isn\u2019t your thing and you simply need a tool to monitor and control all your devices in real time, check out Neon Monitor.</p> <p>Not a Python user?</p> <p>We offer a Matlab wrapper, a Unity3D (C#) client, or you can always implement your own client.</p> <p>Using Pupil Core?</p> <p>This package is designed for use with Pupil Invisible and Neon. For Pupil Core, please check out the Pupil Core Network API.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#150","title":"1.5.0","text":"<ul> <li>Auto-start only necessary streams in Simple API</li> <li>Adds error information</li> </ul>"},{"location":"changelog/#140","title":"1.4.0","text":"<ul> <li>Adds eye events (blinks, fixations)</li> </ul>"},{"location":"changelog/#136","title":"1.3.6","text":"<ul> <li>Adds eyelid data</li> </ul>"},{"location":"changelog/#135","title":"1.3.5","text":"<ul> <li>Fixes streaming bug when audio is enabled</li> </ul>"},{"location":"changelog/#134","title":"1.3.4","text":"<ul> <li>Add av/cv issue workaround to example scripts</li> </ul>"},{"location":"changelog/#133","title":"1.3.3","text":"<ul> <li>Use pl-neon-recording for loading calibration data</li> </ul>"},{"location":"changelog/#130","title":"1.3.0","text":"<ul> <li>Add support for templates for Neon Companion app (2.8.25+)<ul> <li>Simple API<ul> <li>pupil_labs.realtime_api.simple.Device.get_template</li> <li>pupil_labs.realtime_api.simple.Device.get_template_data</li> <li>pupil_labs.realtime_api.simple.Device.post_template_data</li> </ul> </li> <li>Async API<ul> <li>pupil_labs.realtime_api.device.Device.get_template</li> <li>pupil_labs.realtime_api.device.Device.get_template_data</li> <li>pupil_labs.realtime_api.device.Device.post_template_data</li> </ul> </li> </ul> </li> <li>Add [simple_template_example][] example</li> <li>Add [async_template_example][] example</li> </ul>"},{"location":"changelog/#121","title":"1.2.1","text":"<ul> <li>Add typing annotations for various gaze data types</li> </ul>"},{"location":"changelog/#120","title":"1.2.0","text":"<ul> <li>Add support for streaming eye state data from Neon Companion app (2.8.8+)<ul> <li>pupil_labs.realtime_api.streaming.gaze.EyestateGazeData</li> </ul> </li> </ul>"},{"location":"changelog/#112","title":"1.1.2","text":"<ul> <li>Add support for streaming eyes video from Neon Companion app<ul> <li>Simple API<ul> <li>pupil_labs.realtime_api.simple.Device.receive_eyes_video_frame</li> <li>pupil_labs.realtime_api.simple.Device.receive_matched_scene_and_eyes_video_frames_and_gaze</li> </ul> </li> <li>Async API<ul> <li>pupil_labs.realtime_api.models.Status.direct_eyes_sensor, providing an     url that can be used with pupil_labs.realtime_api.streaming.video.receive_video_frames</li> </ul> </li> </ul> </li> <li>Add async support for streaming IMU from Neon Companion app<ul> <li>Async API<ul> <li>pupil_labs.realtime_api.streaming.imu.receive_imu_data</li> </ul> </li> </ul> </li> </ul>"},{"location":"changelog/#111","title":"1.1.1","text":"<ul> <li>Use <code>numpy.typing</code> instead of <code>nptyping</code></li> <li>Add [simple_vs_async_api_guide][] guide</li> </ul>"},{"location":"changelog/#110","title":"1.1.0","text":"<ul> <li>Rename <code>pupil_labs.realtime_api.clock_echo</code> to pupil_labs.realtime_api.time_echo     and all corresponding class and function prefixes.</li> <li>Expose Time Echo port via pupil_labs.realtime_api.models.Phone.time_echo_port</li> <li>Add simple API to estimate time offset pupil_labs.realtime_api.simple.Device.estimate_time_offset</li> <li>Add simple and async time offset estimation examples</li> </ul>"},{"location":"changelog/#110a2","title":"1.1.0a2","text":"<ul> <li>Internal feature</li> </ul>"},{"location":"changelog/#110a1","title":"1.1.0a1","text":"<ul> <li>Add <code>pupil_labs.realtime_api.clock_echo</code></li> </ul>"},{"location":"changelog/#101","title":"1.0.1","text":"<ul> <li>Require <code>nptyping&lt;2.0.0</code> to avoid backwards incompatibility</li> <li>Update link to documentation in README</li> </ul>"},{"location":"changelog/#100post1","title":"1.0.0.post1","text":"<ul> <li>Improve front-page documentation</li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>Fixed wrong variable name and added default value - #11</li> </ul>"},{"location":"changelog/#v100rc4","title":"v1.0.0rc4","text":"<ul> <li>Fix examples and documentation</li> <li>Finalize first draft of the [under_the_hood_guide][] guide</li> </ul>"},{"location":"changelog/#v100rc3","title":"v1.0.0rc3","text":"<ul> <li>Fix documentation</li> <li>Revert: Remove pupil_labs.realtime_api.simple.discover_one_device</li> <li>Revert: Add <code>pupil_labs.realtime_api.simple.Network</code></li> </ul>"},{"location":"changelog/#v100rc2","title":"v1.0.0rc2","text":"<ul> <li>Apply pre-commit fixes</li> </ul>"},{"location":"changelog/#v100rc1","title":"v1.0.0rc1","text":"<ul> <li>Split pupil_labs.realtime_api.simple into multiple files</li> <li>Remove <code>pupil_labs.realtime_api.discovery.discover_one_device</code></li> <li>Remove <code>pupil_labs.realtime_api.simple.discover_one_device</code></li> <li>Add <code>pupil_labs.realtime_api.simple.Network</code></li> <li>Add pupil_labs.realtime_api.discovery.Network</li> </ul>"},{"location":"changelog/#v0012","title":"v0.0.12","text":"<ul> <li>Add pupil_labs.realtime_api.models.UnknownComponentError and let     pupil_labs.realtime_api.models.parse_component raise it when a component     could not be parsed/mapped</li> <li>Drop unknown components in pupil_labs.realtime_api.models.Status.from_dict     and pupil_labs.realtime_api.device.Device.status_updates, and warn about it</li> </ul>"},{"location":"changelog/#v0011","title":"v0.0.11","text":"<ul> <li>Add pupil_labs.realtime_api.models.NetworkDevice</li> <li>Create a new HTTP client session if necessary on [pupil_labs.realtime_api.device.Device.aenter][pupil_labs.realtime_api.device.Device.__aenter__] method</li> </ul>"},{"location":"changelog/#v0010","title":"v0.0.10","text":"<ul> <li>Remove <code>pupil_labs.realtime_api.simple.Device.recording_recent_action</code> and <code>pupil_labs.realtime_api.simple.Device.recording_duration_seconds</code></li> <li>Fix Python 3.7 incompatiblity due to using the <code>name</code> argument in asyncio.create_task (added in Python 3.8)</li> </ul>"},{"location":"changelog/#v009","title":"v0.0.9","text":"<ul> <li>Fix Python 3.7 compatibility</li> <li>Add <code>pupil_labs.realtime_api.discovery.discover_one_device</code></li> </ul>"},{"location":"changelog/#v008","title":"v0.0.8","text":"<ul> <li>Rename <code>pupil_labs.realtime_api.basic</code> to pupil_labs.realtime_api.simple</li> <li>Rename <code>pupil_labs.realtime_api.basic.Device.read_*()</code> methods to <code>Device.receive_*()</code></li> <li>Rename <code>pupil_labs.realtime_api.simple.discovered_devices</code> to pupil_labs.realtime_api.simple.discover_devices</li> <li>Add [pupil_labs.realtime_api.device.Device.status_updates()][] generator</li> <li>Move status update callback functionality into pupil_labs.realtime_api.device.StatusUpdateNotifier</li> <li>Add [simple_auto_update_example][] example</li> <li>Add <code>pupil_labs.realtime_api.simple.Device.recording_recent_action</code> and <code>pupil_labs.realtime_api.simple.Device.recording_duration_seconds</code></li> <li>Add streaming control functionality to pupil_labs.realtime_api.simple.Device<ul> <li>pupil_labs.realtime_api.simple.Device.streaming_start</li> <li>pupil_labs.realtime_api.simple.Device.streaming_stop</li> <li>pupil_labs.realtime_api.simple.Device.is_currently_streaming</li> </ul> </li> <li>Fix examples</li> </ul>"},{"location":"changelog/#v007","title":"v0.0.7","text":"<ul> <li>Fix Python 3.7 and 3.8 compatibility</li> </ul>"},{"location":"changelog/#v006","title":"v0.0.6","text":"<ul> <li>Add pupil_labs.realtime_api.simple.Device.receive_matched_scene_video_frame_and_gaze</li> <li>Add simple [stream_video_with_overlayed_gaze_example_simple][] example</li> </ul>"},{"location":"changelog/#v005","title":"v0.0.5","text":"<ul> <li>Add guides to documentation</li> <li>Add [stream_video_with_overlayed_gaze_example][] example</li> <li>Add pupil_labs.realtime_api.simple API. See the [simple_examples][].</li> <li>Rename <code>pupil_labs.realtime_api.control</code> to pupil_labs.realtime_api.device.</li> <li>Rename <code>pupil_labs.realtime_api.base.ControlBase</code> to pupil_labs.realtime_api.base.DeviceBase.</li> <li>Rename <code>pupil_labs.realtime_api.simple.Control</code> to pupil_labs.realtime_api.simple.Device.</li> <li>Rename <code>pupil_labs.realtime_api.control.Control</code> to pupil_labs.realtime_api.device.Device.</li> <li>Rename <code>pupil_labs.realtime_api.models.DiscoveredDevice</code> to pupil_labs.realtime_api.models.DiscoveredDeviceInfo.</li> <li>Add sensor property accessors to pupil_labs.realtime_api.simple.Device.</li> <li>Add simple streaming with pupil_labs.realtime_api.simple.Device.receive_scene_video_frame     and pupil_labs.realtime_api.simple.Device.receive_gaze_datum.</li> </ul>"},{"location":"changelog/#v004","title":"v0.0.4","text":"<ul> <li>Include examples in documentation</li> <li>Implement pupil_labs.realtime_api.models.Recording model class</li> <li>Add pupil_labs.realtime_api.models.Status.recording attribute</li> </ul>"},{"location":"changelog/#v003","title":"v0.0.3","text":"<ul> <li>Move Control.Error to dedicated pupil_labs.realtime_api.device.DeviceError class</li> <li>Implement pupil_labs.realtime_api.streaming.gaze and     pupil_labs.realtime_api.streaming.video streaming</li> </ul>"},{"location":"changelog/#v002","title":"v0.0.2","text":"<ul> <li>Require <code>aiohttp[speedups]</code></li> <li>Implement pupil_labs.realtime_api.discovery.discover_devices</li> <li>Implement pupil_labs.realtime_api.device.Device</li> </ul>"},{"location":"contributing/","title":"Developer","text":""},{"location":"getting-started/","title":"Getting Started","text":"<p>The Real-time Python Client comes in two flavours, simple and async. For most applications the <code>simple</code> interface is appropriate and we recommend it for new users. The <code>async</code> interface uses Python's asyncio features to enable non-blocking communication, which can be beneficial for applications with very strict latency requirements. It is more difficult to use though and for most users the <code>simple</code> interface will suffice. To learn more, check out the Simple vs Async guide.</p> <p>In the examples below, we will use the <code>simple</code> interface. We are assuming a Neon device is used, but the same code works for Pupil Invisible devices analoguously as well.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>The package is available on PyPI and can be installed using pip:</p> <pre><code>pip install pupil-labs-realtime-api\n</code></pre> <p>Python Compatibility</p> <p>This package requires Python 3.10 or higher. For compatibility with Python 3.9 you can consider user older versions of this package <code>&lt;1.6</code>.</p> <pre><code>pip install pupil-labs-realtime-api&lt;1.6\n</code></pre>"},{"location":"getting-started/#connecting-to-a-device-receiving-data","title":"Connecting to a Device &amp; Receiving Data","text":"<p>Using the <code>discover_one_device</code> function to connect to a Neon device in your local network. Make sure the Neon Companion app is running! If no device can be found, please check the troubleshooting section.</p> <p>Using the <code>receive_matched_scene_video_frame_and_gaze</code> method, you can receive the current scene camera frame and gaze sample. This method ensures that both samples are matched temporally.</p> <p>The following example uses these methods to visualize a real-time gaze overlay on the scene camera frame:</p> <pre><code>from pupil_labs.realtime_api.simple import discover_one_device\nimport cv2\n\ndevice = discover_one_device()\nprint(f\"Successfully connected to device with serial {device.serial_number_glasses}\")\n\nwhile True:\n    frame, gaze = device.receive_matched_scene_video_frame_and_gaze()\n    cv2.circle(\n        frame.bgr_pixels,\n        (int(gaze.x), int(gaze.y)),\n        radius=80,\n        color=(0, 0, 255),\n        thickness=15,\n    )\n\n    cv2.imshow(\"Scene camera with gaze overlay\", frame.bgr_pixels)\n    if cv2.waitKey(1) &amp; 0xFF == 27:\n        break\n</code></pre>"},{"location":"getting-started/#more-data-remote-control","title":"More Data &amp; Remote Control","text":"<p>Many more data streams are available in real-time, including fixations, blinks, pupil diameter, IMU data and more. You can also control the device remotely, for example to start or stop a recording. Lastly, you can also save events as part of running recordings, to automatically annotate your data via the API.</p> <p>All of these features are demonstrated and explained in more detail in the Simple API section. More involved example applications can be found in the Cookbook. All methods are documented in the API reference and Code Examples.</p>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 Pupil Labs GmbH\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"modules/","title":"Index","text":""},{"location":"modules/#pupil_labs.realtime_api","title":"realtime_api","text":"<p>pupil_labs.realtime_api package.</p> <p>Python Client for the Pupil Labs Real-Time API</p> <p>Modules:</p> <ul> <li> <code>base</code>           \u2013            </li> <li> <code>device</code>           \u2013            </li> <li> <code>discovery</code>           \u2013            </li> <li> <code>models</code>           \u2013            </li> <li> <code>simple</code>           \u2013            </li> <li> <code>streaming</code>           \u2013            </li> <li> <code>time_echo</code>           \u2013            <p>Manual time offset estimation via the Pupil Labs Time Echo protocol.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>APIPath</code>           \u2013            <p>API endpoint paths for the Realtime API.</p> </li> <li> <code>BlinkEventData</code>           \u2013            <p>Data for a blink event.</p> </li> <li> <code>Device</code>           \u2013            <p>Class representing a Pupil Labs device.</p> </li> <li> <code>DeviceError</code>           \u2013            <p>Exception raised when a device operation fails.</p> </li> <li> <code>DualMonocularGazeData</code>           \u2013            <p>Experimental class for dual monocular gaze data.</p> </li> <li> <code>EyestateGazeData</code>           \u2013            <p>Gaze data with additional eye state information.</p> </li> <li> <code>FixationEventData</code>           \u2013            <p>Data for a fixation or saccade event.</p> </li> <li> <code>FixationOnsetEventData</code>           \u2013            <p>Data for a fixation or saccade onset event.</p> </li> <li> <code>GazeData</code>           \u2013            <p>Basic gaze data with position, timestamp and indicator of glasses worn status.</p> </li> <li> <code>Network</code>           \u2013            <p>Network discovery client for finding devices.</p> </li> <li> <code>RTSPData</code>           \u2013            <p>Container for RTSP data with timestamp information.</p> </li> <li> <code>RTSPEyeEventStreamer</code>           \u2013            <p>Stream and parse eye events from an RTSP source.</p> </li> <li> <code>RTSPGazeStreamer</code>           \u2013            <p>Stream and parse gaze data from an RTSP source.</p> </li> <li> <code>RTSPImuStreamer</code>           \u2013            <p>Stream and parse IMU data from an RTSP source.</p> </li> <li> <code>RTSPRawStreamer</code>           \u2013            <p>Stream raw data from an RTSP source.</p> </li> <li> <code>RTSPVideoFrameStreamer</code>           \u2013            <p>Stream and decode video frames from an RTSP source.</p> </li> <li> <code>StatusUpdateNotifier</code>           \u2013            <p>Helper class for handling device status update callbacks.</p> </li> <li> <code>VideoFrame</code>           \u2013            <p>A video frame with timestamp information.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>discover_devices</code>             \u2013              <p>Use Bonjour to find devices in the local network that serve the Realtime API.</p> </li> <li> <code>receive_eye_events_data</code>             \u2013              <p>Receive eye events data from an RTSP stream.</p> </li> <li> <code>receive_gaze_data</code>             \u2013              <p>Receive gaze data from an RTSP stream.</p> </li> <li> <code>receive_imu_data</code>             \u2013              <p>Receive IMU data from a given RTSP URL.</p> </li> <li> <code>receive_raw_rtsp_data</code>             \u2013              <p>Receive raw data from an RTSP stream.</p> </li> <li> <code>receive_video_frames</code>             \u2013              <p>Receive video frames from an RTSP stream.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.APIPath","title":"APIPath","text":"<p>               Bases: <code>Enum</code></p> <p>API endpoint paths for the Realtime API.</p> <p>This enum defines the various API endpoints that can be accessed through the Realtime API.</p> <p>Methods:</p> <ul> <li> <code>full_address</code>             \u2013              <p>Construct a full URL for this API endpoint.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.APIPath.full_address","title":"full_address","text":"<pre><code>full_address(address: str, port: int, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full URL for this API endpoint.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def full_address(\n    self, address: str, port: int, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full URL for this API endpoint.\"\"\"\n    return f\"{protocol}://{address}:{port}\" + prefix + self.value\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData","title":"BlinkEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a blink event.</p> <p>Represents a detected blink event with timing information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a BlinkEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the blink in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (4 -&gt; blink events).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the blink in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the blink in nanoseconds.</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (4 -&gt; blink events).</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the blink in nanoseconds.</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.BlinkEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; BlinkEventData\n</code></pre> <p>Create a BlinkEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"BlinkEventData\":\n    \"\"\"Create a BlinkEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n    ) = struct.unpack(\"!iqq\", data.raw)\n    return cls(event_type, start_time_ns, end_time_ns, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device","title":"Device","text":"<pre><code>Device(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>DeviceBase</code></p> <p>Class representing a Pupil Labs device.</p> <p>This class provides methods to interact with the device, such as starting and stopping recordings, sending events, and fetching device status. It also provides a context manager for automatically closing the device session.</p> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>close</code>             \u2013              <p>Close the connection to the device.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> <li> <code>get_calibration</code>             \u2013              <p>Get the current cameras calibration data.</p> </li> <li> <code>get_status</code>             \u2013              <p>Get the current status of the device.</p> </li> <li> <code>get_template</code>             \u2013              <p>Get the template currently selected on device.</p> </li> <li> <code>get_template_data</code>             \u2013              <p>Get the template data entered on device.</p> </li> <li> <code>post_template_data</code>             \u2013              <p>Set the data for the currently selected template.</p> </li> <li> <code>recording_cancel</code>             \u2013              <p>Cancel the current recording without saving it.</p> </li> <li> <code>recording_start</code>             \u2013              <p>Start a recording on the device.</p> </li> <li> <code>recording_stop_and_save</code>             \u2013              <p>Stop and save the current recording.</p> </li> <li> <code>send_event</code>             \u2013              <p>Send an event to the device.</p> </li> <li> <code>status_updates</code>             \u2013              <p>Stream status updates from the device.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>active_session</code>               (<code>ClientSession</code>)           \u2013            <p>Returns the active session, raising an error if it's None.</p> </li> <li> <code>session</code>               (<code>ClientSession | None</code>)           \u2013            <p>The HTTP session used for making requests.</p> </li> <li> <code>template_definition</code>               (<code>Template | None</code>)           \u2013            <p>The template definition currently selected on the device.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the Device class.\"\"\"\n    super().__init__(*args, **kwargs)\n    self._create_client_session()\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.active_session","title":"active_session  <code>property</code>","text":"<pre><code>active_session: ClientSession\n</code></pre> <p>Returns the active session, raising an error if it's None.</p>"},{"location":"modules/#pupil_labs.realtime_api.Device.session","title":"session  <code>instance-attribute</code>","text":"<pre><code>session: ClientSession | None\n</code></pre> <p>The HTTP session used for making requests.</p>"},{"location":"modules/#pupil_labs.realtime_api.Device.template_definition","title":"template_definition  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>template_definition: Template | None = None\n</code></pre> <p>The template definition currently selected on the device.</p>"},{"location":"modules/#pupil_labs.realtime_api.Device.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection to the device.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the connection to the device.\"\"\"\n    await self.active_session.close()\n    self.session = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.get_calibration","title":"get_calibration  <code>async</code>","text":"<pre><code>get_calibration() -&gt; Calibration\n</code></pre> <p>Get the current cameras calibration data.</p> <p>Note that Pupil Invisible and Neon are calibration free systems, this refers to the intrinsincs and extrinsics of the cameras and is only available for Neon.</p> <p>Returns:</p> <ul> <li> <code>Calibration</code>           \u2013            <p>pupil_labs.neon_recording.calib.Calibration: The calibration data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_calibration(self) -&gt; Calibration:\n    \"\"\"Get the current cameras calibration data.\n\n    Note that Pupil Invisible and Neon are calibration free systems, this refers to\n    the intrinsincs and extrinsics of the cameras and is only available for Neon.\n\n    Returns:\n        pupil_labs.neon_recording.calib.Calibration: The calibration data.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.CALIBRATION)\n    ) as response:\n        if response.status != 200:\n            raise DeviceError(response.status, \"Failed to fetch calibration\")\n\n        raw_data = await response.read()\n        return cast(Calibration, Calibration.from_buffer(raw_data))\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status() -&gt; Status\n</code></pre> <p>Get the current status of the device.</p> <p>Returns:</p> <ul> <li> <code>Status</code> (              <code>Status</code> )          \u2013            <p>The current device status.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_status(self) -&gt; Status:\n    \"\"\"Get the current status of the device.\n\n    Returns:\n        Status: The current device status.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(self.api_url(APIPath.STATUS)) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_status] Received status: {result}\")\n        return Status.from_dict(result)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.get_template","title":"get_template  <code>async</code>","text":"<pre><code>get_template() -&gt; Template\n</code></pre> <p>Get the template currently selected on device.</p> <p>Returns:</p> <ul> <li> <code>Template</code> (              <code>Template</code> )          \u2013            <p>The currently selected template.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template can't be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template(self) -&gt; Template:\n    \"\"\"Get the template currently selected on device.\n\n    Returns:\n        Template: The currently selected template.\n\n    Raises:\n        DeviceError: If the template can't be fetched.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DEFINITION)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_def] Received template def: {result}\")\n        self.template_definition = Template(**result)\n        return self.template_definition\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.get_template_data","title":"get_template_data  <code>async</code>","text":"<pre><code>get_template_data(template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Get the template data entered on device.</p> <p>Parameters:</p> <ul> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the returned data. - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]} - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The template data in the requested format.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template's data could not be fetched.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template_data(\n    self, template_format: TemplateDataFormat = \"simple\"\n) -&gt; Any:\n    \"\"\"Get the template data entered on device.\n\n    Args:\n        template_format (TemplateDataFormat): Format of the returned data.\n            - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]}\n            - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The template data in the requested format.\n\n    Raises:\n        DeviceError: If the template's data could not be fetched.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DATA)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(\n            f\"[{self}.get_template_data] Received data's template: {result}\"\n        )\n        if template_format == \"api\":\n            return result\n        elif template_format == \"simple\":\n            template = await self.get_template()\n            return template.convert_from_api_to_simple_format(result)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.post_template_data","title":"post_template_data  <code>async</code>","text":"<pre><code>post_template_data(template_answers: dict[str, list[str]], template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Set the data for the currently selected template.</p> <p>Parameters:</p> <ul> <li> <code>template_answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>The template data to send.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the input data. - \"api\" accepts the data as in realtime api format e.g.,     {\"item_uuid\": [\"42\"]} - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result of the operation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the data can not be sent.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid data type.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def post_template_data(\n    self,\n    template_answers: dict[str, list[str]],\n    template_format: TemplateDataFormat = \"simple\",\n) -&gt; Any:\n    \"\"\"Set the data for the currently selected template.\n\n    Args:\n        template_answers: The template data to send.\n        template_format (TemplateDataFormat): Format of the input data.\n            - \"api\" accepts the data as in realtime api format e.g.,\n                {\"item_uuid\": [\"42\"]}\n            - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result of the operation.\n\n    Raises:\n        DeviceError: If the data can not be sent.\n        ValueError: If invalid data type.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    self.template_definition = await self.get_template()\n\n    if template_format == \"simple\":\n        template_answers = (\n            self.template_definition.convert_from_simple_to_api_format(\n                template_answers\n            )\n        )\n\n    pre_populated_data = await self.get_template_data(template_format=\"api\")\n    errors = self.template_definition.validate_answers(\n        pre_populated_data | template_answers, template_format=\"api\"\n    )\n    if errors:\n        raise ValueError(errors)\n\n    # workaround for issue with api as it fails when passing in an empty list\n    # ie. it wants [\"\"] instead of []\n    template_answers = {\n        key: value or [\"\"] for key, value in template_answers.items()\n    }\n\n    async with self.active_session.post(\n        self.api_url(APIPath.TEMPLATE_DATA), json=template_answers\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_data] Send data's template: {result}\")\n        return result\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.recording_cancel","title":"recording_cancel  <code>async</code>","text":"<pre><code>recording_cancel() -&gt; None\n</code></pre> <p>Cancel the current recording without saving it.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be cancelled. Possible reasons include: - Recording not running</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_cancel(self) -&gt; None:\n    \"\"\"Cancel the current recording without saving it.\n\n    Raises:\n        DeviceError: If the recording could not be cancelled.\n            Possible reasons include:\n            - Recording not running\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_CANCEL)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.recording_start","title":"recording_start  <code>async</code>","text":"<pre><code>recording_start() -&gt; str\n</code></pre> <p>Start a recording on the device.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>ID of the started recording.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be started. Possible reasons include: - Recording already running - Template has required fields - Low battery - Low storage - No wearer selected - No workspace selected - Setup bottom sheets not completed</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_start(self) -&gt; str:\n    \"\"\"Start a recording on the device.\n\n    Returns:\n        str: ID of the started recording.\n\n    Raises:\n        DeviceError: If recording could not be started. Possible reasons include:\n            - Recording already running\n            - Template has required fields\n            - Low battery\n            - Low storage\n            - No wearer selected\n            - No workspace selected\n            - Setup bottom sheets not completed\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_START)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.start_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        return cast(str, confirmation[\"result\"][\"id\"])\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.recording_stop_and_save","title":"recording_stop_and_save  <code>async</code>","text":"<pre><code>recording_stop_and_save() -&gt; None\n</code></pre> <p>Stop and save the current recording.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be stopped. Possible reasons include: - Recording not running - Template has required fields</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_stop_and_save(self) -&gt; None:\n    \"\"\"Stop and save the current recording.\n\n    Raises:\n        DeviceError: If recording could not be stopped. Possible reasons include:\n            - Recording not running\n            - Template has required fields\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_STOP_AND_SAVE)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.send_event","title":"send_event  <code>async</code>","text":"<pre><code>send_event(event_name: str, event_timestamp_unix_ns: int | None = None) -&gt; Event\n</code></pre> <p>Send an event to the device.</p> <p>Parameters:</p> <ul> <li> <code>event_name</code>               (<code>str</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>event_timestamp_unix_ns</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional timestamp in unix nanoseconds. If None, the current time will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>The created event.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If sending the event fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def send_event(\n    self, event_name: str, event_timestamp_unix_ns: int | None = None\n) -&gt; Event:\n    \"\"\"Send an event to the device.\n\n    Args:\n        event_name: Name of the event.\n        event_timestamp_unix_ns: Optional timestamp in unix nanoseconds.\n            If None, the current time will be used.\n\n    Returns:\n        Event: The created event.\n\n    Raises:\n        DeviceError: If sending the event fails.\n\n    \"\"\"\n    event: dict[str, Any] = {\"name\": event_name}\n    if event_timestamp_unix_ns is not None:\n        event[\"timestamp\"] = event_timestamp_unix_ns\n\n    async with self.active_session.post(\n        self.api_url(APIPath.EVENT), json=event\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.send_event] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        confirmation[\"result\"][\"name\"] = (\n            event_name  # As the API does not return the name yet\n        )\n        return Event.from_dict(confirmation[\"result\"])\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Device.status_updates","title":"status_updates  <code>async</code>","text":"<pre><code>status_updates() -&gt; AsyncIterator[Component]\n</code></pre> <p>Stream status updates from the device.</p> <p>Yields:</p> <ul> <li> <code>Component</code> (              <code>AsyncIterator[Component]</code> )          \u2013            <p>Status update components as they arrive.</p> </li> </ul> <p>Auto-reconnect, see:     https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def status_updates(self) -&gt; AsyncIterator[Component]:\n    \"\"\"Stream status updates from the device.\n\n    Yields:\n        Component: Status update components as they arrive.\n\n    Auto-reconnect, see:\n        https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect\n\n    \"\"\"\n    websocket_status_endpoint = self.api_url(APIPath.STATUS, protocol=\"ws\")\n    async for websocket in websockets.connect(websocket_status_endpoint):\n        try:\n            async for message_raw in websocket:\n                message_json = json.loads(message_raw)\n                try:\n                    component = parse_component(message_json)\n                except UnknownComponentError:\n                    logger.warning(f\"Dropping unknown component: {component}\")\n                    continue\n                yield component\n        except websockets.ConnectionClosed:\n            logger.debug(\"Websocket connection closed. Reconnecting...\")\n            continue\n        except asyncio.CancelledError:\n            logger.debug(\"status_updates() cancelled\")\n            break\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.DeviceError","title":"DeviceError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a device operation fails.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData","title":"DualMonocularGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Experimental class for dual monocular gaze data.</p> <p>Contains separate gaze points for left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a DualMonocularGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>left</code>               (<code>Point</code>)           \u2013            <p>Gaze point for the left eye.</p> </li> <li> <code>right</code>               (<code>Point</code>)           \u2013            <p>Gaze point for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.left","title":"left  <code>instance-attribute</code>","text":"<pre><code>left: Point\n</code></pre> <p>Gaze point for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.right","title":"right  <code>instance-attribute</code>","text":"<pre><code>right: Point\n</code></pre> <p>Gaze point for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"modules/#pupil_labs.realtime_api.DualMonocularGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; DualMonocularGazeData\n</code></pre> <p>Create a DualMonocularGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DualMonocularGazeData</code> (              <code>DualMonocularGazeData</code> )          \u2013            <p>An instance of DualMonocularGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"DualMonocularGazeData\":\n    \"\"\"Create a DualMonocularGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        DualMonocularGazeData: An instance of DualMonocularGazeData with the parsed\n            values.\n\n    \"\"\"\n    x1, y1, worn, x2, y2 = struct.unpack(\"!ffBff\", data.raw)\n    return cls(\n        Point(x1, y1), Point(x2, y2), worn == 255, data.timestamp_unix_seconds\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData","title":"EyestateGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Gaze data with additional eye state information.</p> <p>Contains gaze point, pupil diameter, eyeball center coordinates, and optical axis coordinates for both left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create an EyestateGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>eyeball_center_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the right eye.</p> </li> <li> <code>optical_axis_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the right eye.</p> </li> <li> <code>pupil_diameter_left</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the left eye.</p> </li> <li> <code>pupil_diameter_right</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the gaze point.</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_left_x","title":"eyeball_center_left_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_x: float\n</code></pre> <p>X coordinate of the eyeball center for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_left_y","title":"eyeball_center_left_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_left_z","title":"eyeball_center_left_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_right_x","title":"eyeball_center_right_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_x: float\n</code></pre> <p>X coordinate of the eyeball center for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_right_y","title":"eyeball_center_right_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.eyeball_center_right_z","title":"eyeball_center_right_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_left_x","title":"optical_axis_left_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_x: float\n</code></pre> <p>X coordinate of the optical axis for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_left_y","title":"optical_axis_left_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_y: float\n</code></pre> <p>Y coordinate of the optical axis for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_left_z","title":"optical_axis_left_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_z: float\n</code></pre> <p>Z coordinate of the optical axis for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_right_x","title":"optical_axis_right_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_x: float\n</code></pre> <p>X coordinate of the optical axis for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_right_y","title":"optical_axis_right_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_y: float\n</code></pre> <p>Y coordinate of the optical axis for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.optical_axis_right_z","title":"optical_axis_right_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_z: float\n</code></pre> <p>Z coordinate of the optical axis for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.pupil_diameter_left","title":"pupil_diameter_left  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left: float\n</code></pre> <p>Pupil diameter for the left eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.pupil_diameter_right","title":"pupil_diameter_right  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right: float\n</code></pre> <p>Pupil diameter for the right eye.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>X coordinate of the gaze point.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"modules/#pupil_labs.realtime_api.EyestateGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; EyestateGazeData\n</code></pre> <p>Create an EyestateGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EyestateGazeData</code> (              <code>EyestateGazeData</code> )          \u2013            <p>An instance of EyestateGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"EyestateGazeData\":\n    \"\"\"Create an EyestateGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        EyestateGazeData: An instance of EyestateGazeData with the parsed values.\n\n    \"\"\"\n    (\n        x,\n        y,\n        worn,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n    ) = struct.unpack(\"!ffBffffffffffffff\", data.raw)\n    return cls(\n        x,\n        y,\n        worn == 255,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData","title":"FixationEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade event.</p> <p>Represents a completed fixation or saccade event with detailed information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a FixationEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>amplitude_angle_deg</code>               (<code>float</code>)           \u2013            <p>Amplitude in degrees.</p> </li> <li> <code>amplitude_pixels</code>               (<code>float</code>)           \u2013            <p>Amplitude in pixels.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_gaze_x</code>               (<code>float</code>)           \u2013            <p>End gaze x-coordinate in pixels.</p> </li> <li> <code>end_gaze_y</code>               (<code>float</code>)           \u2013            <p>End gaze y-coordinate in pixels.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the event in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (0 for saccade, 1 for fixation).</p> </li> <li> <code>max_velocity</code>               (<code>float</code>)           \u2013            <p>Maximum velocity in pixels per degree.</p> </li> <li> <code>mean_gaze_x</code>               (<code>float</code>)           \u2013            <p>Mean gaze x-coordinate in pixels.</p> </li> <li> <code>mean_gaze_y</code>               (<code>float</code>)           \u2013            <p>Mean gaze y-coordinate in pixels.</p> </li> <li> <code>mean_velocity</code>               (<code>float</code>)           \u2013            <p>Mean velocity in pixels per degree.</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_gaze_x</code>               (<code>float</code>)           \u2013            <p>Start gaze x-coordinate in pixels.</p> </li> <li> <code>start_gaze_y</code>               (<code>float</code>)           \u2013            <p>Start gaze y-coordinate in pixels.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.amplitude_angle_deg","title":"amplitude_angle_deg  <code>instance-attribute</code>","text":"<pre><code>amplitude_angle_deg: float\n</code></pre> <p>Amplitude in degrees.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.amplitude_pixels","title":"amplitude_pixels  <code>instance-attribute</code>","text":"<pre><code>amplitude_pixels: float\n</code></pre> <p>Amplitude in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.end_gaze_x","title":"end_gaze_x  <code>instance-attribute</code>","text":"<pre><code>end_gaze_x: float\n</code></pre> <p>End gaze x-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.end_gaze_y","title":"end_gaze_y  <code>instance-attribute</code>","text":"<pre><code>end_gaze_y: float\n</code></pre> <p>End gaze y-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the event in nanoseconds.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (0 for saccade, 1 for fixation).</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.max_velocity","title":"max_velocity  <code>instance-attribute</code>","text":"<pre><code>max_velocity: float\n</code></pre> <p>Maximum velocity in pixels per degree.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.mean_gaze_x","title":"mean_gaze_x  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_x: float\n</code></pre> <p>Mean gaze x-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.mean_gaze_y","title":"mean_gaze_y  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_y: float\n</code></pre> <p>Mean gaze y-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.mean_velocity","title":"mean_velocity  <code>instance-attribute</code>","text":"<pre><code>mean_velocity: float\n</code></pre> <p>Mean velocity in pixels per degree.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.start_gaze_x","title":"start_gaze_x  <code>instance-attribute</code>","text":"<pre><code>start_gaze_x: float\n</code></pre> <p>Start gaze x-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.start_gaze_y","title":"start_gaze_y  <code>instance-attribute</code>","text":"<pre><code>start_gaze_y: float\n</code></pre> <p>Start gaze y-coordinate in pixels.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; FixationEventData\n</code></pre> <p>Create a FixationEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"FixationEventData\":\n    \"\"\"Create a FixationEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n    ) = struct.unpack(\"!iqqffffffffff\", data.raw)\n    return cls(\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData","title":"FixationOnsetEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade onset event.</p> <p>Represents the beginning of a fixation or saccade event.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (2 for saccade onset, 3 for fixation onset).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (2 for saccade onset, 3 for fixation onset).</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"modules/#pupil_labs.realtime_api.FixationOnsetEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData","title":"GazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Basic gaze data with position, timestamp and indicator of glasses worn status.</p> <p>Represents the 2D gaze point on the scene camera coordinates with a timestamp in nanoseconds unix epoch and an indicator of whether the glasses are being worn.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a GazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>\"X coordinate of the gaze point</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>\"X coordinate of the gaze point</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"modules/#pupil_labs.realtime_api.GazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; GazeData\n</code></pre> <p>Create a GazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GazeData</code> (              <code>GazeData</code> )          \u2013            <p>An instance of GazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"GazeData\":\n    \"\"\"Create a GazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        GazeData: An instance of GazeData with the parsed values.\n\n    \"\"\"\n    x, y, worn = struct.unpack(\"!ffB\", data.raw)\n    return cls(x, y, worn == 255, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Network","title":"Network","text":"<pre><code>Network()\n</code></pre> <p>Network discovery client for finding devices.</p> <p>This class manages device discovery on the local network using Zeroconf/Bonjour. It maintains a list of discovered devices and provides methods to access them.</p> <p>Attributes:</p> <ul> <li> <code>_devices</code>               (<code>dict | None</code>)           \u2013            <p>A dictionary of discovered devices, where the keys are device names and the values are DiscoveredDeviceInfo objects.</p> </li> <li> <code>_new_devices</code>               (<code>Queue</code>)           \u2013            <p>A queue to hold newly discovered devices.</p> </li> <li> <code>_aiozeroconf</code>               (<code>AsyncZeroconf | None</code>)           \u2013            <p>An instance of AsyncZeroconf for network discovery.</p> </li> <li> <code>_aiobrowser</code>               (<code>AsyncServiceBrowser | None</code>)           \u2013            <p>An instance of AsyncServiceBrowser for browsing services on the network.</p> </li> <li> <code>_open</code>               (<code>bool</code>)           \u2013            <p>A flag indicating whether the network discovery client is open.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>close</code>             \u2013              <p>Close all network resources.</p> </li> <li> <code>wait_for_new_device</code>             \u2013              <p>Wait for a new device to be discovered.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._devices: dict | None = {}\n    self._new_devices: asyncio.Queue[DiscoveredDeviceInfo] = asyncio.Queue()\n    self._aiozeroconf: AsyncZeroconf | None = AsyncZeroconf()\n    self._aiobrowser: AsyncServiceBrowser | None = AsyncServiceBrowser(\n        self._aiozeroconf.zeroconf,\n        \"_http._tcp.local.\",\n        handlers=[self._handle_service_change],\n    )\n    self._open: bool = True\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Network.devices","title":"devices  <code>property</code>","text":"<pre><code>devices: tuple[DiscoveredDeviceInfo, ...]\n</code></pre> <p>Return a tuple of discovered devices.</p>"},{"location":"modules/#pupil_labs.realtime_api.Network.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close all network resources.</p> <p>This method stops the Zeroconf browser, closes connections, and clears the device list.</p> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close all network resources.\n\n    This method stops the Zeroconf browser, closes connections, and clears\n    the device list.\n    \"\"\"\n    if self._open:\n        await self._aiobrowser.async_cancel() if self._aiobrowser else None\n        await self._aiozeroconf.async_close() if self._aiozeroconf else None\n        if self._devices:\n            self._devices.clear()\n            self._devices = None\n        while not self._new_devices.empty():\n            self._new_devices.get_nowait()\n        self._aiobrowser = None\n        self._aiozeroconf = None\n        self._open = False\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.Network.wait_for_new_device","title":"wait_for_new_device  <code>async</code>","text":"<pre><code>wait_for_new_device(timeout_seconds: float | None = None) -&gt; DiscoveredDeviceInfo | None\n</code></pre> <p>Wait for a new device to be discovered.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new device. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DiscoveredDeviceInfo | None</code>           \u2013            <p>Optional[DiscoveredDeviceInfo]: The newly discovered device, or None if the timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def wait_for_new_device(\n    self, timeout_seconds: float | None = None\n) -&gt; DiscoveredDeviceInfo | None:\n    \"\"\"Wait for a new device to be discovered.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new device.\n            If None, wait indefinitely.\n\n    Returns:\n        Optional[DiscoveredDeviceInfo]: The newly discovered device,\n            or None if the timeout was reached.\n\n    \"\"\"\n    try:\n        return await asyncio.wait_for(self._new_devices.get(), timeout_seconds)\n    except asyncio.TimeoutError:\n        return None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPData","title":"RTSPData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for RTSP data with timestamp information.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>raw</code>               (<code>ByteString</code>)           \u2013            <p>Raw binary data received from the RTSP stream.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since the Unix epoch from RTCP SR packets.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPData.raw","title":"raw  <code>instance-attribute</code>","text":"<pre><code>raw: ByteString\n</code></pre> <p>Raw binary data received from the RTSP stream.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since the Unix epoch from RTCP SR packets.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPEyeEventStreamer","title":"RTSPEyeEventStreamer","text":"<pre><code>RTSPEyeEventStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse eye events from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured eye event data objects.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse eye events from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPEyeEventStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPEyeEventStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPEyeEventStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]\n</code></pre> <p>Receive and parse eye events from the RTSP stream.</p> <p>Yields:</p> <ul> <li> <code>AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]</code>           \u2013            <p>FixationEventData | FixationOnsetEventData | BlinkEventData: Parsed eye event data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the event type is not recognized.</p> </li> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the event data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]:\n    \"\"\"Receive and parse eye events from the RTSP stream.\n\n    Yields:\n        FixationEventData | FixationOnsetEventData | BlinkEventData: Parsed eye\n            event data.\n\n    Raises:\n        KeyError: If the event type is not recognized.\n        Exception: If there is an error parsing the event data.\n\n    \"\"\"\n    data_class_by_type = {\n        0: FixationEventData,\n        1: FixationEventData,\n        2: FixationOnsetEventData,\n        3: FixationOnsetEventData,\n        4: BlinkEventData,\n        5: None,  # KEEPALIVE MSG, SKIP\n    }\n    async for data in super().receive():\n        try:\n            event_type = struct.unpack_from(\"!i\", data.raw)[0]\n            cls = data_class_by_type[event_type]\n            if cls is not None:\n                yield cls.from_raw(data)  # type: ignore[attr-defined]\n        except KeyError:\n            logger.exception(f\"Raw eye event data has unexpected type: {data}\")\n            raise\n        except Exception:\n            logger.exception(f\"Unable to parse eye event data {data}\")\n            raise\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPGazeStreamer","title":"RTSPGazeStreamer","text":"<pre><code>RTSPGazeStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse gaze data from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured gaze data objects. The specific type of gaze data is determined by the length of the raw data packet.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse gaze data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPGazeStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPGazeStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPGazeStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[GazeDataType]\n</code></pre> <p>Receive and parse gaze data from the RTSP stream.</p> <p>Yields:</p> <ul> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>GazeDataType</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>Parsed gaze data of various types. The type of gaze data object is</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>determined by the length of the raw data packet:</p> </li> </ul> <ul> <li>9 bytes: GazeData (basic gaze position)</li> <li>17 bytes: DualMonocularGazeData (left and right eye positions)</li> <li>65 bytes: EyestateGazeData (gaze with eye state)</li> <li>89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)</li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the data length does not match any known format.</p> </li> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the gaze data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[GazeDataType]:\n    \"\"\"Receive and parse gaze data from the RTSP stream.\n\n    Yields:\n        GazeDataType\n\n        Parsed gaze data of various types. The type of gaze data object is\n        determined by the length of the raw data packet:\n    - 9 bytes: GazeData (basic gaze position)\n    - 17 bytes: DualMonocularGazeData (left and right eye positions)\n    - 65 bytes: EyestateGazeData (gaze with eye state)\n    - 89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)\n\n    Raises:\n        KeyError: If the data length does not match any known format.\n        Exception: If there is an error parsing the gaze data.\n\n    \"\"\"\n    data_class_by_raw_len = {\n        9: GazeData,\n        17: DualMonocularGazeData,\n        65: EyestateGazeData,\n        89: EyestateEyelidGazeData,\n    }\n    async for data in super().receive():\n        try:\n            cls = data_class_by_raw_len[len(data.raw)]\n            yield cls.from_raw(data)  # type: ignore[attr-defined]\n        except KeyError:\n            logger.exception(f\"Raw gaze data has unexpected length: {data}\")\n            raise\n        except Exception:\n            logger.exception(f\"Unable to parse gaze data {data}\")\n            raise\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPImuStreamer","title":"RTSPImuStreamer","text":"<pre><code>RTSPImuStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse IMU data from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured IMU data objects.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse IMU data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPImuStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPImuStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPImuStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[IMUData]\n</code></pre> <p>Receive and parse IMU data from the RTSP stream.</p> <p>This method parses the raw binary data into IMUData objects by using the protobuf deserializer.</p> <p>Yields:</p> <ul> <li> <code>IMUData</code> (              <code>AsyncIterator[IMUData]</code> )          \u2013            <p>Parsed IMU data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the IMU data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/imu.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[IMUData]:\n    \"\"\"Receive and parse IMU data from the RTSP stream.\n\n    This method parses the raw binary data into IMUData objects by using\n    the protobuf deserializer.\n\n    Yields:\n        IMUData: Parsed IMU data.\n\n    Raises:\n        Exception: If there is an error parsing the IMU data.\n\n    \"\"\"\n    async for data in super().receive():\n        try:\n            imu_packet = ImuPacket()\n            imu_packet.ParseFromString(data.raw)\n            imu_data = IMUPacket_to_IMUData(imu_packet)\n            yield imu_data\n        except Exception:\n            logger.exception(f\"Unable to parse imu data {data}\")\n            raise\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPRawStreamer","title":"RTSPRawStreamer","text":"<pre><code>RTSPRawStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>Stream raw data from an RTSP source.</p> <p>This class connects to an RTSP source and provides access to the raw data with timestamps synchronized to the device's clock.</p> <p>All constructor arguments are forwarded to the underlying aiortsp.rtsp.reader.RTSPReader.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive raw data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPRawStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPRawStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPRawStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[RTSPData]\n</code></pre> <p>Receive raw data from the RTSP stream.</p> <p>This method yields RTSPData objects containing the raw data and corresponding timestamps.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>async def receive(self) -&gt; AsyncIterator[RTSPData]:\n    \"\"\"Receive raw data from the RTSP stream.\n\n    This method yields RTSPData objects containing the raw data and\n    corresponding timestamps.\n    \"\"\"\n    async for pkt in self.reader.iter_packets():\n        try:\n            timestamp_seconds = self.reader.absolute_timestamp_from_packet(pkt)\n        except _UnknownClockoffsetError:\n            # The absolute timestamp is not known yet.\n            # Waiting for the first RTCP SR packet...\n            continue\n        yield RTSPData(pkt.data, timestamp_seconds)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPVideoFrameStreamer","title":"RTSPVideoFrameStreamer","text":"<pre><code>RTSPVideoFrameStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and decode video frames from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into video frames using the pupil_labs.video and pyav library for decoding.</p> <p>Attributes:</p> <ul> <li> <code>_sprop_parameter_set_payloads</code>               (<code>list[ByteString] | None</code>)           \u2013            <p>Cached SPS/PPS parameters for the H.264 codec.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and decode video frames from the RTSP stream.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self._sprop_parameter_set_payloads: list[ByteString] | None = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.RTSPVideoFrameStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPVideoFrameStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"modules/#pupil_labs.realtime_api.RTSPVideoFrameStreamer.sprop_parameter_set_payloads","title":"sprop_parameter_set_payloads  <code>property</code>","text":"<pre><code>sprop_parameter_set_payloads: list[ByteString] | None\n</code></pre> <p>Get the SPS/PPS parameter set payloads for the H.264 codec.</p> <p>These parameters are extracted from the SDP data and are required for initializing the H.264 decoder.</p> <p>Returns:</p> <ul> <li> <code>list[ByteString] | None</code>           \u2013            <p>list[ByteString]: List of parameter set payloads.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing required fields.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.RTSPVideoFrameStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[VideoFrame]\n</code></pre> <p>Receive and decode video frames from the RTSP stream.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>async def receive(self) -&gt; AsyncIterator[VideoFrame]:  # type: ignore[override]\n    \"\"\"Receive and decode video frames from the RTSP stream.\"\"\"\n    codec = None\n    frame_timestamp = None\n\n    async for data in super().receive():\n        if not codec:\n            try:\n                codec = av.CodecContext.create(self.encoding, \"r\")\n                if self.sprop_parameter_set_payloads:\n                    for param in self.sprop_parameter_set_payloads:\n                        codec.parse(param)\n            except SDPDataNotAvailableError as err:\n                logger.debug(\n                    f\"Session description protocol data not available yet: {err}\"\n                )\n                continue\n            except av.codec.codec.UnknownCodecError:\n                logger.exception(\n                    \"Unknown codec error: \"\n                    \"Please try clearing the app's storage and cache.\"\n                )\n                raise\n        # if pkt is the start of a new fragmented frame, parse will return a packet\n        # containing the data from the previous fragments\n        for packet in codec.parse(extract_payload_from_nal_unit(data.raw)):\n            # use timestamp of previous packets\n            for av_frame in codec.decode(packet):  # type: ignore[attr-defined]\n                if frame_timestamp is None:\n                    raise ValueError(\"No timestamp available for the video frame.\")\n                yield VideoFrame(av_frame, frame_timestamp)\n\n        frame_timestamp = data.timestamp_unix_seconds\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.StatusUpdateNotifier","title":"StatusUpdateNotifier","text":"<pre><code>StatusUpdateNotifier(device: Device, callbacks: list[UpdateCallback])\n</code></pre> <p>Helper class for handling device status update callbacks.</p> <p>This class manages the streaming of status updates from a device and dispatches them to registered callbacks.</p> <p>Attributes:</p> <ul> <li> <code>_auto_update_task</code>               (<code>Task | None</code>)           \u2013            <p>Task for the update loop.</p> </li> <li> <code>_device</code>               (<code>Device</code>)           \u2013            <p>The device to get updates from.</p> </li> <li> <code>_callbacks</code>               (<code>list[UpdateCallback]</code>)           \u2013            <p>List of callbacks to invoke.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>receive_updates_start</code>             \u2013              <p>Start receiving status updates.</p> </li> <li> <code>receive_updates_stop</code>             \u2013              <p>Stop receiving status updates.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>def __init__(self, device: Device, callbacks: list[UpdateCallback]) -&gt; None:\n    self._auto_update_task: asyncio.Task | None = None\n    self._device = device\n    self._callbacks = callbacks\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.StatusUpdateNotifier.receive_updates_start","title":"receive_updates_start  <code>async</code>","text":"<pre><code>receive_updates_start() -&gt; None\n</code></pre> <p>Start receiving status updates.</p> <p>This method starts the background task that receives updates and dispatches them to registered callbacks.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def receive_updates_start(self) -&gt; None:\n    \"\"\"Start receiving status updates.\n\n    This method starts the background task that receives updates\n    and dispatches them to registered callbacks.\n    \"\"\"\n    if self._auto_update_task is not None:\n        logger.debug(\"Auto-update already started!\")\n        return\n    self._auto_update_task = asyncio.create_task(self._auto_update())\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.StatusUpdateNotifier.receive_updates_stop","title":"receive_updates_stop  <code>async</code>","text":"<pre><code>receive_updates_stop() -&gt; None\n</code></pre> <p>Stop receiving status updates.</p> <p>This method cancels the background task that receives updates.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def receive_updates_stop(self) -&gt; None:\n    \"\"\"Stop receiving status updates.\n\n    This method cancels the background task that receives updates.\n    \"\"\"\n    if self._auto_update_task is None:\n        logger.debug(\"Auto-update is not running!\")\n        return\n    self._auto_update_task.cancel()\n    with contextlib.suppress(asyncio.CancelledError):\n        # wait for the task to be cancelled\n        await self._auto_update_task\n    self._auto_update_task = None\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame","title":"VideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A video frame with timestamp information.</p> <p>This class represents a video frame from the scene camera with associated timestamp information. The Class inherits VideoFrame from py.av library.</p> <p>Methods:</p> <ul> <li> <code>bgr_buffer</code>             \u2013              <p>Convert the video frame to a BGR buffer.</p> </li> <li> <code>to_ndarray</code>             \u2013              <p>Convert the video frame to a NumPy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>VideoFrame</code>)           \u2013            <p>The video frame.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: VideoFrame\n</code></pre> <p>The video frame.</p>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.bgr_buffer","title":"bgr_buffer","text":"<pre><code>bgr_buffer() -&gt; BGRBuffer\n</code></pre> <p>Convert the video frame to a BGR buffer.</p> <p>This method converts the video frame to a BGR buffer, which is a NumPy array with the shape (height, width, 3) and dtype uint8. The BGR format is commonly used in computer vision applications.</p> <p>Returns:</p> <ul> <li> <code>BGRBuffer</code> (              <code>BGRBuffer</code> )          \u2013            <p>The BGR buffer as a NumPy array.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def bgr_buffer(self) -&gt; BGRBuffer:\n    \"\"\"Convert the video frame to a BGR buffer.\n\n    This method converts the video frame to a BGR buffer, which is a\n    NumPy array with the shape (height, width, 3) and dtype uint8.\n    The BGR format is commonly used in computer vision applications.\n\n    Returns:\n        BGRBuffer: The BGR buffer as a NumPy array.\n\n    \"\"\"\n    return self.to_ndarray(format=\"bgr24\")\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.VideoFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray(*args: Any, **kwargs: Any) -&gt; NDArray\n</code></pre> <p>Convert the video frame to a NumPy array.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def to_ndarray(self, *args: Any, **kwargs: Any) -&gt; npt.NDArray:\n    \"\"\"Convert the video frame to a NumPy array.\"\"\"\n    return self.av_frame.to_ndarray(*args, **kwargs)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.discover_devices","title":"discover_devices  <code>async</code>","text":"<pre><code>discover_devices(timeout_seconds: float | None = None) -&gt; AsyncIterator[DiscoveredDeviceInfo]\n</code></pre> <p>Use Bonjour to find devices in the local network that serve the Realtime API.</p> <p>This function creates a temporary network discovery client and yields discovered devices as they are found.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Stop after <code>timeout_seconds</code>. If <code>None</code>, run discovery forever.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>DiscoveredDeviceInfo</code> (              <code>AsyncIterator[DiscoveredDeviceInfo]</code> )          \u2013            <p>Information about discovered devices.</p> </li> </ul> Example <pre><code>async for device in discover_devices(timeout_seconds=10.0):\n    print(f\"Found device: {device.name} at {device.addresses[0]}:{device.port}\")\n</code></pre> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def discover_devices(\n    timeout_seconds: float | None = None,\n) -&gt; AsyncIterator[DiscoveredDeviceInfo]:\n    \"\"\"Use Bonjour to find devices in the local network that serve the Realtime API.\n\n    This function creates a temporary network discovery client and yields\n    discovered devices as they are found.\n\n    Args:\n        timeout_seconds: Stop after ``timeout_seconds``. If ``None``, run discovery\n            forever.\n\n    Yields:\n        DiscoveredDeviceInfo: Information about discovered devices.\n\n    Example:\n        ```python\n        async for device in discover_devices(timeout_seconds=10.0):\n            print(f\"Found device: {device.name} at {device.addresses[0]}:{device.port}\")\n        ```\n\n    \"\"\"\n    async with Network() as network:\n        while True:\n            if timeout_seconds is not None and timeout_seconds &lt;= 0.0:\n                return\n            t0 = time.perf_counter()\n            device = await network.wait_for_new_device(timeout_seconds)\n            if device is None:\n                return  # timeout reached\n            else:\n                yield device\n            if timeout_seconds is not None:\n                timeout_seconds -= time.perf_counter() - t0\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.receive_eye_events_data","title":"receive_eye_events_data  <code>async</code>","text":"<pre><code>receive_eye_events_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]\n</code></pre> <p>Receive eye events data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPEyeEventStreamer and yields parsed eye event data.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPEyeEventStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPEyeEventStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>FixationEventData</code> (              <code>AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]</code> )          \u2013            <p>Parsed fixation event data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>async def receive_eye_events_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]:\n    \"\"\"Receive eye events data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPEyeEventStreamer and yields\n    parsed eye event data.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPEyeEventStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPEyeEventStreamer.\n\n    Yields:\n        FixationEventData: Parsed fixation event data.\n\n    \"\"\"\n    async with RTSPEyeEventStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(\n                FixationEventData | FixationOnsetEventData | BlinkEventData, datum\n            )\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.receive_gaze_data","title":"receive_gaze_data  <code>async</code>","text":"<pre><code>receive_gaze_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[GazeDataType]\n</code></pre> <p>Receive gaze data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPGazeStreamer and yields parsed gaze data.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPGazeStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPGazeStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>GazeDataType</code> (              <code>AsyncIterator[GazeDataType]</code> )          \u2013            <p>Parsed gaze data of various types.</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>The type of gaze data object is determined by the length of the raw data packet:</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>9 bytes: GazeData (basic gaze position)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>17 bytes: DualMonocularGazeData (left and right eye positions)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>65 bytes: EyestateGazeData (gaze with eye state)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>async def receive_gaze_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[GazeDataType]:\n    \"\"\"Receive gaze data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPGazeStreamer and yields\n    parsed gaze data.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPGazeStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPGazeStreamer.\n\n    Yields:\n        GazeDataType: Parsed gaze data of various types.\n        The type of gaze data object is determined by the length of the raw data packet:\n        - 9 bytes: GazeData (basic gaze position)\n        - 17 bytes: DualMonocularGazeData (left and right eye positions)\n        - 65 bytes: EyestateGazeData (gaze with eye state)\n        - 89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)\n\n    \"\"\"\n    async with RTSPGazeStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(GazeDataType, datum)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.receive_imu_data","title":"receive_imu_data  <code>async</code>","text":"<pre><code>receive_imu_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[IMUData]\n</code></pre> <p>Receive IMU data from a given RTSP URL.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional arguments for the streamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the streamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>IMUData</code> (              <code>AsyncIterator[IMUData]</code> )          \u2013            <p>Parsed IMU data from the RTSP stream.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/imu.py</code> <pre><code>async def receive_imu_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[IMUData]:\n    \"\"\"Receive IMU data from a given RTSP URL.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional arguments for the streamer.\n        **kwargs: Additional keyword arguments for the streamer.\n\n    Yields:\n        IMUData: Parsed IMU data from the RTSP stream.\n\n    \"\"\"\n    async with RTSPImuStreamer(url, *args, **kwargs) as streamer:\n        assert isinstance(streamer, RTSPImuStreamer)\n        async for datum in streamer.receive():\n            yield cast(IMUData, datum)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.receive_raw_rtsp_data","title":"receive_raw_rtsp_data  <code>async</code>","text":"<pre><code>receive_raw_rtsp_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[RTSPData]\n</code></pre> <p>Receive raw data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPRawStreamer and yields timestamped data packets.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPRawStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPRawStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>RTSPData</code> (              <code>AsyncIterator[RTSPData]</code> )          \u2013            <p>Timestamped RTSP data packets.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>async def receive_raw_rtsp_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[RTSPData]:\n    \"\"\"Receive raw data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPRawStreamer and yields\n    timestamped data packets.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPRawStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPRawStreamer.\n\n    Yields:\n        RTSPData: Timestamped RTSP data packets.\n\n    \"\"\"\n    async with RTSPRawStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(RTSPData, datum)\n</code></pre>"},{"location":"modules/#pupil_labs.realtime_api.receive_video_frames","title":"receive_video_frames  <code>async</code>","text":"<pre><code>receive_video_frames(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[VideoFrame]\n</code></pre> <p>Receive video frames from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPVideoFrameStreamer and yields video frames.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPVideoFrameStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPVideoFrameStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>VideoFrame</code> (              <code>AsyncIterator[VideoFrame]</code> )          \u2013            <p>Parsed video frames.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>async def receive_video_frames(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[VideoFrame]:\n    \"\"\"Receive video frames from an RTSP stream.\n\n    This is a convenience function that creates an RTSPVideoFrameStreamer and yields\n    video frames.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPVideoFrameStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPVideoFrameStreamer.\n\n    Yields:\n        VideoFrame: Parsed video frames.\n\n    \"\"\"\n    async with RTSPVideoFrameStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(VideoFrame, datum)\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>If you are having trouble connecting to your Neon / Pupil Invisible device via the real-time API, consider the following points:</p> <ol> <li> <p>Make sure the Companion Device and the computer/device you are using to access the API are connected to the same local network.</p> </li> <li> <p>For discovery the local network must allow mDNS and UDP traffic. In large public networks this may be prohibited for security reasons.</p> <ul> <li>You may still be able to connect to Neon using its IP address. You can find the IP address in the WiFi settings of the phone or in the Network tab.</li> <li>Alternatively, you can circumvent this by running a separate WiFi using the phone's hotspot functionality or a dedicated WiFi router.</li> </ul> </li> </ol>"},{"location":"api/async/","title":"Asynchronous API","text":""},{"location":"api/async/#device-discovery","title":"Device Discovery","text":""},{"location":"api/async/#pupil_labs.realtime_api.discovery","title":"discovery","text":"<p>Classes:</p> <ul> <li> <code>Network</code>           \u2013            <p>Network discovery client for finding devices.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>discover_devices</code>             \u2013              <p>Use Bonjour to find devices in the local network that serve the Realtime API.</p> </li> <li> <code>is_valid_service_name</code>             \u2013              <p>Check if the service name is valid for Realtime API</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.Network","title":"Network","text":"<pre><code>Network()\n</code></pre> <p>Network discovery client for finding devices.</p> <p>This class manages device discovery on the local network using Zeroconf/Bonjour. It maintains a list of discovered devices and provides methods to access them.</p> <p>Attributes:</p> <ul> <li> <code>_devices</code>               (<code>dict | None</code>)           \u2013            <p>A dictionary of discovered devices, where the keys are device names and the values are DiscoveredDeviceInfo objects.</p> </li> <li> <code>_new_devices</code>               (<code>Queue</code>)           \u2013            <p>A queue to hold newly discovered devices.</p> </li> <li> <code>_aiozeroconf</code>               (<code>AsyncZeroconf | None</code>)           \u2013            <p>An instance of AsyncZeroconf for network discovery.</p> </li> <li> <code>_aiobrowser</code>               (<code>AsyncServiceBrowser | None</code>)           \u2013            <p>An instance of AsyncServiceBrowser for browsing services on the network.</p> </li> <li> <code>_open</code>               (<code>bool</code>)           \u2013            <p>A flag indicating whether the network discovery client is open.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>close</code>             \u2013              <p>Close all network resources.</p> </li> <li> <code>wait_for_new_device</code>             \u2013              <p>Wait for a new device to be discovered.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._devices: dict | None = {}\n    self._new_devices: asyncio.Queue[DiscoveredDeviceInfo] = asyncio.Queue()\n    self._aiozeroconf: AsyncZeroconf | None = AsyncZeroconf()\n    self._aiobrowser: AsyncServiceBrowser | None = AsyncServiceBrowser(\n        self._aiozeroconf.zeroconf,\n        \"_http._tcp.local.\",\n        handlers=[self._handle_service_change],\n    )\n    self._open: bool = True\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.Network.devices","title":"devices  <code>property</code>","text":"<pre><code>devices: tuple[DiscoveredDeviceInfo, ...]\n</code></pre> <p>Return a tuple of discovered devices.</p>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.Network.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close all network resources.</p> <p>This method stops the Zeroconf browser, closes connections, and clears the device list.</p> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close all network resources.\n\n    This method stops the Zeroconf browser, closes connections, and clears\n    the device list.\n    \"\"\"\n    if self._open:\n        await self._aiobrowser.async_cancel() if self._aiobrowser else None\n        await self._aiozeroconf.async_close() if self._aiozeroconf else None\n        if self._devices:\n            self._devices.clear()\n            self._devices = None\n        while not self._new_devices.empty():\n            self._new_devices.get_nowait()\n        self._aiobrowser = None\n        self._aiozeroconf = None\n        self._open = False\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.Network.wait_for_new_device","title":"wait_for_new_device  <code>async</code>","text":"<pre><code>wait_for_new_device(timeout_seconds: float | None = None) -&gt; DiscoveredDeviceInfo | None\n</code></pre> <p>Wait for a new device to be discovered.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new device. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DiscoveredDeviceInfo | None</code>           \u2013            <p>Optional[DiscoveredDeviceInfo]: The newly discovered device, or None if the timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def wait_for_new_device(\n    self, timeout_seconds: float | None = None\n) -&gt; DiscoveredDeviceInfo | None:\n    \"\"\"Wait for a new device to be discovered.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new device.\n            If None, wait indefinitely.\n\n    Returns:\n        Optional[DiscoveredDeviceInfo]: The newly discovered device,\n            or None if the timeout was reached.\n\n    \"\"\"\n    try:\n        return await asyncio.wait_for(self._new_devices.get(), timeout_seconds)\n    except asyncio.TimeoutError:\n        return None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.discover_devices","title":"discover_devices  <code>async</code>","text":"<pre><code>discover_devices(timeout_seconds: float | None = None) -&gt; AsyncIterator[DiscoveredDeviceInfo]\n</code></pre> <p>Use Bonjour to find devices in the local network that serve the Realtime API.</p> <p>This function creates a temporary network discovery client and yields discovered devices as they are found.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Stop after <code>timeout_seconds</code>. If <code>None</code>, run discovery forever.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>DiscoveredDeviceInfo</code> (              <code>AsyncIterator[DiscoveredDeviceInfo]</code> )          \u2013            <p>Information about discovered devices.</p> </li> </ul> Example <pre><code>async for device in discover_devices(timeout_seconds=10.0):\n    print(f\"Found device: {device.name} at {device.addresses[0]}:{device.port}\")\n</code></pre> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>async def discover_devices(\n    timeout_seconds: float | None = None,\n) -&gt; AsyncIterator[DiscoveredDeviceInfo]:\n    \"\"\"Use Bonjour to find devices in the local network that serve the Realtime API.\n\n    This function creates a temporary network discovery client and yields\n    discovered devices as they are found.\n\n    Args:\n        timeout_seconds: Stop after ``timeout_seconds``. If ``None``, run discovery\n            forever.\n\n    Yields:\n        DiscoveredDeviceInfo: Information about discovered devices.\n\n    Example:\n        ```python\n        async for device in discover_devices(timeout_seconds=10.0):\n            print(f\"Found device: {device.name} at {device.addresses[0]}:{device.port}\")\n        ```\n\n    \"\"\"\n    async with Network() as network:\n        while True:\n            if timeout_seconds is not None and timeout_seconds &lt;= 0.0:\n                return\n            t0 = time.perf_counter()\n            device = await network.wait_for_new_device(timeout_seconds)\n            if device is None:\n                return  # timeout reached\n            else:\n                yield device\n            if timeout_seconds is not None:\n                timeout_seconds -= time.perf_counter() - t0\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.discovery.is_valid_service_name","title":"is_valid_service_name","text":"<pre><code>is_valid_service_name(name: str) -&gt; bool\n</code></pre> <p>Check if the service name is valid for Realtime API</p> Source code in <code>src/pupil_labs/realtime_api/discovery.py</code> <pre><code>def is_valid_service_name(name: str) -&gt; bool:\n    \"\"\"Check if the service name is valid for Realtime API\"\"\"\n    return name.split(\":\")[0] == \"PI monitor\"\n</code></pre>"},{"location":"api/async/#remote-control","title":"Remote Control","text":""},{"location":"api/async/#pupil_labs.realtime_api.device","title":"device","text":"<p>Classes:</p> <ul> <li> <code>Device</code>           \u2013            <p>Class representing a Pupil Labs device.</p> </li> <li> <code>DeviceError</code>           \u2013            <p>Exception raised when a device operation fails.</p> </li> <li> <code>StatusUpdateNotifier</code>           \u2013            <p>Helper class for handling device status update callbacks.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>UpdateCallback</code>           \u2013            <p>Type annotation for synchronous and asynchronous callbacks</p> </li> <li> <code>UpdateCallbackAsync</code>           \u2013            <p>Type annotation for asynchronous update callbacks</p> </li> <li> <code>UpdateCallbackSync</code>           \u2013            <p>Type annotation for synchronous update callbacks</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.device.UpdateCallback","title":"UpdateCallback  <code>module-attribute</code>","text":"<pre><code>UpdateCallback = UpdateCallbackSync | UpdateCallbackAsync\n</code></pre> <p>Type annotation for synchronous and asynchronous callbacks</p>"},{"location":"api/async/#pupil_labs.realtime_api.device.UpdateCallbackAsync","title":"UpdateCallbackAsync  <code>module-attribute</code>","text":"<pre><code>UpdateCallbackAsync = Callable[[Component], Awaitable[None]]\n</code></pre> <p>Type annotation for asynchronous update callbacks</p> See Also <p>:class:<code>~pupil_labs.realtime_api.models.Component</code></p>"},{"location":"api/async/#pupil_labs.realtime_api.device.UpdateCallbackSync","title":"UpdateCallbackSync  <code>module-attribute</code>","text":"<pre><code>UpdateCallbackSync = Callable[[Component], None]\n</code></pre> <p>Type annotation for synchronous update callbacks</p> See Also <p>:class:<code>~pupil_labs.realtime_api.models.Component</code></p>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device","title":"Device","text":"<pre><code>Device(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>DeviceBase</code></p> <p>Class representing a Pupil Labs device.</p> <p>This class provides methods to interact with the device, such as starting and stopping recordings, sending events, and fetching device status. It also provides a context manager for automatically closing the device session.</p> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>close</code>             \u2013              <p>Close the connection to the device.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> <li> <code>get_calibration</code>             \u2013              <p>Get the current cameras calibration data.</p> </li> <li> <code>get_status</code>             \u2013              <p>Get the current status of the device.</p> </li> <li> <code>get_template</code>             \u2013              <p>Get the template currently selected on device.</p> </li> <li> <code>get_template_data</code>             \u2013              <p>Get the template data entered on device.</p> </li> <li> <code>post_template_data</code>             \u2013              <p>Set the data for the currently selected template.</p> </li> <li> <code>recording_cancel</code>             \u2013              <p>Cancel the current recording without saving it.</p> </li> <li> <code>recording_start</code>             \u2013              <p>Start a recording on the device.</p> </li> <li> <code>recording_stop_and_save</code>             \u2013              <p>Stop and save the current recording.</p> </li> <li> <code>send_event</code>             \u2013              <p>Send an event to the device.</p> </li> <li> <code>status_updates</code>             \u2013              <p>Stream status updates from the device.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>active_session</code>               (<code>ClientSession</code>)           \u2013            <p>Returns the active session, raising an error if it's None.</p> </li> <li> <code>session</code>               (<code>ClientSession | None</code>)           \u2013            <p>The HTTP session used for making requests.</p> </li> <li> <code>template_definition</code>               (<code>Template | None</code>)           \u2013            <p>The template definition currently selected on the device.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the Device class.\"\"\"\n    super().__init__(*args, **kwargs)\n    self._create_client_session()\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.active_session","title":"active_session  <code>property</code>","text":"<pre><code>active_session: ClientSession\n</code></pre> <p>Returns the active session, raising an error if it's None.</p>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.session","title":"session  <code>instance-attribute</code>","text":"<pre><code>session: ClientSession | None\n</code></pre> <p>The HTTP session used for making requests.</p>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.template_definition","title":"template_definition  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>template_definition: Template | None = None\n</code></pre> <p>The template definition currently selected on the device.</p>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection to the device.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the connection to the device.\"\"\"\n    await self.active_session.close()\n    self.session = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.get_calibration","title":"get_calibration  <code>async</code>","text":"<pre><code>get_calibration() -&gt; Calibration\n</code></pre> <p>Get the current cameras calibration data.</p> <p>Note that Pupil Invisible and Neon are calibration free systems, this refers to the intrinsincs and extrinsics of the cameras and is only available for Neon.</p> <p>Returns:</p> <ul> <li> <code>Calibration</code>           \u2013            <p>pupil_labs.neon_recording.calib.Calibration: The calibration data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_calibration(self) -&gt; Calibration:\n    \"\"\"Get the current cameras calibration data.\n\n    Note that Pupil Invisible and Neon are calibration free systems, this refers to\n    the intrinsincs and extrinsics of the cameras and is only available for Neon.\n\n    Returns:\n        pupil_labs.neon_recording.calib.Calibration: The calibration data.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.CALIBRATION)\n    ) as response:\n        if response.status != 200:\n            raise DeviceError(response.status, \"Failed to fetch calibration\")\n\n        raw_data = await response.read()\n        return cast(Calibration, Calibration.from_buffer(raw_data))\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status() -&gt; Status\n</code></pre> <p>Get the current status of the device.</p> <p>Returns:</p> <ul> <li> <code>Status</code> (              <code>Status</code> )          \u2013            <p>The current device status.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_status(self) -&gt; Status:\n    \"\"\"Get the current status of the device.\n\n    Returns:\n        Status: The current device status.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(self.api_url(APIPath.STATUS)) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_status] Received status: {result}\")\n        return Status.from_dict(result)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.get_template","title":"get_template  <code>async</code>","text":"<pre><code>get_template() -&gt; Template\n</code></pre> <p>Get the template currently selected on device.</p> <p>Returns:</p> <ul> <li> <code>Template</code> (              <code>Template</code> )          \u2013            <p>The currently selected template.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template can't be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template(self) -&gt; Template:\n    \"\"\"Get the template currently selected on device.\n\n    Returns:\n        Template: The currently selected template.\n\n    Raises:\n        DeviceError: If the template can't be fetched.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DEFINITION)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_def] Received template def: {result}\")\n        self.template_definition = Template(**result)\n        return self.template_definition\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.get_template_data","title":"get_template_data  <code>async</code>","text":"<pre><code>get_template_data(template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Get the template data entered on device.</p> <p>Parameters:</p> <ul> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the returned data. - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]} - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The template data in the requested format.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template's data could not be fetched.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template_data(\n    self, template_format: TemplateDataFormat = \"simple\"\n) -&gt; Any:\n    \"\"\"Get the template data entered on device.\n\n    Args:\n        template_format (TemplateDataFormat): Format of the returned data.\n            - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]}\n            - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The template data in the requested format.\n\n    Raises:\n        DeviceError: If the template's data could not be fetched.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DATA)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(\n            f\"[{self}.get_template_data] Received data's template: {result}\"\n        )\n        if template_format == \"api\":\n            return result\n        elif template_format == \"simple\":\n            template = await self.get_template()\n            return template.convert_from_api_to_simple_format(result)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.post_template_data","title":"post_template_data  <code>async</code>","text":"<pre><code>post_template_data(template_answers: dict[str, list[str]], template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Set the data for the currently selected template.</p> <p>Parameters:</p> <ul> <li> <code>template_answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>The template data to send.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the input data. - \"api\" accepts the data as in realtime api format e.g.,     {\"item_uuid\": [\"42\"]} - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result of the operation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the data can not be sent.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid data type.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def post_template_data(\n    self,\n    template_answers: dict[str, list[str]],\n    template_format: TemplateDataFormat = \"simple\",\n) -&gt; Any:\n    \"\"\"Set the data for the currently selected template.\n\n    Args:\n        template_answers: The template data to send.\n        template_format (TemplateDataFormat): Format of the input data.\n            - \"api\" accepts the data as in realtime api format e.g.,\n                {\"item_uuid\": [\"42\"]}\n            - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result of the operation.\n\n    Raises:\n        DeviceError: If the data can not be sent.\n        ValueError: If invalid data type.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    self.template_definition = await self.get_template()\n\n    if template_format == \"simple\":\n        template_answers = (\n            self.template_definition.convert_from_simple_to_api_format(\n                template_answers\n            )\n        )\n\n    pre_populated_data = await self.get_template_data(template_format=\"api\")\n    errors = self.template_definition.validate_answers(\n        pre_populated_data | template_answers, template_format=\"api\"\n    )\n    if errors:\n        raise ValueError(errors)\n\n    # workaround for issue with api as it fails when passing in an empty list\n    # ie. it wants [\"\"] instead of []\n    template_answers = {\n        key: value or [\"\"] for key, value in template_answers.items()\n    }\n\n    async with self.active_session.post(\n        self.api_url(APIPath.TEMPLATE_DATA), json=template_answers\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_data] Send data's template: {result}\")\n        return result\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.recording_cancel","title":"recording_cancel  <code>async</code>","text":"<pre><code>recording_cancel() -&gt; None\n</code></pre> <p>Cancel the current recording without saving it.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be cancelled. Possible reasons include: - Recording not running</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_cancel(self) -&gt; None:\n    \"\"\"Cancel the current recording without saving it.\n\n    Raises:\n        DeviceError: If the recording could not be cancelled.\n            Possible reasons include:\n            - Recording not running\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_CANCEL)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.recording_start","title":"recording_start  <code>async</code>","text":"<pre><code>recording_start() -&gt; str\n</code></pre> <p>Start a recording on the device.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>ID of the started recording.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be started. Possible reasons include: - Recording already running - Template has required fields - Low battery - Low storage - No wearer selected - No workspace selected - Setup bottom sheets not completed</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_start(self) -&gt; str:\n    \"\"\"Start a recording on the device.\n\n    Returns:\n        str: ID of the started recording.\n\n    Raises:\n        DeviceError: If recording could not be started. Possible reasons include:\n            - Recording already running\n            - Template has required fields\n            - Low battery\n            - Low storage\n            - No wearer selected\n            - No workspace selected\n            - Setup bottom sheets not completed\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_START)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.start_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        return cast(str, confirmation[\"result\"][\"id\"])\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.recording_stop_and_save","title":"recording_stop_and_save  <code>async</code>","text":"<pre><code>recording_stop_and_save() -&gt; None\n</code></pre> <p>Stop and save the current recording.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be stopped. Possible reasons include: - Recording not running - Template has required fields</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_stop_and_save(self) -&gt; None:\n    \"\"\"Stop and save the current recording.\n\n    Raises:\n        DeviceError: If recording could not be stopped. Possible reasons include:\n            - Recording not running\n            - Template has required fields\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_STOP_AND_SAVE)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.send_event","title":"send_event  <code>async</code>","text":"<pre><code>send_event(event_name: str, event_timestamp_unix_ns: int | None = None) -&gt; Event\n</code></pre> <p>Send an event to the device.</p> <p>Parameters:</p> <ul> <li> <code>event_name</code>               (<code>str</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>event_timestamp_unix_ns</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional timestamp in unix nanoseconds. If None, the current time will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>The created event.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If sending the event fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def send_event(\n    self, event_name: str, event_timestamp_unix_ns: int | None = None\n) -&gt; Event:\n    \"\"\"Send an event to the device.\n\n    Args:\n        event_name: Name of the event.\n        event_timestamp_unix_ns: Optional timestamp in unix nanoseconds.\n            If None, the current time will be used.\n\n    Returns:\n        Event: The created event.\n\n    Raises:\n        DeviceError: If sending the event fails.\n\n    \"\"\"\n    event: dict[str, Any] = {\"name\": event_name}\n    if event_timestamp_unix_ns is not None:\n        event[\"timestamp\"] = event_timestamp_unix_ns\n\n    async with self.active_session.post(\n        self.api_url(APIPath.EVENT), json=event\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.send_event] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        confirmation[\"result\"][\"name\"] = (\n            event_name  # As the API does not return the name yet\n        )\n        return Event.from_dict(confirmation[\"result\"])\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.Device.status_updates","title":"status_updates  <code>async</code>","text":"<pre><code>status_updates() -&gt; AsyncIterator[Component]\n</code></pre> <p>Stream status updates from the device.</p> <p>Yields:</p> <ul> <li> <code>Component</code> (              <code>AsyncIterator[Component]</code> )          \u2013            <p>Status update components as they arrive.</p> </li> </ul> <p>Auto-reconnect, see:     https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def status_updates(self) -&gt; AsyncIterator[Component]:\n    \"\"\"Stream status updates from the device.\n\n    Yields:\n        Component: Status update components as they arrive.\n\n    Auto-reconnect, see:\n        https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect\n\n    \"\"\"\n    websocket_status_endpoint = self.api_url(APIPath.STATUS, protocol=\"ws\")\n    async for websocket in websockets.connect(websocket_status_endpoint):\n        try:\n            async for message_raw in websocket:\n                message_json = json.loads(message_raw)\n                try:\n                    component = parse_component(message_json)\n                except UnknownComponentError:\n                    logger.warning(f\"Dropping unknown component: {component}\")\n                    continue\n                yield component\n        except websockets.ConnectionClosed:\n            logger.debug(\"Websocket connection closed. Reconnecting...\")\n            continue\n        except asyncio.CancelledError:\n            logger.debug(\"status_updates() cancelled\")\n            break\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.DeviceError","title":"DeviceError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a device operation fails.</p>"},{"location":"api/async/#pupil_labs.realtime_api.device.StatusUpdateNotifier","title":"StatusUpdateNotifier","text":"<pre><code>StatusUpdateNotifier(device: Device, callbacks: list[UpdateCallback])\n</code></pre> <p>Helper class for handling device status update callbacks.</p> <p>This class manages the streaming of status updates from a device and dispatches them to registered callbacks.</p> <p>Attributes:</p> <ul> <li> <code>_auto_update_task</code>               (<code>Task | None</code>)           \u2013            <p>Task for the update loop.</p> </li> <li> <code>_device</code>               (<code>Device</code>)           \u2013            <p>The device to get updates from.</p> </li> <li> <code>_callbacks</code>               (<code>list[UpdateCallback]</code>)           \u2013            <p>List of callbacks to invoke.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>receive_updates_start</code>             \u2013              <p>Start receiving status updates.</p> </li> <li> <code>receive_updates_stop</code>             \u2013              <p>Stop receiving status updates.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>def __init__(self, device: Device, callbacks: list[UpdateCallback]) -&gt; None:\n    self._auto_update_task: asyncio.Task | None = None\n    self._device = device\n    self._callbacks = callbacks\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.StatusUpdateNotifier.receive_updates_start","title":"receive_updates_start  <code>async</code>","text":"<pre><code>receive_updates_start() -&gt; None\n</code></pre> <p>Start receiving status updates.</p> <p>This method starts the background task that receives updates and dispatches them to registered callbacks.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def receive_updates_start(self) -&gt; None:\n    \"\"\"Start receiving status updates.\n\n    This method starts the background task that receives updates\n    and dispatches them to registered callbacks.\n    \"\"\"\n    if self._auto_update_task is not None:\n        logger.debug(\"Auto-update already started!\")\n        return\n    self._auto_update_task = asyncio.create_task(self._auto_update())\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.device.StatusUpdateNotifier.receive_updates_stop","title":"receive_updates_stop  <code>async</code>","text":"<pre><code>receive_updates_stop() -&gt; None\n</code></pre> <p>Stop receiving status updates.</p> <p>This method cancels the background task that receives updates.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def receive_updates_stop(self) -&gt; None:\n    \"\"\"Stop receiving status updates.\n\n    This method cancels the background task that receives updates.\n    \"\"\"\n    if self._auto_update_task is None:\n        logger.debug(\"Auto-update is not running!\")\n        return\n    self._auto_update_task.cancel()\n    with contextlib.suppress(asyncio.CancelledError):\n        # wait for the task to be cancelled\n        await self._auto_update_task\n    self._auto_update_task = None\n</code></pre>"},{"location":"api/async/#streaming","title":"Streaming","text":""},{"location":"api/async/#gaze-data","title":"Gaze Data","text":""},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze","title":"gaze","text":"<p>Classes:</p> <ul> <li> <code>DualMonocularGazeData</code>           \u2013            <p>Experimental class for dual monocular gaze data.</p> </li> <li> <code>EyestateEyelidGazeData</code>           \u2013            <p>Gaze data with additional eyelid state information.</p> </li> <li> <code>EyestateGazeData</code>           \u2013            <p>Gaze data with additional eye state information.</p> </li> <li> <code>GazeData</code>           \u2013            <p>Basic gaze data with position, timestamp and indicator of glasses worn status.</p> </li> <li> <code>Point</code>           \u2013            <p>A point in 2D space, represented by x and y coordinates.</p> </li> <li> <code>RTSPGazeStreamer</code>           \u2013            <p>Stream and parse gaze data from an RTSP source.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>receive_gaze_data</code>             \u2013              <p>Receive gaze data from an RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>GazeDataType</code>           \u2013            <p>Type alias for various gaze data types.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeDataType","title":"GazeDataType  <code>module-attribute</code>","text":"<pre><code>GazeDataType = GazeData | DualMonocularGazeData | EyestateGazeData | EyestateEyelidGazeData\n</code></pre> <p>Type alias for various gaze data types.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData","title":"DualMonocularGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Experimental class for dual monocular gaze data.</p> <p>Contains separate gaze points for left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a DualMonocularGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>left</code>               (<code>Point</code>)           \u2013            <p>Gaze point for the left eye.</p> </li> <li> <code>right</code>               (<code>Point</code>)           \u2013            <p>Gaze point for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.left","title":"left  <code>instance-attribute</code>","text":"<pre><code>left: Point\n</code></pre> <p>Gaze point for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.right","title":"right  <code>instance-attribute</code>","text":"<pre><code>right: Point\n</code></pre> <p>Gaze point for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.DualMonocularGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; DualMonocularGazeData\n</code></pre> <p>Create a DualMonocularGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DualMonocularGazeData</code> (              <code>DualMonocularGazeData</code> )          \u2013            <p>An instance of DualMonocularGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"DualMonocularGazeData\":\n    \"\"\"Create a DualMonocularGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        DualMonocularGazeData: An instance of DualMonocularGazeData with the parsed\n            values.\n\n    \"\"\"\n    x1, y1, worn, x2, y2 = struct.unpack(\"!ffBff\", data.raw)\n    return cls(\n        Point(x1, y1), Point(x2, y2), worn == 255, data.timestamp_unix_seconds\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData","title":"EyestateEyelidGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Gaze data with additional eyelid state information.</p> <p>Contains gaze point, pupil diameter, eyeball center coordinates, optical axis coordinates, as well as eyelid angles and aperture for both left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create an EyestateEyelidGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>eyeball_center_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyelid_angle_bottom_left</code>               (<code>float</code>)           \u2013            <p>Angle of the bottom eyelid for the left eye(rad).</p> </li> <li> <code>eyelid_angle_bottom_right</code>               (<code>float</code>)           \u2013            <p>Angle of the bottom eyelid for the right eye (rad).</p> </li> <li> <code>eyelid_angle_top_left</code>               (<code>float</code>)           \u2013            <p>Angle of the top eyelid for the left eye(rad).</p> </li> <li> <code>eyelid_angle_top_right</code>               (<code>float</code>)           \u2013            <p>Angle of the top eyelid for the right eye (rad).</p> </li> <li> <code>eyelid_aperture_left</code>               (<code>float</code>)           \u2013            <p>Aperture of the eyelid for the left eye (mm).</p> </li> <li> <code>eyelid_aperture_right</code>               (<code>float</code>)           \u2013            <p>Aperture of the eyelid for the right eye (mm).</p> </li> <li> <code>optical_axis_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the right eye.</p> </li> <li> <code>pupil_diameter_left</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the left eye.</p> </li> <li> <code>pupil_diameter_right</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the gaze point.</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_x","title":"eyeball_center_left_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_x: float\n</code></pre> <p>X coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_y","title":"eyeball_center_left_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_z","title":"eyeball_center_left_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_x","title":"eyeball_center_right_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_x: float\n</code></pre> <p>X coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_y","title":"eyeball_center_right_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_z","title":"eyeball_center_right_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_bottom_left","title":"eyelid_angle_bottom_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_bottom_left: float\n</code></pre> <p>Angle of the bottom eyelid for the left eye(rad).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_bottom_right","title":"eyelid_angle_bottom_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_bottom_right: float\n</code></pre> <p>Angle of the bottom eyelid for the right eye (rad).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_top_left","title":"eyelid_angle_top_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_top_left: float\n</code></pre> <p>Angle of the top eyelid for the left eye(rad).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_top_right","title":"eyelid_angle_top_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_top_right: float\n</code></pre> <p>Angle of the top eyelid for the right eye (rad).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_aperture_left","title":"eyelid_aperture_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_aperture_left: float\n</code></pre> <p>Aperture of the eyelid for the left eye (mm).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_aperture_right","title":"eyelid_aperture_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_aperture_right: float\n</code></pre> <p>Aperture of the eyelid for the right eye (mm).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_x","title":"optical_axis_left_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_x: float\n</code></pre> <p>X coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_y","title":"optical_axis_left_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_y: float\n</code></pre> <p>Y coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_z","title":"optical_axis_left_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_z: float\n</code></pre> <p>Z coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_x","title":"optical_axis_right_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_x: float\n</code></pre> <p>X coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_y","title":"optical_axis_right_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_y: float\n</code></pre> <p>Y coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_z","title":"optical_axis_right_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_z: float\n</code></pre> <p>Z coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.pupil_diameter_left","title":"pupil_diameter_left  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left: float\n</code></pre> <p>Pupil diameter for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.pupil_diameter_right","title":"pupil_diameter_right  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right: float\n</code></pre> <p>Pupil diameter for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>X coordinate of the gaze point.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; EyestateEyelidGazeData\n</code></pre> <p>Create an EyestateEyelidGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EyestateEyelidGazeData</code> (              <code>EyestateEyelidGazeData</code> )          \u2013            <p>An instance of EyestateEyelidGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"EyestateEyelidGazeData\":\n    \"\"\"Create an EyestateEyelidGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        EyestateEyelidGazeData: An instance of EyestateEyelidGazeData with the\n            parsed values.\n\n    \"\"\"\n    (\n        x,\n        y,\n        worn,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        eyelid_angle_top_left,\n        eyelid_angle_bottom_left,\n        eyelid_aperture_left,\n        eyelid_angle_top_right,\n        eyelid_angle_bottom_right,\n        eyelid_aperture_right,\n    ) = struct.unpack(\"!ffBffffffffffffffffffff\", data.raw)\n    return cls(\n        x,\n        y,\n        worn == 255,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        eyelid_angle_top_left,\n        eyelid_angle_bottom_left,\n        eyelid_aperture_left,\n        eyelid_angle_top_right,\n        eyelid_angle_bottom_right,\n        eyelid_aperture_right,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData","title":"EyestateGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Gaze data with additional eye state information.</p> <p>Contains gaze point, pupil diameter, eyeball center coordinates, and optical axis coordinates for both left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create an EyestateGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>eyeball_center_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the right eye.</p> </li> <li> <code>optical_axis_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the right eye.</p> </li> <li> <code>pupil_diameter_left</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the left eye.</p> </li> <li> <code>pupil_diameter_right</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the gaze point.</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_x","title":"eyeball_center_left_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_x: float\n</code></pre> <p>X coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_y","title":"eyeball_center_left_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_z","title":"eyeball_center_left_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_x","title":"eyeball_center_right_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_x: float\n</code></pre> <p>X coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_y","title":"eyeball_center_right_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_z","title":"eyeball_center_right_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_x","title":"optical_axis_left_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_x: float\n</code></pre> <p>X coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_y","title":"optical_axis_left_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_y: float\n</code></pre> <p>Y coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_z","title":"optical_axis_left_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_z: float\n</code></pre> <p>Z coordinate of the optical axis for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_x","title":"optical_axis_right_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_x: float\n</code></pre> <p>X coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_y","title":"optical_axis_right_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_y: float\n</code></pre> <p>Y coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_z","title":"optical_axis_right_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_z: float\n</code></pre> <p>Z coordinate of the optical axis for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.pupil_diameter_left","title":"pupil_diameter_left  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left: float\n</code></pre> <p>Pupil diameter for the left eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.pupil_diameter_right","title":"pupil_diameter_right  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right: float\n</code></pre> <p>Pupil diameter for the right eye.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>X coordinate of the gaze point.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; EyestateGazeData\n</code></pre> <p>Create an EyestateGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EyestateGazeData</code> (              <code>EyestateGazeData</code> )          \u2013            <p>An instance of EyestateGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"EyestateGazeData\":\n    \"\"\"Create an EyestateGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        EyestateGazeData: An instance of EyestateGazeData with the parsed values.\n\n    \"\"\"\n    (\n        x,\n        y,\n        worn,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n    ) = struct.unpack(\"!ffBffffffffffffff\", data.raw)\n    return cls(\n        x,\n        y,\n        worn == 255,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData","title":"GazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Basic gaze data with position, timestamp and indicator of glasses worn status.</p> <p>Represents the 2D gaze point on the scene camera coordinates with a timestamp in nanoseconds unix epoch and an indicator of whether the glasses are being worn.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a GazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>\"X coordinate of the gaze point</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>\"X coordinate of the gaze point</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.GazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; GazeData\n</code></pre> <p>Create a GazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GazeData</code> (              <code>GazeData</code> )          \u2013            <p>An instance of GazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"GazeData\":\n    \"\"\"Create a GazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        GazeData: An instance of GazeData with the parsed values.\n\n    \"\"\"\n    x, y, worn = struct.unpack(\"!ffB\", data.raw)\n    return cls(x, y, worn == 255, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.Point","title":"Point","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A point in 2D space, represented by x and y coordinates.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.RTSPGazeStreamer","title":"RTSPGazeStreamer","text":"<pre><code>RTSPGazeStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse gaze data from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured gaze data objects. The specific type of gaze data is determined by the length of the raw data packet.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse gaze data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.RTSPGazeStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.RTSPGazeStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.RTSPGazeStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[GazeDataType]\n</code></pre> <p>Receive and parse gaze data from the RTSP stream.</p> <p>Yields:</p> <ul> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>GazeDataType</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>Parsed gaze data of various types. The type of gaze data object is</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>determined by the length of the raw data packet:</p> </li> </ul> <ul> <li>9 bytes: GazeData (basic gaze position)</li> <li>17 bytes: DualMonocularGazeData (left and right eye positions)</li> <li>65 bytes: EyestateGazeData (gaze with eye state)</li> <li>89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)</li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the data length does not match any known format.</p> </li> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the gaze data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[GazeDataType]:\n    \"\"\"Receive and parse gaze data from the RTSP stream.\n\n    Yields:\n        GazeDataType\n\n        Parsed gaze data of various types. The type of gaze data object is\n        determined by the length of the raw data packet:\n    - 9 bytes: GazeData (basic gaze position)\n    - 17 bytes: DualMonocularGazeData (left and right eye positions)\n    - 65 bytes: EyestateGazeData (gaze with eye state)\n    - 89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)\n\n    Raises:\n        KeyError: If the data length does not match any known format.\n        Exception: If there is an error parsing the gaze data.\n\n    \"\"\"\n    data_class_by_raw_len = {\n        9: GazeData,\n        17: DualMonocularGazeData,\n        65: EyestateGazeData,\n        89: EyestateEyelidGazeData,\n    }\n    async for data in super().receive():\n        try:\n            cls = data_class_by_raw_len[len(data.raw)]\n            yield cls.from_raw(data)  # type: ignore[attr-defined]\n        except KeyError:\n            logger.exception(f\"Raw gaze data has unexpected length: {data}\")\n            raise\n        except Exception:\n            logger.exception(f\"Unable to parse gaze data {data}\")\n            raise\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.gaze.receive_gaze_data","title":"receive_gaze_data  <code>async</code>","text":"<pre><code>receive_gaze_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[GazeDataType]\n</code></pre> <p>Receive gaze data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPGazeStreamer and yields parsed gaze data.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPGazeStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPGazeStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>GazeDataType</code> (              <code>AsyncIterator[GazeDataType]</code> )          \u2013            <p>Parsed gaze data of various types.</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <p>The type of gaze data object is determined by the length of the raw data packet:</p> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>9 bytes: GazeData (basic gaze position)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>17 bytes: DualMonocularGazeData (left and right eye positions)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>65 bytes: EyestateGazeData (gaze with eye state)</li> </ul> </li> <li> <code>AsyncIterator[GazeDataType]</code>           \u2013            <ul> <li>89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>async def receive_gaze_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[GazeDataType]:\n    \"\"\"Receive gaze data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPGazeStreamer and yields\n    parsed gaze data.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPGazeStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPGazeStreamer.\n\n    Yields:\n        GazeDataType: Parsed gaze data of various types.\n        The type of gaze data object is determined by the length of the raw data packet:\n        - 9 bytes: GazeData (basic gaze position)\n        - 17 bytes: DualMonocularGazeData (left and right eye positions)\n        - 65 bytes: EyestateGazeData (gaze with eye state)\n        - 89 bytes: EyestateEyelidGazeData (gaze with eye state and eyelid info)\n\n    \"\"\"\n    async with RTSPGazeStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(GazeDataType, datum)\n</code></pre>"},{"location":"api/async/#imu-data","title":"IMU Data","text":""},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu","title":"imu","text":"<p>Classes:</p> <ul> <li> <code>Data3D</code>           \u2013            <p>3D data point with x, y, z coordinates.</p> </li> <li> <code>IMUData</code>           \u2013            <p>Data from the Inertial Measurement Unit (IMU).</p> </li> <li> <code>Quaternion</code>           \u2013            <p>Quaternion data point with x, y, z, w coordinates.</p> </li> <li> <code>RTSPImuStreamer</code>           \u2013            <p>Stream and parse IMU data from an RTSP source.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>IMUPacket_to_IMUData</code>             \u2013              <p>Create an IMUData instance from a protobuf IMU packet.</p> </li> <li> <code>receive_imu_data</code>             \u2013              <p>Receive IMU data from a given RTSP URL.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.Data3D","title":"Data3D","text":"<p>               Bases: <code>NamedTuple</code></p> <p>3D data point with x, y, z coordinates.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData","title":"IMUData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data from the Inertial Measurement Unit (IMU).</p> <p>Contains gyroscope, accelerometer, and rotation data from the IMU sensor.</p> <p>Attributes:</p> <ul> <li> <code>accel_data</code>               (<code>Data3D</code>)           \u2013            <p>Accelerometer data in m/s\u00b2.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>gyro_data</code>               (<code>Data3D</code>)           \u2013            <p>Gyroscope data in deg/s.</p> </li> <li> <code>quaternion</code>               (<code>Quaternion</code>)           \u2013            <p>Rotation represented as a quaternion.</p> </li> <li> <code>timestamp_unix_nanoseconds</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.accel_data","title":"accel_data  <code>instance-attribute</code>","text":"<pre><code>accel_data: Data3D\n</code></pre> <p>Accelerometer data in m/s\u00b2.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.gyro_data","title":"gyro_data  <code>instance-attribute</code>","text":"<pre><code>gyro_data: Data3D\n</code></pre> <p>Gyroscope data in deg/s.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.quaternion","title":"quaternion  <code>instance-attribute</code>","text":"<pre><code>quaternion: Quaternion\n</code></pre> <p>Rotation represented as a quaternion.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_nanoseconds","title":"timestamp_unix_nanoseconds  <code>property</code>","text":"<pre><code>timestamp_unix_nanoseconds: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p> Deprecated <p>This class property is deprecated and will be removed in future versions. Use timestamp_unix_ns() instead.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.Quaternion","title":"Quaternion","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Quaternion data point with x, y, z, w coordinates.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.RTSPImuStreamer","title":"RTSPImuStreamer","text":"<pre><code>RTSPImuStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse IMU data from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured IMU data objects.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse IMU data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.RTSPImuStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.RTSPImuStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.RTSPImuStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[IMUData]\n</code></pre> <p>Receive and parse IMU data from the RTSP stream.</p> <p>This method parses the raw binary data into IMUData objects by using the protobuf deserializer.</p> <p>Yields:</p> <ul> <li> <code>IMUData</code> (              <code>AsyncIterator[IMUData]</code> )          \u2013            <p>Parsed IMU data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the IMU data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/imu.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[IMUData]:\n    \"\"\"Receive and parse IMU data from the RTSP stream.\n\n    This method parses the raw binary data into IMUData objects by using\n    the protobuf deserializer.\n\n    Yields:\n        IMUData: Parsed IMU data.\n\n    Raises:\n        Exception: If there is an error parsing the IMU data.\n\n    \"\"\"\n    async for data in super().receive():\n        try:\n            imu_packet = ImuPacket()\n            imu_packet.ParseFromString(data.raw)\n            imu_data = IMUPacket_to_IMUData(imu_packet)\n            yield imu_data\n        except Exception:\n            logger.exception(f\"Unable to parse imu data {data}\")\n            raise\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.IMUPacket_to_IMUData","title":"IMUPacket_to_IMUData","text":"<pre><code>IMUPacket_to_IMUData(imu_packet: ImuPacket) -&gt; IMUData\n</code></pre> <p>Create an IMUData instance from a protobuf IMU packet.</p> <p>Parameters:</p> <ul> <li> <code>imu_packet</code>               (<code>ImuPacket</code>)           \u2013            <p>Protobuf IMU packet.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IMUData</code> (              <code>IMUData</code> )          \u2013            <p>Converted IMU data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/imu.py</code> <pre><code>def IMUPacket_to_IMUData(imu_packet: \"ImuPacket\") -&gt; IMUData:  # type: ignore[no-any-unimported]\n    \"\"\"Create an IMUData instance from a protobuf IMU packet.\n\n    Args:\n        imu_packet: Protobuf IMU packet.\n\n    Returns:\n        IMUData: Converted IMU data.\n\n    \"\"\"\n    gyro_data = Data3D(\n        x=imu_packet.gyroData.x,\n        y=imu_packet.gyroData.y,\n        z=imu_packet.gyroData.z,\n    )\n    accel_data = Data3D(\n        x=imu_packet.accelData.x,\n        y=imu_packet.accelData.y,\n        z=imu_packet.accelData.z,\n    )\n    quaternion = Quaternion(\n        x=imu_packet.rotVecData.x,\n        y=imu_packet.rotVecData.y,\n        z=imu_packet.rotVecData.z,\n        w=imu_packet.rotVecData.w,\n    )\n    imu_data = IMUData(\n        gyro_data=gyro_data,\n        accel_data=accel_data,\n        quaternion=quaternion,\n        timestamp_unix_seconds=imu_packet.tsNs / 1e9,\n    )\n    return imu_data\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.imu.receive_imu_data","title":"receive_imu_data  <code>async</code>","text":"<pre><code>receive_imu_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[IMUData]\n</code></pre> <p>Receive IMU data from a given RTSP URL.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional arguments for the streamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the streamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>IMUData</code> (              <code>AsyncIterator[IMUData]</code> )          \u2013            <p>Parsed IMU data from the RTSP stream.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/imu.py</code> <pre><code>async def receive_imu_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[IMUData]:\n    \"\"\"Receive IMU data from a given RTSP URL.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional arguments for the streamer.\n        **kwargs: Additional keyword arguments for the streamer.\n\n    Yields:\n        IMUData: Parsed IMU data from the RTSP stream.\n\n    \"\"\"\n    async with RTSPImuStreamer(url, *args, **kwargs) as streamer:\n        assert isinstance(streamer, RTSPImuStreamer)\n        async for datum in streamer.receive():\n            yield cast(IMUData, datum)\n</code></pre>"},{"location":"api/async/#eye-events","title":"Eye Events","text":""},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events","title":"eye_events","text":"<p>Classes:</p> <ul> <li> <code>BlinkEventData</code>           \u2013            <p>Data for a blink event.</p> </li> <li> <code>FixationEventData</code>           \u2013            <p>Data for a fixation or saccade event.</p> </li> <li> <code>FixationOnsetEventData</code>           \u2013            <p>Data for a fixation or saccade onset event.</p> </li> <li> <code>RTSPEyeEventStreamer</code>           \u2013            <p>Stream and parse eye events from an RTSP source.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>receive_eye_events_data</code>             \u2013              <p>Receive eye events data from an RTSP stream.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData","title":"BlinkEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a blink event.</p> <p>Represents a detected blink event with timing information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a BlinkEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the blink in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (4 -&gt; blink events).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the blink in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the blink in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (4 -&gt; blink events).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the blink in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; BlinkEventData\n</code></pre> <p>Create a BlinkEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"BlinkEventData\":\n    \"\"\"Create a BlinkEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n    ) = struct.unpack(\"!iqq\", data.raw)\n    return cls(event_type, start_time_ns, end_time_ns, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData","title":"FixationEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade event.</p> <p>Represents a completed fixation or saccade event with detailed information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a FixationEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>amplitude_angle_deg</code>               (<code>float</code>)           \u2013            <p>Amplitude in degrees.</p> </li> <li> <code>amplitude_pixels</code>               (<code>float</code>)           \u2013            <p>Amplitude in pixels.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_gaze_x</code>               (<code>float</code>)           \u2013            <p>End gaze x-coordinate in pixels.</p> </li> <li> <code>end_gaze_y</code>               (<code>float</code>)           \u2013            <p>End gaze y-coordinate in pixels.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the event in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (0 for saccade, 1 for fixation).</p> </li> <li> <code>max_velocity</code>               (<code>float</code>)           \u2013            <p>Maximum velocity in pixels per degree.</p> </li> <li> <code>mean_gaze_x</code>               (<code>float</code>)           \u2013            <p>Mean gaze x-coordinate in pixels.</p> </li> <li> <code>mean_gaze_y</code>               (<code>float</code>)           \u2013            <p>Mean gaze y-coordinate in pixels.</p> </li> <li> <code>mean_velocity</code>               (<code>float</code>)           \u2013            <p>Mean velocity in pixels per degree.</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_gaze_x</code>               (<code>float</code>)           \u2013            <p>Start gaze x-coordinate in pixels.</p> </li> <li> <code>start_gaze_y</code>               (<code>float</code>)           \u2013            <p>Start gaze y-coordinate in pixels.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_angle_deg","title":"amplitude_angle_deg  <code>instance-attribute</code>","text":"<pre><code>amplitude_angle_deg: float\n</code></pre> <p>Amplitude in degrees.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_pixels","title":"amplitude_pixels  <code>instance-attribute</code>","text":"<pre><code>amplitude_pixels: float\n</code></pre> <p>Amplitude in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_x","title":"end_gaze_x  <code>instance-attribute</code>","text":"<pre><code>end_gaze_x: float\n</code></pre> <p>End gaze x-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_y","title":"end_gaze_y  <code>instance-attribute</code>","text":"<pre><code>end_gaze_y: float\n</code></pre> <p>End gaze y-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the event in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (0 for saccade, 1 for fixation).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.max_velocity","title":"max_velocity  <code>instance-attribute</code>","text":"<pre><code>max_velocity: float\n</code></pre> <p>Maximum velocity in pixels per degree.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_x","title":"mean_gaze_x  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_x: float\n</code></pre> <p>Mean gaze x-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_y","title":"mean_gaze_y  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_y: float\n</code></pre> <p>Mean gaze y-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_velocity","title":"mean_velocity  <code>instance-attribute</code>","text":"<pre><code>mean_velocity: float\n</code></pre> <p>Mean velocity in pixels per degree.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_x","title":"start_gaze_x  <code>instance-attribute</code>","text":"<pre><code>start_gaze_x: float\n</code></pre> <p>Start gaze x-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_y","title":"start_gaze_y  <code>instance-attribute</code>","text":"<pre><code>start_gaze_y: float\n</code></pre> <p>Start gaze y-coordinate in pixels.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; FixationEventData\n</code></pre> <p>Create a FixationEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"FixationEventData\":\n    \"\"\"Create a FixationEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n    ) = struct.unpack(\"!iqqffffffffff\", data.raw)\n    return cls(\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData","title":"FixationOnsetEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade onset event.</p> <p>Represents the beginning of a fixation or saccade event.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (2 for saccade onset, 3 for fixation onset).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (2 for saccade onset, 3 for fixation onset).</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.RTSPEyeEventStreamer","title":"RTSPEyeEventStreamer","text":"<pre><code>RTSPEyeEventStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and parse eye events from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into structured eye event data objects.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and parse eye events from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.RTSPEyeEventStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.RTSPEyeEventStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.RTSPEyeEventStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]\n</code></pre> <p>Receive and parse eye events from the RTSP stream.</p> <p>Yields:</p> <ul> <li> <code>AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]</code>           \u2013            <p>FixationEventData | FixationOnsetEventData | BlinkEventData: Parsed eye event data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the event type is not recognized.</p> </li> <li> <code>Exception</code>             \u2013            <p>If there is an error parsing the event data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>async def receive(  # type: ignore[override]\n    self,\n) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]:\n    \"\"\"Receive and parse eye events from the RTSP stream.\n\n    Yields:\n        FixationEventData | FixationOnsetEventData | BlinkEventData: Parsed eye\n            event data.\n\n    Raises:\n        KeyError: If the event type is not recognized.\n        Exception: If there is an error parsing the event data.\n\n    \"\"\"\n    data_class_by_type = {\n        0: FixationEventData,\n        1: FixationEventData,\n        2: FixationOnsetEventData,\n        3: FixationOnsetEventData,\n        4: BlinkEventData,\n        5: None,  # KEEPALIVE MSG, SKIP\n    }\n    async for data in super().receive():\n        try:\n            event_type = struct.unpack_from(\"!i\", data.raw)[0]\n            cls = data_class_by_type[event_type]\n            if cls is not None:\n                yield cls.from_raw(data)  # type: ignore[attr-defined]\n        except KeyError:\n            logger.exception(f\"Raw eye event data has unexpected type: {data}\")\n            raise\n        except Exception:\n            logger.exception(f\"Unable to parse eye event data {data}\")\n            raise\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.eye_events.receive_eye_events_data","title":"receive_eye_events_data  <code>async</code>","text":"<pre><code>receive_eye_events_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]\n</code></pre> <p>Receive eye events data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPEyeEventStreamer and yields parsed eye event data.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPEyeEventStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPEyeEventStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>FixationEventData</code> (              <code>AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]</code> )          \u2013            <p>Parsed fixation event data.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>async def receive_eye_events_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[FixationEventData | FixationOnsetEventData | BlinkEventData]:\n    \"\"\"Receive eye events data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPEyeEventStreamer and yields\n    parsed eye event data.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPEyeEventStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPEyeEventStreamer.\n\n    Yields:\n        FixationEventData: Parsed fixation event data.\n\n    \"\"\"\n    async with RTSPEyeEventStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(\n                FixationEventData | FixationOnsetEventData | BlinkEventData, datum\n            )\n</code></pre>"},{"location":"api/async/#scene-video","title":"Scene Video","text":""},{"location":"api/async/#pupil_labs.realtime_api.streaming.video","title":"video","text":"<p>Classes:</p> <ul> <li> <code>RTSPVideoFrameStreamer</code>           \u2013            <p>Stream and decode video frames from an RTSP source.</p> </li> <li> <code>VideoFrame</code>           \u2013            <p>A video frame with timestamp information.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>receive_video_frames</code>             \u2013              <p>Receive video frames from an RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BGRBuffer</code>           \u2013            <p>Type annotation for raw BGR image buffers of the scene camera</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.BGRBuffer","title":"BGRBuffer  <code>module-attribute</code>","text":"<pre><code>BGRBuffer = NDArray[uint8]\n</code></pre> <p>Type annotation for raw BGR image buffers of the scene camera</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.RTSPVideoFrameStreamer","title":"RTSPVideoFrameStreamer","text":"<pre><code>RTSPVideoFrameStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>RTSPRawStreamer</code></p> <p>Stream and decode video frames from an RTSP source.</p> <p>This class extends RTSPRawStreamer to parse raw RTSP data into video frames using the pupil_labs.video and pyav library for decoding.</p> <p>Attributes:</p> <ul> <li> <code>_sprop_parameter_set_payloads</code>               (<code>list[ByteString] | None</code>)           \u2013            <p>Cached SPS/PPS parameters for the H.264 codec.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive and decode video frames from the RTSP stream.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self._sprop_parameter_set_payloads: list[ByteString] | None = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.RTSPVideoFrameStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.RTSPVideoFrameStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.RTSPVideoFrameStreamer.sprop_parameter_set_payloads","title":"sprop_parameter_set_payloads  <code>property</code>","text":"<pre><code>sprop_parameter_set_payloads: list[ByteString] | None\n</code></pre> <p>Get the SPS/PPS parameter set payloads for the H.264 codec.</p> <p>These parameters are extracted from the SDP data and are required for initializing the H.264 decoder.</p> <p>Returns:</p> <ul> <li> <code>list[ByteString] | None</code>           \u2013            <p>list[ByteString]: List of parameter set payloads.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing required fields.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.RTSPVideoFrameStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[VideoFrame]\n</code></pre> <p>Receive and decode video frames from the RTSP stream.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>async def receive(self) -&gt; AsyncIterator[VideoFrame]:  # type: ignore[override]\n    \"\"\"Receive and decode video frames from the RTSP stream.\"\"\"\n    codec = None\n    frame_timestamp = None\n\n    async for data in super().receive():\n        if not codec:\n            try:\n                codec = av.CodecContext.create(self.encoding, \"r\")\n                if self.sprop_parameter_set_payloads:\n                    for param in self.sprop_parameter_set_payloads:\n                        codec.parse(param)\n            except SDPDataNotAvailableError as err:\n                logger.debug(\n                    f\"Session description protocol data not available yet: {err}\"\n                )\n                continue\n            except av.codec.codec.UnknownCodecError:\n                logger.exception(\n                    \"Unknown codec error: \"\n                    \"Please try clearing the app's storage and cache.\"\n                )\n                raise\n        # if pkt is the start of a new fragmented frame, parse will return a packet\n        # containing the data from the previous fragments\n        for packet in codec.parse(extract_payload_from_nal_unit(data.raw)):\n            # use timestamp of previous packets\n            for av_frame in codec.decode(packet):  # type: ignore[attr-defined]\n                if frame_timestamp is None:\n                    raise ValueError(\"No timestamp available for the video frame.\")\n                yield VideoFrame(av_frame, frame_timestamp)\n\n        frame_timestamp = data.timestamp_unix_seconds\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame","title":"VideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A video frame with timestamp information.</p> <p>This class represents a video frame from the scene camera with associated timestamp information. The Class inherits VideoFrame from py.av library.</p> <p>Methods:</p> <ul> <li> <code>bgr_buffer</code>             \u2013              <p>Convert the video frame to a BGR buffer.</p> </li> <li> <code>to_ndarray</code>             \u2013              <p>Convert the video frame to a NumPy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>VideoFrame</code>)           \u2013            <p>The video frame.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: VideoFrame\n</code></pre> <p>The video frame.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.bgr_buffer","title":"bgr_buffer","text":"<pre><code>bgr_buffer() -&gt; BGRBuffer\n</code></pre> <p>Convert the video frame to a BGR buffer.</p> <p>This method converts the video frame to a BGR buffer, which is a NumPy array with the shape (height, width, 3) and dtype uint8. The BGR format is commonly used in computer vision applications.</p> <p>Returns:</p> <ul> <li> <code>BGRBuffer</code> (              <code>BGRBuffer</code> )          \u2013            <p>The BGR buffer as a NumPy array.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def bgr_buffer(self) -&gt; BGRBuffer:\n    \"\"\"Convert the video frame to a BGR buffer.\n\n    This method converts the video frame to a BGR buffer, which is a\n    NumPy array with the shape (height, width, 3) and dtype uint8.\n    The BGR format is commonly used in computer vision applications.\n\n    Returns:\n        BGRBuffer: The BGR buffer as a NumPy array.\n\n    \"\"\"\n    return self.to_ndarray(format=\"bgr24\")\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.VideoFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray(*args: Any, **kwargs: Any) -&gt; NDArray\n</code></pre> <p>Convert the video frame to a NumPy array.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def to_ndarray(self, *args: Any, **kwargs: Any) -&gt; npt.NDArray:\n    \"\"\"Convert the video frame to a NumPy array.\"\"\"\n    return self.av_frame.to_ndarray(*args, **kwargs)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.video.receive_video_frames","title":"receive_video_frames  <code>async</code>","text":"<pre><code>receive_video_frames(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[VideoFrame]\n</code></pre> <p>Receive video frames from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPVideoFrameStreamer and yields video frames.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPVideoFrameStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPVideoFrameStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>VideoFrame</code> (              <code>AsyncIterator[VideoFrame]</code> )          \u2013            <p>Parsed video frames.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>async def receive_video_frames(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[VideoFrame]:\n    \"\"\"Receive video frames from an RTSP stream.\n\n    This is a convenience function that creates an RTSPVideoFrameStreamer and yields\n    video frames.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPVideoFrameStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPVideoFrameStreamer.\n\n    Yields:\n        VideoFrame: Parsed video frames.\n\n    \"\"\"\n    async with RTSPVideoFrameStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(VideoFrame, datum)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.nal_unit.extract_payload_from_nal_unit","title":"extract_payload_from_nal_unit","text":"<pre><code>extract_payload_from_nal_unit(unit: ByteString) -&gt; ByteString\n</code></pre> <p>Extract and process payload from a Network Abstraction Layer (NAL) unit.</p> <p>This function extracts the payload from a NAL unit, handling various formats: - Prepends NAL unit start code to payload if necessary - Handles fragmented units (of type FU-A)</p> <p>The implementation follows RFC 3984 specifications for H.264 NAL units.</p> <p>Parameters:</p> <ul> <li> <code>unit</code>               (<code>ByteString</code>)           \u2013            <p>The NAL unit as a ByteString.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ByteString</code> (              <code>ByteString</code> )          \u2013            <p>The processed payload, potentially with start code prepended.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the first bit is not zero (forbidden_zero_bit).</p> </li> </ul> References <p>Inspired by https://github.com/runtheops/rtsp-rtp/blob/master/transport/primitives/nal_unit.py Rewritten due to license incompatibility. See RFC 3984 (https://www.ietf.org/rfc/rfc3984.txt) for detailed NAL unit specifications.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/nal_unit.py</code> <pre><code>def extract_payload_from_nal_unit(unit: ByteString) -&gt; ByteString:\n    \"\"\"Extract and process payload from a Network Abstraction Layer (NAL) unit.\n\n    This function extracts the payload from a NAL unit, handling various formats:\n    - Prepends NAL unit start code to payload if necessary\n    - Handles fragmented units (of type FU-A)\n\n    The implementation follows RFC 3984 specifications for H.264 NAL units.\n\n    Args:\n        unit: The NAL unit as a ByteString.\n\n    Returns:\n        ByteString: The processed payload, potentially with start code prepended.\n\n    Raises:\n        ValueError: If the first bit is not zero (forbidden_zero_bit).\n\n    References:\n        Inspired by https://github.com/runtheops/rtsp-rtp/blob/master/transport/primitives/nal_unit.py\n        Rewritten due to license incompatibility.\n        See RFC 3984 (https://www.ietf.org/rfc/rfc3984.txt) for detailed NAL unit\n        specifications.\n\n    \"\"\"\n    start_code = b\"\\x00\\x00\\x00\\x01\"\n    offset = 0\n    # slice to keep ByteString type; indexing would return int in native byte order\n    first_byte_raw = unit[:1]\n    # ensure network order for conversion to uint8\n    first_byte = struct.unpack(\"!B\", first_byte_raw)[0]\n    is_first_bit_one = (first_byte &amp; 0b10000000) != 0\n    if is_first_bit_one:\n        # See section 1.3 of https://www.ietf.org/rfc/rfc3984.txt\n        raise ValueError(\"First bit must be zero (forbidden_zero_bit)\")\n\n    nal_type = first_byte &amp; 0b00011111\n    if nal_type == 28:\n        # Fragmentation unit FU-A\n        # https://www.ietf.org/rfc/rfc3984.txt\n        # Section 5.8.\n        fu_header_raw = unit[1:2]  # get second byte while retaining ByteString type\n        fu_header = struct.unpack(\"!B\", fu_header_raw)[0]\n        offset = 2  # skip first two bytes\n\n        is_fu_start_bit_one = (fu_header &amp; 0b10000000) != 0\n        if is_fu_start_bit_one:\n            # reconstruct header of a non-fragmented NAL unit\n            first_byte_bits_1_to_3 = first_byte &amp; 0b11100000\n            # NAL type of non-fragmented NAL unit:\n            fu_header_bits_4_to_8 = fu_header &amp; 0b00011111\n            reconstructed_header = first_byte_bits_1_to_3 + fu_header_bits_4_to_8\n            start_code += bytes((reconstructed_header,))  # convert int to ByteString\n        else:\n            # do not prepend start code to payload since we are in the middle of a\n            # fragmented unit\n            start_code = b\"\"\n\n    return start_code + unit[offset:]\n</code></pre>"},{"location":"api/async/#raw-rtsp-data","title":"Raw RTSP Data","text":""},{"location":"api/async/#pupil_labs.realtime_api.streaming.base","title":"base","text":"<p>Classes:</p> <ul> <li> <code>RTSPData</code>           \u2013            <p>Container for RTSP data with timestamp information.</p> </li> <li> <code>RTSPRawStreamer</code>           \u2013            <p>Stream raw data from an RTSP source.</p> </li> <li> <code>SDPDataNotAvailableError</code>           \u2013            <p>Exception raised when SDP data is not available or incomplete.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>receive_raw_rtsp_data</code>             \u2013              <p>Receive raw data from an RTSP stream.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPData","title":"RTSPData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for RTSP data with timestamp information.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>raw</code>               (<code>ByteString</code>)           \u2013            <p>Raw binary data received from the RTSP stream.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since the Unix epoch from RTCP SR packets.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPData.raw","title":"raw  <code>instance-attribute</code>","text":"<pre><code>raw: ByteString\n</code></pre> <p>Raw binary data received from the RTSP stream.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since the Unix epoch from RTCP SR packets.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPRawStreamer","title":"RTSPRawStreamer","text":"<pre><code>RTSPRawStreamer(*args: Any, **kwargs: Any)\n</code></pre> <p>Stream raw data from an RTSP source.</p> <p>This class connects to an RTSP source and provides access to the raw data with timestamps synchronized to the device's clock.</p> <p>All constructor arguments are forwarded to the underlying aiortsp.rtsp.reader.RTSPReader.</p> <p>Methods:</p> <ul> <li> <code>receive</code>             \u2013              <p>Receive raw data from the RTSP stream.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>encoding</code>               (<code>str</code>)           \u2013            <p>Get the encoding of the RTSP stream.</p> </li> <li> <code>reader</code>               (<code>_WallclockRTSPReader</code>)           \u2013            <p>Get the underlying RTSP reader.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    self._reader = _WallclockRTSPReader(*args, **kwargs)\n    self._encoding = None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPRawStreamer.encoding","title":"encoding  <code>property</code>","text":"<pre><code>encoding: str\n</code></pre> <p>Get the encoding of the RTSP stream.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The encoding name in lowercase.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>SDPDataNotAvailableError</code>             \u2013            <p>If SDP data is missing or incomplete.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPRawStreamer.reader","title":"reader  <code>property</code>","text":"<pre><code>reader: _WallclockRTSPReader\n</code></pre> <p>Get the underlying RTSP reader.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.RTSPRawStreamer.receive","title":"receive  <code>async</code>","text":"<pre><code>receive() -&gt; AsyncIterator[RTSPData]\n</code></pre> <p>Receive raw data from the RTSP stream.</p> <p>This method yields RTSPData objects containing the raw data and corresponding timestamps.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>async def receive(self) -&gt; AsyncIterator[RTSPData]:\n    \"\"\"Receive raw data from the RTSP stream.\n\n    This method yields RTSPData objects containing the raw data and\n    corresponding timestamps.\n    \"\"\"\n    async for pkt in self.reader.iter_packets():\n        try:\n            timestamp_seconds = self.reader.absolute_timestamp_from_packet(pkt)\n        except _UnknownClockoffsetError:\n            # The absolute timestamp is not known yet.\n            # Waiting for the first RTCP SR packet...\n            continue\n        yield RTSPData(pkt.data, timestamp_seconds)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.SDPDataNotAvailableError","title":"SDPDataNotAvailableError","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when SDP data is not available or incomplete.</p> <p>This exception is raised when attempting to access SDP (Session Description Protocol) data that is not yet available or is missing required fields.</p>"},{"location":"api/async/#pupil_labs.realtime_api.streaming.base.receive_raw_rtsp_data","title":"receive_raw_rtsp_data  <code>async</code>","text":"<pre><code>receive_raw_rtsp_data(url: str, *args: Any, **kwargs: Any) -&gt; AsyncIterator[RTSPData]\n</code></pre> <p>Receive raw data from an RTSP stream.</p> <p>This is a convenience function that creates an RTSPRawStreamer and yields timestamped data packets.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>RTSP URL to connect to.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional positional arguments passed to RTSPRawStreamer.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to RTSPRawStreamer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>RTSPData</code> (              <code>AsyncIterator[RTSPData]</code> )          \u2013            <p>Timestamped RTSP data packets.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/base.py</code> <pre><code>async def receive_raw_rtsp_data(\n    url: str, *args: Any, **kwargs: Any\n) -&gt; AsyncIterator[RTSPData]:\n    \"\"\"Receive raw data from an RTSP stream.\n\n    This is a convenience function that creates an RTSPRawStreamer and yields\n    timestamped data packets.\n\n    Args:\n        url: RTSP URL to connect to.\n        *args: Additional positional arguments passed to RTSPRawStreamer.\n        **kwargs: Additional keyword arguments passed to RTSPRawStreamer.\n\n    Yields:\n        RTSPData: Timestamped RTSP data packets.\n\n    \"\"\"\n    async with RTSPRawStreamer(url, *args, **kwargs) as streamer:\n        async for datum in streamer.receive():\n            yield cast(RTSPData, datum)\n</code></pre>"},{"location":"api/async/#time-echo-protocol","title":"Time Echo Protocol","text":""},{"location":"api/async/#pupil_labs.realtime_api.time_echo","title":"time_echo","text":"<p>Manual time offset estimation via the Pupil Labs Time Echo protocol.</p> <p>The Realtime Network API host device timestamps its data with nanoseconds since the Unix epoch(January 1, 1970, 00:00:00 UTC). This clock is kept in sync by the operating system through NTP Network Time Protocol. For some use cases, this sync is not good enough. For more accurate time syncs, the Time Echo protocol allows the estimation of the direct offset between the host's and the client's clocks.</p> <p>The Time Echo protocol works in the following way:</p> <ol> <li>The API host (Neon / Pupil Invisible Companion app) opens a TCP server    at an specific port</li> <li>The client connects to the host address and port</li> <li>The client sends its current time (<code>t1</code>) in milliseconds as an uint64 in network    byte order to the host</li> <li>The host responds with the time echo, two uint64 values in network byte order</li> <li>The first value is equal to the sent client time (<code>t1</code>)</li> <li>The second value corresponds to the host's time in milliseconds (<code>tH</code>)</li> <li>The client calculates the duration of steps 3 and 4 (roundtrip time) by measuring the    client time before sending the request (<code>t1</code>) and after receiving the echo (<code>t2</code>)</li> <li>The protocol assumes that the transport duration is symmetric. It will assume that    <code>tH</code> was measured at the same time as the midpoint betwee <code>t1</code> and <code>t2</code>.</li> <li> <p>To calculate the offset between the host's and client's clock, we subtract <code>tH</code>    from the client's midpoint <code>(t1 + t2) / 2</code>::</p> <p>offset_ms = ((t1 + t2) / 2) - tH</p> </li> <li> <p>This measurement can be repeated multiple times to make the time offset estimation    more robust.</p> </li> </ol> <p>To convert client to host time, subtract the offset::</p> <p>host_time_ms = client_time_ms() - offset_ms</p> <p>This is particularly helpful to accurately timestamp local events, e.g. a stimulus presentation.</p> <p>To convert host to client time, add the offset::</p> <p>client_time_ms = host_time_ms() + offset_ms</p> <p>This is particularly helpful to convert the received data into the client's time domain.</p> <p>Classes:</p> <ul> <li> <code>Estimate</code>           \u2013            <p>Provides easy access to statistics over a collection of measurements.</p> </li> <li> <code>TimeEcho</code>           \u2013            <p>Measurement of a single time echo.</p> </li> <li> <code>TimeEchoEstimates</code>           \u2013            <p>Provides estimates for the roundtrip duration and time offsets.</p> </li> <li> <code>TimeOffsetEstimator</code>           \u2013            <p>Estimates the time offset between PC and Companion using the Time Echo protocol.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>time_ms</code>             \u2013              <p>Return milliseconds since <code>Unix epoch</code>_ (January 1, 1970, 00:00:00 UTC)</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>TimeFunction</code>           \u2013            <p>Returns time in milliseconds</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeFunction","title":"TimeFunction  <code>module-attribute</code>","text":"<pre><code>TimeFunction = Callable[[], int]\n</code></pre> <p>Returns time in milliseconds</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.Estimate","title":"Estimate","text":"<pre><code>Estimate(measurements: Iterable[int])\n</code></pre> <p>Provides easy access to statistics over a collection of measurements.</p> <p>This class calculates descriptive statistics (mean, standard deviation, median) over a collection of measurements.</p> <p>Attributes:</p> <ul> <li> <code>measurements</code>               (<code>tuple[int]</code>)           \u2013            <p>The raw measurements.</p> </li> <li> <code>mean</code>               (<code>float</code>)           \u2013            <p>Mean value of the measurements.</p> </li> <li> <code>std</code>               (<code>float</code>)           \u2013            <p>Standard deviation of the measurements.</p> </li> <li> <code>median</code>               (<code>float</code>)           \u2013            <p>Median value of the measurements.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/time_echo.py</code> <pre><code>def __init__(self, measurements: Iterable[int]) -&gt; None:\n    self.measurements = tuple(measurements)\n    self._mean = statistics.mean(self.measurements)\n    self._std = statistics.stdev(self.measurements)\n    self._median = statistics.median(self.measurements)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.Estimate.mean","title":"mean  <code>property</code>","text":"<pre><code>mean: float\n</code></pre> <p>Mean value of the measurements.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.Estimate.median","title":"median  <code>property</code>","text":"<pre><code>median: float\n</code></pre> <p>Median value of the measurements.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.Estimate.std","title":"std  <code>property</code>","text":"<pre><code>std: float\n</code></pre> <p>Standard deviation of the measurements.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEcho","title":"TimeEcho","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Measurement of a single time echo.</p> <p>Attributes:</p> <ul> <li> <code>roundtrip_duration_ms</code>               (<code>int</code>)           \u2013            <p>Round trip duration of the time echo, in milliseconds.</p> </li> <li> <code>time_offset_ms</code>               (<code>int</code>)           \u2013            <p>Time offset between host and client, in milliseconds.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEcho.roundtrip_duration_ms","title":"roundtrip_duration_ms  <code>instance-attribute</code>","text":"<pre><code>roundtrip_duration_ms: int\n</code></pre> <p>Round trip duration of the time echo, in milliseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEcho.time_offset_ms","title":"time_offset_ms  <code>instance-attribute</code>","text":"<pre><code>time_offset_ms: int\n</code></pre> <p>Time offset between host and client, in milliseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEchoEstimates","title":"TimeEchoEstimates","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Provides estimates for the roundtrip duration and time offsets.</p> <p>Attributes:</p> <ul> <li> <code>roundtrip_duration_ms</code>               (<code>Estimate</code>)           \u2013            <p>Statistics for roundtrip durations.</p> </li> <li> <code>time_offset_ms</code>               (<code>Estimate</code>)           \u2013            <p>Statistics for time offsets.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEchoEstimates.roundtrip_duration_ms","title":"roundtrip_duration_ms  <code>instance-attribute</code>","text":"<pre><code>roundtrip_duration_ms: Estimate\n</code></pre> <p>Statistics for roundtrip durations.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeEchoEstimates.time_offset_ms","title":"time_offset_ms  <code>instance-attribute</code>","text":"<pre><code>time_offset_ms: Estimate\n</code></pre> <p>Statistics for time offsets.</p>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeOffsetEstimator","title":"TimeOffsetEstimator","text":"<pre><code>TimeOffsetEstimator(address: str, port: int)\n</code></pre> <p>Estimates the time offset between PC and Companion using the Time Echo protocol.</p> <p>This class implements the Time Echo protocol to estimate the time offset between the client and host clocks.</p> <p>Attributes:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>Host address.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Host port for the Time Echo protocol.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>estimate</code>             \u2013              <p>Estimate the time offset between client and host.</p> </li> <li> <code>request_time_echo</code>             \u2013              <p>Request a time echo, measure the roundtrip time and estimate the time offset.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/time_echo.py</code> <pre><code>def __init__(self, address: str, port: int) -&gt; None:\n    self.address = address\n    self.port = port\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeOffsetEstimator.estimate","title":"estimate  <code>async</code>","text":"<pre><code>estimate(number_of_measurements: int = 100, sleep_between_measurements_seconds: float | None = None, time_fn_ms: TimeFunction = time_ms) -&gt; TimeEchoEstimates | None\n</code></pre> <p>Estimate the time offset between client and host.</p> <p>Parameters:</p> <ul> <li> <code>number_of_measurements</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of measurements to take.</p> </li> <li> <code>sleep_between_measurements_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional sleep time between measurements.</p> </li> <li> <code>time_fn_ms</code>               (<code>TimeFunction</code>, default:                   <code>time_ms</code> )           \u2013            <p>Function that returns the current time in milliseconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TimeEchoEstimates</code> (              <code>TimeEchoEstimates | None</code> )          \u2013            <p>Statistics for roundtrip durations and time offsets, or None if estimation failed.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/time_echo.py</code> <pre><code>async def estimate(\n    self,\n    number_of_measurements: int = 100,\n    sleep_between_measurements_seconds: float | None = None,\n    time_fn_ms: TimeFunction = time_ms,\n) -&gt; TimeEchoEstimates | None:\n    \"\"\"Estimate the time offset between client and host.\n\n    Args:\n        number_of_measurements: Number of measurements to take.\n        sleep_between_measurements_seconds: Optional sleep time between\n            measurements.\n        time_fn_ms: Function that returns the current time in milliseconds.\n\n    Returns:\n        TimeEchoEstimates: Statistics for roundtrip durations and time offsets,\n            or None if estimation failed.\n\n    \"\"\"\n    measurements = collections.defaultdict(list)\n\n    try:\n        logger.debug(f\"Connecting to {self.address}:{self.port}...\")\n        reader, writer = await asyncio.open_connection(self.address, self.port)\n    except ConnectionError:\n        logger.exception(\"Could not connect to Time Echo server\")\n        return None\n\n    try:\n        rt, offset = await self.request_time_echo(time_fn_ms, reader, writer)\n        logger.debug(\n            f\"Dropping first measurement (roundtrip: {rt} ms, offset: {offset} ms)\"\n        )\n        logger.info(f\"Measuring {number_of_measurements} times...\")\n        for _ in range(number_of_measurements):\n            try:\n                rt, offset = await self.request_time_echo(\n                    time_fn_ms, reader, writer\n                )\n                measurements[\"roundtrip\"].append(rt)\n                measurements[\"offset\"].append(offset)\n                if sleep_between_measurements_seconds is not None:\n                    await asyncio.sleep(sleep_between_measurements_seconds)\n            except ValueError as err:\n                logger.warning(err)\n    finally:\n        writer.close()\n        await writer.wait_closed()\n        logger.debug(f\"Connection closed {writer.is_closing()}\")\n\n    try:\n        estimates = TimeEchoEstimates(\n            roundtrip_duration_ms=Estimate(measurements[\"roundtrip\"]),\n            time_offset_ms=Estimate(measurements[\"offset\"]),\n        )\n    except statistics.StatisticsError:\n        logger.exception(\"Not enough valid samples were collected\")\n        return None\n\n    return estimates\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.TimeOffsetEstimator.request_time_echo","title":"request_time_echo  <code>async</code> <code>staticmethod</code>","text":"<pre><code>request_time_echo(time_fn_ms: TimeFunction, reader: StreamReader, writer: StreamWriter) -&gt; TimeEcho\n</code></pre> <p>Request a time echo, measure the roundtrip time and estimate the time offset.</p> <p>Parameters:</p> <ul> <li> <code>time_fn_ms</code>               (<code>TimeFunction</code>)           \u2013            <p>Function that returns the current time in milliseconds.</p> </li> <li> <code>reader</code>               (<code>StreamReader</code>)           \u2013            <p>Stream reader for receiving responses.</p> </li> <li> <code>writer</code>               (<code>StreamWriter</code>)           \u2013            <p>Stream writer for sending requests.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TimeEcho</code> (              <code>TimeEcho</code> )          \u2013            <p>Roundtrip duration and time offset.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the response is invalid.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/time_echo.py</code> <pre><code>@staticmethod\nasync def request_time_echo(\n    time_fn_ms: TimeFunction,\n    reader: asyncio.StreamReader,\n    writer: asyncio.StreamWriter,\n) -&gt; TimeEcho:\n    \"\"\"Request a time echo, measure the roundtrip time and estimate the time offset.\n\n    Args:\n        time_fn_ms: Function that returns the current time in milliseconds.\n        reader: Stream reader for receiving responses.\n        writer: Stream writer for sending requests.\n\n    Returns:\n        TimeEcho: Roundtrip duration and time offset.\n\n    Raises:\n        ValueError: If the response is invalid.\n\n    \"\"\"\n    before_ms = time_fn_ms()\n    before_ms_bytes = struct.pack(\"!Q\", before_ms)\n    writer.write(before_ms_bytes)\n    await writer.drain()\n    validation_server_ms_bytes = await reader.read(16)\n    after_ms = time_fn_ms()\n    if len(validation_server_ms_bytes) != 16:\n        raise ValueError(\n            \"Dropping invalid measurement. Expected response of length 16 \"\n            f\"(got {len(validation_server_ms_bytes)})\"\n        )\n    validation_ms, server_ms = struct.unpack(\"!QQ\", validation_server_ms_bytes)\n    logger.debug(\n        f\"Response: {validation_ms} {server_ms} ({validation_server_ms_bytes!r})\"\n    )\n    if validation_ms != before_ms:\n        raise ValueError(\n            \"Dropping invalid measurement. Expected validation timestamp: \"\n            f\"{before_ms} (got {validation_ms})\"\n        )\n    server_ts_in_client_time_ms = round((before_ms + after_ms) / 2)\n    offset_ms = server_ts_in_client_time_ms - server_ms\n    return TimeEcho(after_ms - before_ms, offset_ms)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.time_echo.time_ms","title":"time_ms","text":"<pre><code>time_ms() -&gt; int\n</code></pre> <p>Return milliseconds since <code>Unix epoch</code>_ (January 1, 1970, 00:00:00 UTC)</p> Source code in <code>src/pupil_labs/realtime_api/time_echo.py</code> <pre><code>def time_ms() -&gt; int:\n    \"\"\"Return milliseconds since `Unix epoch`_ (January 1, 1970, 00:00:00 UTC)\"\"\"\n    return time_ns() // 1_000_000\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models","title":"models","text":"<p>Classes:</p> <ul> <li> <code>APIPath</code>           \u2013            <p>API endpoint paths for the Realtime API.</p> </li> <li> <code>ConnectionType</code>           \u2013            <p>Enumeration of connection types.</p> </li> <li> <code>DiscoveredDeviceInfo</code>           \u2013            <p>Information about a discovered device on the network.</p> </li> <li> <code>Event</code>           \u2013            <p>Event information from the device.</p> </li> <li> <code>Hardware</code>           \u2013            <p>Information about the Hardware connected (eye tracker).</p> </li> <li> <code>InvalidTemplateAnswersError</code>           \u2013            <p>Exception raised when template answers fail validation.</p> </li> <li> <code>NetworkDevice</code>           \u2013            <p>Information about devices discovered by the host device, not the client.</p> </li> <li> <code>Phone</code>           \u2013            <p>Information relative to the Companion Device.</p> </li> <li> <code>Recording</code>           \u2013            <p>Information about a recording.</p> </li> <li> <code>Sensor</code>           \u2013            <p>Information about a sensor on the device.</p> </li> <li> <code>SensorName</code>           \u2013            <p>Enumeration of sensor types.</p> </li> <li> <code>Status</code>           \u2013            <p>Represents the Companion's Device full status</p> </li> <li> <code>Template</code>           \u2013            <p>Template Class for data collection.</p> </li> <li> <code>TemplateItem</code>           \u2013            <p>Individual item/ question in a Template.</p> </li> <li> <code>UnknownComponentError</code>           \u2013            <p>Exception raised when a component cannot be parsed.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>allow_empty</code>             \u2013              <p>Convert empty strings to None.</p> </li> <li> <code>make_template_answer_model_base</code>             \u2013              <p>Create a base class for template answer models.</p> </li> <li> <code>not_empty</code>             \u2013              <p>Validate that a string is not empty.</p> </li> <li> <code>option_in_allowed_values</code>             \u2013              <p>Validate that a value is in a list of allowed values.</p> </li> <li> <code>parse_component</code>             \u2013              <p>Initialize an explicitly modelled representation</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>Component</code>           \u2013            <p>Type annotation for :class:<code>Status</code> components.</p> </li> <li> <code>ComponentRaw</code>           \u2013            <p>Type annotation for json-parsed responses from the REST and Websocket API.</p> </li> <li> <code>TemplateDataFormat</code>           \u2013            <p>Format specification for template data.</p> </li> <li> <code>TemplateItemInputType</code>           \u2013            <p>Type of input data for a template item.</p> </li> <li> <code>TemplateItemWidgetType</code>           \u2013            <p>Type of widget to display for a template item.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Component","title":"Component  <code>module-attribute</code>","text":"<pre><code>Component = Phone | Hardware | Sensor | Recording | NetworkDevice | Event\n</code></pre> <p>Type annotation for :class:<code>Status</code> components.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.ComponentRaw","title":"ComponentRaw  <code>module-attribute</code>","text":"<pre><code>ComponentRaw = dict[str, Any]\n</code></pre> <p>Type annotation for json-parsed responses from the REST and Websocket API.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateDataFormat","title":"TemplateDataFormat  <code>module-attribute</code>","text":"<pre><code>TemplateDataFormat = Literal['api', 'simple']\n</code></pre> <p>Format specification for template data.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItemInputType","title":"TemplateItemInputType  <code>module-attribute</code>","text":"<pre><code>TemplateItemInputType = Literal['any', 'integer', 'float']\n</code></pre> <p>Type of input data for a template item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItemWidgetType","title":"TemplateItemWidgetType  <code>module-attribute</code>","text":"<pre><code>TemplateItemWidgetType = Literal['TEXT', 'PARAGRAPH', 'RADIO_LIST', 'CHECKBOX_LIST', 'SECTION_HEADER', 'PAGE_BREAK']\n</code></pre> <p>Type of widget to display for a template item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.APIPath","title":"APIPath","text":"<p>               Bases: <code>Enum</code></p> <p>API endpoint paths for the Realtime API.</p> <p>This enum defines the various API endpoints that can be accessed through the Realtime API.</p> <p>Methods:</p> <ul> <li> <code>full_address</code>             \u2013              <p>Construct a full URL for this API endpoint.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.APIPath.full_address","title":"full_address","text":"<pre><code>full_address(address: str, port: int, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full URL for this API endpoint.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def full_address(\n    self, address: str, port: int, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full URL for this API endpoint.\"\"\"\n    return f\"{protocol}://{address}:{port}\" + prefix + self.value\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.ConnectionType","title":"ConnectionType","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of connection types.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.DiscoveredDeviceInfo","title":"DiscoveredDeviceInfo","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about a discovered device on the network.</p> <p>Attributes:</p> <ul> <li> <code>addresses</code>               (<code>list[str]</code>)           \u2013            <p>IP addresses, e.g. <code>['192.168.0.2']</code>.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Full mDNS service name.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Port number, e.g. <code>8080</code>.</p> </li> <li> <code>server</code>               (<code>str</code>)           \u2013            <p>mDNS server name. e.g. <code>'neon.local.' or 'pi.local.'</code>.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.DiscoveredDeviceInfo.addresses","title":"addresses  <code>instance-attribute</code>","text":"<pre><code>addresses: list[str]\n</code></pre> <p>IP addresses, e.g. <code>['192.168.0.2']</code>.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.DiscoveredDeviceInfo.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Full mDNS service name.</p> <p>Follows <code>'PI monitor:&lt;phone name&gt;:&lt;hardware id&gt;._http._tcp.local.'</code> naming pattern.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.DiscoveredDeviceInfo.port","title":"port  <code>instance-attribute</code>","text":"<pre><code>port: int\n</code></pre> <p>Port number, e.g. <code>8080</code>.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.DiscoveredDeviceInfo.server","title":"server  <code>instance-attribute</code>","text":"<pre><code>server: str\n</code></pre> <p>mDNS server name. e.g. <code>'neon.local.' or 'pi.local.'</code>.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event","title":"Event","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Event information from the device.</p> <p>Methods:</p> <ul> <li> <code>from_dict</code>             \u2013              <p>Create an Event from a dictionary.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the event time as a datetime object.</p> </li> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>recording_id</code>               (<code>str | None</code>)           \u2013            <p>ID of the recording this event belongs to.</p> </li> <li> <code>timestamp</code>               (<code>int</code>)           \u2013            <p>Unix epoch timestamp in nanoseconds.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the event time as a datetime object.</p> <p>Returns:</p> <ul> <li> <code>datetime</code> (              <code>datetime</code> )          \u2013            <p>Event time as a Python datetime.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str | None\n</code></pre> <p>Name of the event.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event.recording_id","title":"recording_id  <code>instance-attribute</code>","text":"<pre><code>recording_id: str | None\n</code></pre> <p>ID of the recording this event belongs to.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: int\n</code></pre> <p>Unix epoch timestamp in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Event.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(event_dict: dict[str, Any]) -&gt; Event\n</code></pre> <p>Create an Event from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>event_dict</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary containing event data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>New Event instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>@classmethod\ndef from_dict(cls, event_dict: dict[str, Any]) -&gt; \"Event\":\n    \"\"\"Create an Event from a dictionary.\n\n    Args:\n        event_dict: Dictionary containing event data.\n\n    Returns:\n        Event: New Event instance.\n\n    \"\"\"\n    return cls(\n        name=event_dict.get(\"name\"),\n        recording_id=event_dict.get(\"recording_id\"),\n        timestamp=event_dict[\"timestamp\"],\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Hardware","title":"Hardware","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about the Hardware connected (eye tracker).</p> <p>Attributes:</p> <ul> <li> <code>glasses_serial</code>               (<code>str</code>)           \u2013            <p>Serial number of the glasses. For Pupil Invisible devices.</p> </li> <li> <code>module_serial</code>               (<code>str</code>)           \u2013            <p>Serial number of the module. For Neon devices.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Hardware version.</p> </li> <li> <code>world_camera_serial</code>               (<code>str</code>)           \u2013            <p>Serial number of the world camera. For Pupil Invisible devices.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Hardware.glasses_serial","title":"glasses_serial  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>glasses_serial: str = 'unknown'\n</code></pre> <p>Serial number of the glasses. For Pupil Invisible devices.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Hardware.module_serial","title":"module_serial  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>module_serial: str = 'unknown'\n</code></pre> <p>Serial number of the module. For Neon devices.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Hardware.version","title":"version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>version: str = 'unknown'\n</code></pre> <p>Hardware version. 1-&gt; Pupil Invisible 2-&gt; Neon</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Hardware.world_camera_serial","title":"world_camera_serial  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>world_camera_serial: str = 'unknown'\n</code></pre> <p>Serial number of the world camera. For Pupil Invisible devices.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.InvalidTemplateAnswersError","title":"InvalidTemplateAnswersError","text":"<pre><code>InvalidTemplateAnswersError(template: Template | TemplateItem, answers: dict[str, Any], errors: list[ErrorDetails])\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when template answers fail validation.</p> <p>Attributes:</p> <ul> <li> <code>template</code>               (<code>Template | TemplateItem</code>)           \u2013            <p>Template or item that failed validation.</p> </li> <li> <code>errors</code>               (<code>list[dict]</code>)           \u2013            <p>List of validation errors.</p> </li> <li> <code>answers</code>               (<code>dict</code>)           \u2013            <p>The answers that failed validation.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def __init__(\n    self,\n    template: Template | TemplateItem,\n    answers: dict[str, Any],\n    errors: list[ErrorDetails],\n) -&gt; None:\n    self.template = template\n    self.errors = errors\n    self.answers = answers\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.NetworkDevice","title":"NetworkDevice","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about devices discovered by the host device, not the client.</p> <p>This class represents device information made available via the websocket update connection by the host device (exposed via <code>pupil_labs.realtime_api.device.Device.status_updates</code>. Devices discovered directly by this library are represented as DiscoveredDeviceInfo and returned by discover_devices Network.</p> <p>Attributes:</p> <ul> <li> <code>connected</code>               (<code>bool</code>)           \u2013            <p>Whether the device is connected.</p> </li> <li> <code>device_id</code>               (<code>str</code>)           \u2013            <p>Unique device identifier.</p> </li> <li> <code>device_name</code>               (<code>str</code>)           \u2013            <p>Human-readable device name (can be modified by the user in the Companion App</p> </li> <li> <code>ip</code>               (<code>str</code>)           \u2013            <p>IP address of the device.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.NetworkDevice.connected","title":"connected  <code>instance-attribute</code>","text":"<pre><code>connected: bool\n</code></pre> <p>Whether the device is connected.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.NetworkDevice.device_id","title":"device_id  <code>instance-attribute</code>","text":"<pre><code>device_id: str\n</code></pre> <p>Unique device identifier.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.NetworkDevice.device_name","title":"device_name  <code>instance-attribute</code>","text":"<pre><code>device_name: str\n</code></pre> <p>Human-readable device name (can be modified by the user in the Companion App settings).</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.NetworkDevice.ip","title":"ip  <code>instance-attribute</code>","text":"<pre><code>ip: str\n</code></pre> <p>IP address of the device.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone","title":"Phone","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information relative to the Companion Device.</p> <p>Attributes:</p> <ul> <li> <code>battery_level</code>               (<code>int</code>)           \u2013            <p>Battery percentage (0-100)</p> </li> <li> <code>battery_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Battery state.</p> </li> <li> <code>device_id</code>               (<code>str</code>)           \u2013            <p>Unique device identifier.</p> </li> <li> <code>device_name</code>               (<code>str</code>)           \u2013            <p>Human-readable device name.</p> </li> <li> <code>ip</code>               (<code>str</code>)           \u2013            <p>IP address of the phone.</p> </li> <li> <code>memory</code>               (<code>int</code>)           \u2013            <p>Available memory in bytes.</p> </li> <li> <code>memory_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Memory state.</p> </li> <li> <code>time_echo_port</code>               (<code>int | None</code>)           \u2013            <p>Port for time synchronization, if available.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.battery_level","title":"battery_level  <code>instance-attribute</code>","text":"<pre><code>battery_level: int\n</code></pre> <p>Battery percentage (0-100)</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.battery_state","title":"battery_state  <code>instance-attribute</code>","text":"<pre><code>battery_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Battery state.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.device_id","title":"device_id  <code>instance-attribute</code>","text":"<pre><code>device_id: str\n</code></pre> <p>Unique device identifier.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.device_name","title":"device_name  <code>instance-attribute</code>","text":"<pre><code>device_name: str\n</code></pre> <p>Human-readable device name.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.ip","title":"ip  <code>instance-attribute</code>","text":"<pre><code>ip: str\n</code></pre> <p>IP address of the phone.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory: int\n</code></pre> <p>Available memory in bytes.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.memory_state","title":"memory_state  <code>instance-attribute</code>","text":"<pre><code>memory_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Memory state.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Phone.time_echo_port","title":"time_echo_port  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_echo_port: int | None = None\n</code></pre> <p>Port for time synchronization, if available.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording","title":"Recording","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about a recording.</p> <p>Attributes:</p> <ul> <li> <code>action</code>               (<code>str</code>)           \u2013            <p>Current recording action.</p> </li> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>Unique recording identifier.</p> </li> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>Status message.</p> </li> <li> <code>rec_duration_ns</code>               (<code>int</code>)           \u2013            <p>Recording duration in nanoseconds.</p> </li> <li> <code>rec_duration_seconds</code>               (<code>float</code>)           \u2013            <p>Get the recording duration in seconds.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: str\n</code></pre> <p>Current recording action.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Unique recording identifier.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>Status message.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording.rec_duration_ns","title":"rec_duration_ns  <code>instance-attribute</code>","text":"<pre><code>rec_duration_ns: int\n</code></pre> <p>Recording duration in nanoseconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Recording.rec_duration_seconds","title":"rec_duration_seconds  <code>property</code>","text":"<pre><code>rec_duration_seconds: float\n</code></pre> <p>Get the recording duration in seconds.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor","title":"Sensor","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about a sensor on the device.</p> <p>Attributes:</p> <ul> <li> <code>conn_type</code>               (<code>str</code>)           \u2013            <p>Connection type (see ConnectionType Enum).</p> </li> <li> <code>connected</code>               (<code>bool</code>)           \u2013            <p>Whether the sensor is connected.</p> </li> <li> <code>ip</code>               (<code>str | None</code>)           \u2013            <p>IP address of the sensor.</p> </li> <li> <code>params</code>               (<code>str | None</code>)           \u2013            <p>Additional parameters.</p> </li> <li> <code>port</code>               (<code>int | None</code>)           \u2013            <p>Port number.</p> </li> <li> <code>protocol</code>               (<code>str</code>)           \u2013            <p>Protocol used for the connection.</p> </li> <li> <code>sensor</code>               (<code>str</code>)           \u2013            <p>Sensor type (see Name Enum).</p> </li> <li> <code>stream_error</code>               (<code>bool</code>)           \u2013            <p>Whether the stream errors.</p> </li> <li> <code>url</code>               (<code>str | None</code>)           \u2013            <p>Get the URL for accessing this sensor's data stream.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.conn_type","title":"conn_type  <code>instance-attribute</code>","text":"<pre><code>conn_type: str\n</code></pre> <p>Connection type (see ConnectionType Enum).</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.connected","title":"connected  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>connected: bool = False\n</code></pre> <p>Whether the sensor is connected.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.ip","title":"ip  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ip: str | None = None\n</code></pre> <p>IP address of the sensor.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: str | None = None\n</code></pre> <p>Additional parameters.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.port","title":"port  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>port: int | None = None\n</code></pre> <p>Port number.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.protocol","title":"protocol  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>protocol: str = 'rtsp'\n</code></pre> <p>Protocol used for the connection.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.sensor","title":"sensor  <code>instance-attribute</code>","text":"<pre><code>sensor: str\n</code></pre> <p>Sensor type (see Name Enum).</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.stream_error","title":"stream_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stream_error: bool = True\n</code></pre> <p>Whether the stream errors.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Sensor.url","title":"url  <code>property</code>","text":"<pre><code>url: str | None\n</code></pre> <p>Get the URL for accessing this sensor's data stream.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>str | None: URL if connected, None otherwise.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.SensorName","title":"SensorName","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of sensor types.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status","title":"Status  <code>dataclass</code>","text":"<pre><code>Status(phone: Phone, hardware: Hardware, sensors: list[Sensor], recording: Recording | None)\n</code></pre> <p>Represents the Companion's Device full status</p> <p>Methods:</p> <ul> <li> <code>direct_eye_events_sensor</code>             \u2013              <p>Get blinks, fixations sensor.</p> </li> <li> <code>direct_eyes_sensor</code>             \u2013              <p>Get the eye camera sensor with direct connection. Only available on Neon.</p> </li> <li> <code>direct_gaze_sensor</code>             \u2013              <p>Get the gaze sensor with direct connection.</p> </li> <li> <code>direct_imu_sensor</code>             \u2013              <p>Get the IMU sensor with direct connection.</p> </li> <li> <code>direct_world_sensor</code>             \u2013              <p>Get the scene camera sensor with direct connection.</p> </li> <li> <code>from_dict</code>             \u2013              <p>Create a Status from a list of raw components.</p> </li> <li> <code>matching_sensors</code>             \u2013              <p>Find sensors matching specified criteria.</p> </li> <li> <code>update</code>             \u2013              <p>Update Component.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>hardware</code>               (<code>Hardware</code>)           \u2013            <p>Information about glasses connected, won't be present if not connected</p> </li> <li> <code>phone</code>               (<code>Phone</code>)           \u2013            <p>Information about the connected phone. Always present.</p> </li> <li> <code>recording</code>               (<code>Recording | None</code>)           \u2013            <p>Current recording, if any.</p> </li> <li> <code>sensors</code>               (<code>list[Sensor]</code>)           \u2013            <p>\"List of sensor information.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.hardware","title":"hardware  <code>instance-attribute</code>","text":"<pre><code>hardware: Hardware\n</code></pre> <p>Information about glasses connected, won't be present if not connected</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.phone","title":"phone  <code>instance-attribute</code>","text":"<pre><code>phone: Phone\n</code></pre> <p>Information about the connected phone. Always present.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.recording","title":"recording  <code>instance-attribute</code>","text":"<pre><code>recording: Recording | None\n</code></pre> <p>Current recording, if any.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.sensors","title":"sensors  <code>instance-attribute</code>","text":"<pre><code>sensors: list[Sensor]\n</code></pre> <p>\"List of sensor information.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.direct_eye_events_sensor","title":"direct_eye_events_sensor","text":"<pre><code>direct_eye_events_sensor() -&gt; Sensor | None\n</code></pre> <p>Get blinks, fixations sensor.</p> <p>Only available on Neon with Companion App version 2.9 or newer.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_eye_events_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get blinks, fixations _sensor_.\n\n    Only available on Neon with Companion App version 2.9 or newer.\n    \"\"\"\n    return next(\n        self.matching_sensors(SensorName.EYE_EVENTS, ConnectionType.DIRECT),\n        Sensor(\n            sensor=SensorName.EYE_EVENTS.value,\n            conn_type=ConnectionType.DIRECT.value,\n        ),\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.direct_eyes_sensor","title":"direct_eyes_sensor","text":"<pre><code>direct_eyes_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the eye camera sensor with direct connection. Only available on Neon.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_eyes_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the eye camera sensor with direct connection. Only available on Neon.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.EYES, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.EYES.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.direct_gaze_sensor","title":"direct_gaze_sensor","text":"<pre><code>direct_gaze_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the gaze sensor with direct connection.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_gaze_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the gaze sensor with direct connection.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.GAZE, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.GAZE.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.direct_imu_sensor","title":"direct_imu_sensor","text":"<pre><code>direct_imu_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the IMU sensor with direct connection.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_imu_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the IMU sensor with direct connection.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.IMU, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.IMU.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.direct_world_sensor","title":"direct_world_sensor","text":"<pre><code>direct_world_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the scene camera sensor with direct connection.</p> Note <p>Pupil Invisible devices, the world camera can be detached</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_world_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the scene camera sensor with direct connection.\n\n    Note:\n        Pupil Invisible devices, the world camera can be detached\n\n    \"\"\"\n    return next(\n        self.matching_sensors(SensorName.WORLD, ConnectionType.DIRECT),\n        Sensor(\n            sensor=SensorName.WORLD.value, conn_type=ConnectionType.DIRECT.value\n        ),\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(status_json_result: list[ComponentRaw]) -&gt; Status\n</code></pre> <p>Create a Status from a list of raw components.</p> <p>Parameters:</p> <ul> <li> <code>status_json_result</code>               (<code>list[ComponentRaw]</code>)           \u2013            <p>List of raw component dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Status</code> (              <code>Status</code> )          \u2013            <p>New Status instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>@classmethod\ndef from_dict(cls, status_json_result: list[ComponentRaw]) -&gt; \"Status\":\n    \"\"\"Create a Status from a list of raw components.\n\n    Args:\n        status_json_result: List of raw component dictionaries.\n\n    Returns:\n        Status: New Status instance.\n\n    \"\"\"\n    phone = None\n    recording = None\n    hardware = Hardware()\n    sensors = []\n    for dct in status_json_result:\n        try:\n            component = parse_component(dct)\n        except UnknownComponentError:\n            logger.warning(f\"Dropping unknown component: {dct}\")\n            continue\n        if isinstance(component, Phone):\n            phone = component\n        elif isinstance(component, Hardware):\n            hardware = component\n        elif isinstance(component, Sensor):\n            sensors.append(component)\n        elif isinstance(component, Recording):\n            recording = component\n        elif isinstance(component, NetworkDevice):\n            pass  # no need to handle NetworkDevice updates here\n        else:\n            logger.warning(f\"Unknown model class: {type(component).__name__}\")\n    sensors.sort(key=lambda s: (not s.connected, s.conn_type, s.sensor))\n    if not phone:\n        raise ValueError(\"Status data must include a 'Phone' component.\")\n    return cls(phone, hardware, sensors, recording)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.matching_sensors","title":"matching_sensors","text":"<pre><code>matching_sensors(name: SensorName, connection: ConnectionType) -&gt; Iterator[Sensor]\n</code></pre> <p>Find sensors matching specified criteria.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>SensorName</code>)           \u2013            <p>Sensor name to match, or ANY to match any name.</p> </li> <li> <code>connection</code>               (<code>ConnectionType</code>)           \u2013            <p>Connection type to match, or ANY to match any type.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Sensor</code> (              <code>Sensor</code> )          \u2013            <p>Sensors matching the criteria.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def matching_sensors(\n    self, name: SensorName, connection: ConnectionType\n) -&gt; Iterator[Sensor]:\n    \"\"\"Find sensors matching specified criteria.\n\n    Args:\n        name: Sensor name to match, or ANY to match any name.\n        connection: Connection type to match, or ANY to match any type.\n\n    Yields:\n        Sensor: Sensors matching the criteria.\n\n    \"\"\"\n    for sensor in self.sensors:\n        if name is not SensorName.ANY and sensor.sensor != name.value:\n            continue\n        if (\n            connection is not ConnectionType.ANY\n            and sensor.conn_type != connection.value\n        ):\n            continue\n        yield sensor\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Status.update","title":"update","text":"<pre><code>update(component: Component) -&gt; None\n</code></pre> <p>Update Component.</p> <p>Parameters:</p> <ul> <li> <code>component</code>               (<code>Component</code>)           \u2013            <p>Component to update.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def update(self, component: Component) -&gt; None:\n    \"\"\"Update Component.\n\n    Args:\n        component: Component to update.\n\n    \"\"\"\n    if isinstance(component, Phone):\n        self.phone = component\n    elif isinstance(component, Hardware):\n        self.hardware = component\n    elif isinstance(component, Recording):\n        self.recording = component\n    elif isinstance(component, Sensor):\n        for idx, sensor in enumerate(self.sensors):\n            if (\n                sensor.sensor == component.sensor\n                and sensor.conn_type == component.conn_type\n            ):\n                self.sensors[idx] = component\n                break\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template","title":"Template","text":"<p>Template Class for data collection.</p> <p>Methods:</p> <ul> <li> <code>convert_from_api_to_simple_format</code>             \u2013              <p>Convert data from API format to simple format.</p> </li> <li> <code>convert_from_simple_to_api_format</code>             \u2013              <p>Convert data from simple format to API format.</p> </li> <li> <code>get_question_by_id</code>             \u2013              <p>Get a template item by ID.</p> </li> <li> <code>validate_answers</code>             \u2013              <p>Validate answers for this Template.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>archived_at</code>               (<code>datetime | None</code>)           \u2013            <p>Archival timestamp (if archived).</p> </li> <li> <code>created_at</code>               (<code>datetime</code>)           \u2013            <p>Creation timestamp.</p> </li> <li> <code>description</code>               (<code>str | None</code>)           \u2013            <p>Template description.</p> </li> <li> <code>id</code>               (<code>UUID</code>)           \u2013            <p>Unique identifier.</p> </li> <li> <code>is_default_template</code>               (<code>bool</code>)           \u2013            <p>Whether this is the default template for the Workspace</p> </li> <li> <code>items</code>               (<code>list[TemplateItem]</code>)           \u2013            <p>List of template items.</p> </li> <li> <code>label_ids</code>               (<code>list[UUID]</code>)           \u2013            <p>Associated label IDs.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Template name.</p> </li> <li> <code>published_at</code>               (<code>datetime | None</code>)           \u2013            <p>Publication timestamp.</p> </li> <li> <code>recording_ids</code>               (<code>list[UUID] | None</code>)           \u2013            <p>Associated recording IDs.</p> </li> <li> <code>recording_name_format</code>               (<code>list[str]</code>)           \u2013            <p>Format for recording name.</p> </li> <li> <code>updated_at</code>               (<code>datetime</code>)           \u2013            <p>Last update timestamp.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.archived_at","title":"archived_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>archived_at: datetime | None = None\n</code></pre> <p>Archival timestamp (if archived).</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre> <p>Creation timestamp.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Template description.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: UUID\n</code></pre> <p>Unique identifier.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.is_default_template","title":"is_default_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_default_template: bool = True\n</code></pre> <p>Whether this is the default template for the Workspace</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.items","title":"items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>items: list[TemplateItem] = field(default_factory=list)\n</code></pre> <p>List of template items.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.label_ids","title":"label_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_ids: list[UUID] = field(default_factory=list, metadata={'readonly': True})\n</code></pre> <p>Associated label IDs.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Template name.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.published_at","title":"published_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>published_at: datetime | None = None\n</code></pre> <p>Publication timestamp.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.recording_ids","title":"recording_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>recording_ids: list[UUID] | None = None\n</code></pre> <p>Associated recording IDs.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.recording_name_format","title":"recording_name_format  <code>instance-attribute</code>","text":"<pre><code>recording_name_format: list[str]\n</code></pre> <p>Format for recording name.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: datetime\n</code></pre> <p>Last update timestamp.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.convert_from_api_to_simple_format","title":"convert_from_api_to_simple_format","text":"<pre><code>convert_from_api_to_simple_format(data: dict[str, list[str]]) -&gt; dict[str, Any]\n</code></pre> <p>Convert data from API format to simple format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Data in API format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, Any]</code> )          \u2013            <p>Data in simple format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_api_to_simple_format(\n    self, data: dict[str, list[str]]\n) -&gt; dict[str, Any]:\n    \"\"\"Convert data from API format to simple format.\n\n    Args:\n        data: Data in API format.\n\n    Returns:\n        dict: Data in simple format.\n\n    \"\"\"\n    simple_format = {}\n    for question_id, value in data.items():\n        question = self.get_question_by_id(question_id)\n        if question is None:\n            logger.warning(\n                f\"Skipping unknown question ID '{question_id}' during API to \"\n                f\"simple conversion.\"\n            )\n            continue\n        processed_value: Any\n        if question.widget_type in {\"CHECKBOX_LIST\", \"RADIO_LIST\"}:\n            if question.choices is None:\n                logger.warning(\n                    f\"Question {question_id} (type {question.widget_type}) \"\n                    f\"has no choices defined.\"\n                )\n                processed_value = []\n            elif value == [\"\"] and \"\" not in question.choices:\n                processed_value = []\n        else:\n            if not value:\n                value = [\"\"]\n\n            value_str = value[0]\n            if question.input_type != \"any\":\n                processed_value = (\n                    None if value_str == \"\" else question._value_type(value_str)\n                )\n            else:\n                processed_value = value\n\n        simple_format[question_id] = processed_value\n    return simple_format\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.convert_from_simple_to_api_format","title":"convert_from_simple_to_api_format","text":"<pre><code>convert_from_simple_to_api_format(data: dict[str, Any]) -&gt; dict[str, list[Any]]\n</code></pre> <p>Convert data from simple format to API format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Data in simple format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, list[Any]]</code> )          \u2013            <p>Data in API format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_simple_to_api_format(\n    self, data: dict[str, Any]\n) -&gt; dict[str, list[Any]]:\n    \"\"\"Convert data from simple format to API format.\n\n    Args:\n        data: Data in simple format.\n\n    Returns:\n        dict: Data in API format.\n\n    \"\"\"\n    api_format = {}\n    for question_id, value in data.items():\n        if value is None:\n            value = \"\"\n        if not isinstance(value, list):\n            value = [value]\n\n        api_format[question_id] = value\n    return api_format\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.get_question_by_id","title":"get_question_by_id","text":"<pre><code>get_question_by_id(question_id: str | UUID) -&gt; TemplateItem | None\n</code></pre> <p>Get a template item by ID.</p> <p>Parameters:</p> <ul> <li> <code>question_id</code>               (<code>str | UUID</code>)           \u2013            <p>ID of the template item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TemplateItem | None</code>           \u2013            <p>TemplateItem | None: The template item, or None if not found.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def get_question_by_id(self, question_id: str | UUID) -&gt; TemplateItem | None:\n    \"\"\"Get a template item by ID.\n\n    Args:\n        question_id: ID of the template item.\n\n    Returns:\n        TemplateItem | None: The template item, or None if not found.\n\n    \"\"\"\n    for item in self.items:\n        if str(item.id) == str(question_id):\n            return item\n    return None\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.Template.validate_answers","title":"validate_answers","text":"<pre><code>validate_answers(answers: dict[str, list[str]], template_format: TemplateDataFormat, raise_exception: bool = True) -&gt; list[ErrorDetails]\n</code></pre> <p>Validate answers for this Template.</p> <p>Parameters:</p> <ul> <li> <code>answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Answers to validate.</p> </li> <li> <code>raise_exception</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to raise an exception on validation failure.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>)           \u2013            <p>Format of the answers (\"simple\" or \"api\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[ErrorDetails]</code> )          \u2013            <p>List of validation errors, or empty list if validation succeeded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidTemplateAnswersError</code>             \u2013            <p>If validation fails and raise_exception is</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def validate_answers(\n    self,\n    answers: dict[str, list[str]],\n    template_format: TemplateDataFormat,\n    raise_exception: bool = True,\n) -&gt; list[ErrorDetails]:\n    \"\"\"Validate answers for this Template.\n\n    Args:\n        answers: Answers to validate.\n        raise_exception: Whether to raise an exception on validation failure.\n        template_format: Format of the answers (\"simple\" or \"api\").\n\n    Returns:\n        list: List of validation errors, or empty list if validation succeeded.\n\n    Raises:\n        InvalidTemplateAnswersError: If validation fails and raise_exception is\n        True.\n\n    \"\"\"\n    AnswerModel = self._create_answer_model(template_format=template_format)\n    errors = []\n    try:\n        AnswerModel(**answers)\n    except ValidationError as e:\n        errors = e.errors()\n\n    for error in errors:\n        question_id = error[\"loc\"][0]\n        question = self.get_question_by_id(str(question_id))\n        if question:\n            error[\"question\"] = asdict(question)  # type: ignore[typeddict-unknown-key]\n\n    if errors and raise_exception:\n        raise InvalidTemplateAnswersError(self, answers, errors)\n    return errors\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem","title":"TemplateItem","text":"<p>Individual item/ question in a Template.</p> <p>Methods:</p> <ul> <li> <code>validate_answer</code>             \u2013              <p>Validate an answer for this template item.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>choices</code>               (<code>list[str] | None</code>)           \u2013            <p>Available choices for selection items (e.g., radio or checkbox).</p> </li> <li> <code>help_text</code>               (<code>str | None</code>)           \u2013            <p>Help or description text for the item.</p> </li> <li> <code>id</code>               (<code>UUID</code>)           \u2013            <p>Unique identifier for the template item.</p> </li> <li> <code>input_type</code>               (<code>TemplateItemInputType</code>)           \u2013            <p>Type of input data for this item.</p> </li> <li> <code>required</code>               (<code>bool</code>)           \u2013            <p>Whether the item is required or not.</p> </li> <li> <code>title</code>               (<code>str</code>)           \u2013            <p>Title or question text for the template item.</p> </li> <li> <code>widget_type</code>               (<code>TemplateItemWidgetType</code>)           \u2013            <p>Type of widget to display for this item.</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: list[str] | None\n</code></pre> <p>Available choices for selection items (e.g., radio or checkbox).</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.help_text","title":"help_text  <code>instance-attribute</code>","text":"<pre><code>help_text: str | None\n</code></pre> <p>Help or description text for the item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: UUID\n</code></pre> <p>Unique identifier for the template item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.input_type","title":"input_type  <code>instance-attribute</code>","text":"<pre><code>input_type: TemplateItemInputType\n</code></pre> <p>Type of input data for this item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.required","title":"required  <code>instance-attribute</code>","text":"<pre><code>required: bool\n</code></pre> <p>Whether the item is required or not.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.title","title":"title  <code>instance-attribute</code>","text":"<pre><code>title: str\n</code></pre> <p>Title or question text for the template item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.widget_type","title":"widget_type  <code>instance-attribute</code>","text":"<pre><code>widget_type: TemplateItemWidgetType\n</code></pre> <p>Type of widget to display for this item.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.TemplateItem.validate_answer","title":"validate_answer","text":"<pre><code>validate_answer(answer: Any, template_format: TemplateDataFormat = 'simple', raise_exception: bool = True) -&gt; list[ErrorDetails]\n</code></pre> <p>Validate an answer for this template item.</p> <p>Parameters:</p> <ul> <li> <code>answer</code>               (<code>Any</code>)           \u2013            <p>Answer to validate.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the template (\"simple\" or \"api\").</p> </li> <li> <code>raise_exception</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to raise an exception on validation failure.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[ErrorDetails]</code> )          \u2013            <p>List of validation errors, or empty list if validation succeeded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidTemplateAnswersError</code>             \u2013            <p>If validation fails and raise_exception is</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def validate_answer(\n    self,\n    answer: Any,\n    template_format: TemplateDataFormat = \"simple\",\n    raise_exception: bool = True,\n) -&gt; list[ErrorDetails]:\n    \"\"\"Validate an answer for this template item.\n\n    Args:\n        answer: Answer to validate.\n        template_format: Format of the template (\"simple\" or \"api\").\n        raise_exception: Whether to raise an exception on validation failure.\n\n    Returns:\n        list: List of validation errors, or empty list if validation succeeded.\n\n    Raises:\n        InvalidTemplateAnswersError: If validation fails and raise_exception is\n        True.\n\n    \"\"\"\n    validator = self._pydantic_validator(template_format=template_format)\n    if validator is None:\n        logger.warning(\n            f\"Skipping validation for {self.widget_type} item: {self.title}\"\n        )\n        return []\n    answers_model_def = {str(self.id): validator}\n    model = create_model(\n        f\"TemplateItem_{self.id}_Answer\",\n        **answers_model_def,\n        __config__=ConfigDict(extra=\"forbid\"),\n    )  # type: ignore[call-overload]\n    errors = []\n    try:\n        model.__pydantic_validator__.validate_assignment(\n            model.model_construct(), str(self.id), answer\n        )\n    except ValidationError as e:\n        errors = e.errors()\n\n    if errors and raise_exception:\n        raise InvalidTemplateAnswersError(self, answers_model_def, errors)\n    return errors\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.UnknownComponentError","title":"UnknownComponentError","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when a component cannot be parsed.</p>"},{"location":"api/async/#pupil_labs.realtime_api.models.allow_empty","title":"allow_empty","text":"<pre><code>allow_empty(v: str) -&gt; str | None\n</code></pre> <p>Convert empty strings to None.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def allow_empty(v: str) -&gt; str | None:\n    \"\"\"Convert empty strings to None.\"\"\"\n    if v == \"\":\n        return None\n    return v\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.make_template_answer_model_base","title":"make_template_answer_model_base","text":"<pre><code>make_template_answer_model_base(template_: Template) -&gt; type[BaseModel]\n</code></pre> <p>Create a base class for template answer models.</p> <p>Parameters:</p> <ul> <li> <code>template_</code>               (<code>Template</code>)           \u2013            <p>Template to create the model for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type</code> (              <code>type[BaseModel]</code> )          \u2013            <p>Base class for template answer models.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def make_template_answer_model_base(template_: Template) -&gt; type[BaseModel]:\n    \"\"\"Create a base class for template answer models.\n\n    Args:\n        template_: Template to create the model for.\n\n    Returns:\n        type: Base class for template answer models.\n\n    \"\"\"\n\n    class TemplateAnswerModelBase(BaseModel):\n        template: ClassVar[Template] = template_\n        model_config = ConfigDict(extra=\"forbid\")\n\n        def get(self, item_id: str) -&gt; Any | None:\n            return self.__dict__.get(item_id)\n\n        def __repr__(self) -&gt; str:\n            args = []\n            for item_id, _validator in self.model_fields.items():\n                question = self.template.get_question_by_id(item_id)\n                if not question:\n                    raise ValueError(\n                        f\"Question with ID {item_id} not found in template.\"\n                    )\n                infos = map(\n                    str,\n                    [\n                        question.title,\n                        question.widget_type,\n                        question.input_type,\n                        question.choices,\n                    ],\n                )\n                line = (\n                    f\"    {item_id}={self.__dict__[item_id]!r}, # {' - '.join(infos)}\"\n                )\n                args.append(line)\n            args_str = \"\\n\".join(args)\n\n            return f\"Template_{self.template.id}_AnswerModel(\\n\" + args_str + \"\\n)\"\n\n        __str__ = __repr__\n\n    return TemplateAnswerModelBase\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.not_empty","title":"not_empty","text":"<pre><code>not_empty(v: str) -&gt; str\n</code></pre> <p>Validate that a string is not empty.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def not_empty(v: str) -&gt; str:\n    \"\"\"Validate that a string is not empty.\"\"\"\n    if not len(v) &gt; 0:\n        raise ValueError(\"value is required\")\n    return v\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.option_in_allowed_values","title":"option_in_allowed_values","text":"<pre><code>option_in_allowed_values(value: Any, allowed: list[str] | None) -&gt; Any\n</code></pre> <p>Validate that a value is in a list of allowed values.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def option_in_allowed_values(value: Any, allowed: list[str] | None) -&gt; Any:\n    \"\"\"Validate that a value is in a list of allowed values.\"\"\"\n    if allowed is None or value not in allowed:\n        raise ValueError(f\"{value!r} is not a valid choice from: {allowed}\")\n    return value\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.models.parse_component","title":"parse_component","text":"<pre><code>parse_component(raw: ComponentRaw) -&gt; Component\n</code></pre> <p>Initialize an explicitly modelled representation</p> <p>(:obj:<code>pupil_labs.realtime_api.models.Component</code>) from the json-parsed dictionary (:obj:<code>pupil_labs.realtime_api.models.ComponentRaw</code>) received from the API.</p> <p>Parameters:</p> <ul> <li> <code>raw</code>               (<code>ComponentRaw</code>)           \u2013            <p>Dictionary containing component data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Component</code> (              <code>Component</code> )          \u2013            <p>Parsed component instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>UnknownComponentError</code>             \u2013            <p>If the component name cannot be mapped to an</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def parse_component(raw: ComponentRaw) -&gt; Component:\n    \"\"\"Initialize an explicitly modelled representation\n\n    (:obj:`pupil_labs.realtime_api.models.Component`) from the json-parsed dictionary\n    (:obj:`pupil_labs.realtime_api.models.ComponentRaw`) received from the API.\n\n    Args:\n        raw (ComponentRaw): Dictionary containing component data.\n\n    Returns:\n        Component: Parsed component instance.\n\n    Raises:\n        UnknownComponentError: If the component name cannot be mapped to an\n        explicitly modelled class or the contained data does not fit the modelled\n        fields.\n\n    \"\"\"\n    model_name = raw[\"model\"]\n    data = raw[\"data\"]\n    try:\n        model_class = _model_class_map[model_name]\n        return _init_cls_with_annotated_fields_only(model_class, data)\n    except KeyError as err:\n        raise UnknownComponentError(\n            f\"Could not generate component for {model_name} from {data}\"\n        ) from err\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.base","title":"base","text":"<p>Classes:</p> <ul> <li> <code>DeviceBase</code>           \u2013            <p>Abstract base class representing Realtime API host devices.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Type annotation for concrete sub-classes of :class:`DeviceBase</p> </li> </ul>"},{"location":"api/async/#pupil_labs.realtime_api.base.DeviceType","title":"DeviceType  <code>module-attribute</code>","text":"<pre><code>DeviceType = TypeVar('DeviceType', bound='DeviceBase')\n</code></pre> <p>Type annotation for concrete sub-classes of :class:<code>DeviceBase &lt;pupil_labs.realtime_api.base.DeviceBase&gt;</code>.</p>"},{"location":"api/async/#pupil_labs.realtime_api.base.DeviceBase","title":"DeviceBase","text":"<pre><code>DeviceBase(address: str, port: int, full_name: str | None = None, dns_name: str | None = None, suppress_decoding_warnings: bool = True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class representing Realtime API host devices.</p> <p>This class provides the foundation for device implementations that connect to the Realtime API.</p> <p>Attributes:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>REST API server address.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>REST API server port.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>)           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>)           \u2013            <p>REST API server DNS name, e.g.<code>neon.local / pi.local.</code>.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>REST API server address.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>REST API server port.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>REST API server DNS name, e.g.<code>neon.local / pi.local.</code>.</p> </li> <li> <code>suppress_decoding_warnings</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to suppress libav decoding warnings.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    port: int,\n    full_name: str | None = None,\n    dns_name: str | None = None,\n    suppress_decoding_warnings: bool = True,\n):\n    \"\"\"Initialize the DeviceBase instance.\n\n    Args:\n        address (str): REST API server address.\n        port (int): REST API server port.\n        full_name (str | None): Full service discovery name.\n        dns_name (str | None): REST API server DNS name,\n            e.g.``neon.local / pi.local.``.\n        suppress_decoding_warnings: Whether to suppress libav decoding warnings.\n\n    \"\"\"\n    self.address: str = address\n    self.port: int = port\n    self.full_name: str | None = full_name\n    self.dns_name: str | None = dns_name\n    if suppress_decoding_warnings:\n        # suppress decoding warnings due to incomplete data transmissions\n        logging.getLogger(\"libav.h264\").setLevel(logging.CRITICAL)\n        logging.getLogger(\"libav.swscaler\").setLevel(logging.ERROR)\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.base.DeviceBase.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.base.DeviceBase.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"api/async/#pupil_labs.realtime_api.base.DeviceBase.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"api/simple/","title":"Simple API reference","text":""},{"location":"api/simple/#pupil_labs.realtime_api.simple","title":"simple","text":"<p>Modules:</p> <ul> <li> <code>device</code>           \u2013            </li> <li> <code>discovery</code>           \u2013            </li> <li> <code>models</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>Device</code>           \u2013            <p>Simple synchronous API for interacting with Pupil Labs devices.</p> </li> <li> <code>MatchedGazeEyesSceneItem</code>           \u2013            <p>A matched triplet of scene video frame, eye video frame, and gaze data.</p> </li> <li> <code>MatchedItem</code>           \u2013            <p>A matched pair of scene video frame and gaze data.</p> </li> <li> <code>SimpleVideoFrame</code>           \u2013            <p>A simplified video frame representation.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>discover_devices</code>             \u2013              <p>Discover all available devices on the local network.</p> </li> <li> <code>discover_one_device</code>             \u2013              <p>Discover and return the first device found on the local network.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device","title":"Device","text":"<pre><code>Device(address: str, port: int, full_name: str | None = None, dns_name: str | None = None, start_streaming_by_default: bool = False, suppress_decoding_warnings: bool = True)\n</code></pre> <p>               Bases: <code>DeviceBase</code></p> <p>Simple synchronous API for interacting with Pupil Labs devices.</p> <p>This class provides a simplified, synchronous interface to Pupil Labs devices, wrapping the asynchronous API with a more user-friendly interface.</p> Important <p>Use [discover_devices][pupil_labs.realtime_api.simple.discovery. discover_devices] instead of initializing the class manually. See the simple_discovery_example example.</p> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>IP address of the device.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Port number of the device.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>DNS name of the device.</p> </li> <li> <code>start_streaming_by_default</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to start streaming automatically.</p> </li> <li> <code>suppress_decoding_warnings</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to suppress decoding warnings.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>close</code>             \u2013              <p>Close the device connection and stop all background threads.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>estimate_time_offset</code>             \u2013              <p>Estimate the time offset between the host device and the client.</p> </li> <li> <code>eye_events_sensor</code>             \u2013              <p>Get the eye events sensor.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> <li> <code>gaze_sensor</code>             \u2013              <p>Get the gaze sensor.</p> </li> <li> <code>get_calibration</code>             \u2013              <p>Get the current cameras calibration data.</p> </li> <li> <code>get_errors</code>             \u2013              <p>Get a list of errors from the device.</p> </li> <li> <code>get_template</code>             \u2013              <p>Get the template currently selected on the device.</p> </li> <li> <code>get_template_data</code>             \u2013              <p>Get the template data entered on the device.</p> </li> <li> <code>post_template_data</code>             \u2013              <p>Send the data to the currently selected template.</p> </li> <li> <code>receive_eye_events</code>             \u2013              <p>Receive an eye event.</p> </li> <li> <code>receive_eyes_video_frame</code>             \u2013              <p>Receive an eye camera video frame.</p> </li> <li> <code>receive_gaze_datum</code>             \u2013              <p>Receive a gaze data point.</p> </li> <li> <code>receive_imu_datum</code>             \u2013              <p>Receive an IMU data point.</p> </li> <li> <code>receive_matched_scene_and_eyes_video_frames_and_gaze</code>             \u2013              <p>Receive a matched triplet of scene video frame, eye video frame, and gaze.</p> </li> <li> <code>receive_matched_scene_video_frame_and_gaze</code>             \u2013              <p>Receive a matched pair of scene video frame and gaze data.</p> </li> <li> <code>receive_scene_video_frame</code>             \u2013              <p>Receive a scene (world) video frame.</p> </li> <li> <code>recording_cancel</code>             \u2013              <p>Cancel the current recording without saving it.</p> </li> <li> <code>recording_start</code>             \u2013              <p>Start a recording on the device.</p> </li> <li> <code>recording_stop_and_save</code>             \u2013              <p>Stop and save the current recording.</p> </li> <li> <code>send_event</code>             \u2013              <p>Send an event to the device.</p> </li> <li> <code>start_stream_if_needed</code>             \u2013              <p>Start streaming if not already streaming.</p> </li> <li> <code>streaming_start</code>             \u2013              <p>Start streaming data from the specified sensor.</p> </li> <li> <code>streaming_stop</code>             \u2013              <p>Stop streaming data from the specified sensor.</p> </li> <li> <code>world_sensor</code>             \u2013              <p>Get the world sensor.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>battery_level_percent</code>               (<code>int</code>)           \u2013            <p>Get the battery level of the connected phone in percentage.</p> </li> <li> <code>battery_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Get the battery state of the connected phone.</p> </li> <li> <code>is_currently_streaming</code>               (<code>bool</code>)           \u2013            <p>Check if data streaming is currently active.</p> </li> <li> <code>memory_num_free_bytes</code>               (<code>int</code>)           \u2013            <p>Get the available memory of the connected phone in bytes.</p> </li> <li> <code>memory_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Get the memory state of the connected phone.</p> </li> <li> <code>module_serial</code>               (<code>str | Literal['default'] | None</code>)           \u2013            <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> </li> <li> <code>phone_id</code>               (<code>str</code>)           \u2013            <p>Get the ID of the connected phone.</p> </li> <li> <code>phone_ip</code>               (<code>str</code>)           \u2013            <p>Get the IP address of the connected phone.</p> </li> <li> <code>phone_name</code>               (<code>str</code>)           \u2013            <p>Get the name of the connected phone.</p> </li> <li> <code>serial_number_glasses</code>               (<code>str | Literal['default'] | None</code>)           \u2013            <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> </li> <li> <code>serial_number_scene_cam</code>               (<code>str | None</code>)           \u2013            <p>Returns <code>None</code> if no scene camera is connected</p> </li> <li> <code>version_glasses</code>               (<code>str | None</code>)           \u2013            <p>Get the version of the connected glasses.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    port: int,\n    full_name: str | None = None,\n    dns_name: str | None = None,\n    start_streaming_by_default: bool = False,\n    suppress_decoding_warnings: bool = True,\n) -&gt; None:\n    \"\"\"Initialize a Device instance.\n\n    Args:\n        address: IP address of the device.\n        port: Port number of the device.\n        full_name: Full service discovery name.\n        dns_name: DNS name of the device.\n        start_streaming_by_default: Whether to start streaming automatically.\n        suppress_decoding_warnings: Whether to suppress decoding warnings.\n\n    \"\"\"\n    super().__init__(\n        address,\n        port,\n        full_name=full_name,\n        dns_name=dns_name,\n        suppress_decoding_warnings=suppress_decoding_warnings,\n    )\n    self._status = self._get_status()\n    self._start_background_worker(start_streaming_by_default)\n\n    self._errors: list[str] = []\n\n    self.stream_name_start_event_map = {\n        SensorName.GAZE.value: self._EVENT.SHOULD_START_GAZE,\n        SensorName.WORLD.value: self._EVENT.SHOULD_START_WORLD,\n        SensorName.EYES.value: self._EVENT.SHOULD_START_EYES,\n        SensorName.IMU.value: self._EVENT.SHOULD_START_IMU,\n        SensorName.EYE_EVENTS.value: self._EVENT.SHOULD_START_EYE_EVENTS,\n    }\n\n    self.stream_name_stop_event_map = {\n        SensorName.GAZE.value: self._EVENT.SHOULD_STOP_GAZE,\n        SensorName.WORLD.value: self._EVENT.SHOULD_STOP_WORLD,\n        SensorName.EYES.value: self._EVENT.SHOULD_STOP_EYES,\n        SensorName.IMU.value: self._EVENT.SHOULD_STOP_IMU,\n        SensorName.EYE_EVENTS.value: self._EVENT.SHOULD_STOP_EYE_EVENTS,\n    }\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.battery_level_percent","title":"battery_level_percent  <code>property</code>","text":"<pre><code>battery_level_percent: int\n</code></pre> <p>Get the battery level of the connected phone in percentage.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.battery_state","title":"battery_state  <code>property</code>","text":"<pre><code>battery_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Get the battery state of the connected phone.</p> <p>Possible values are \"OK\", \"LOW\", or \"CRITICAL\".</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.is_currently_streaming","title":"is_currently_streaming  <code>property</code>","text":"<pre><code>is_currently_streaming: bool\n</code></pre> <p>Check if data streaming is currently active.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if streaming is active, False otherwise.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.memory_num_free_bytes","title":"memory_num_free_bytes  <code>property</code>","text":"<pre><code>memory_num_free_bytes: int\n</code></pre> <p>Get the available memory of the connected phone in bytes.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.memory_state","title":"memory_state  <code>property</code>","text":"<pre><code>memory_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Get the memory state of the connected phone.</p> <p>Possible values are \"OK\", \"LOW\", or \"CRITICAL\".</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.module_serial","title":"module_serial  <code>property</code>","text":"<pre><code>module_serial: str | Literal['default'] | None\n</code></pre> <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> Info <p>Only available on Neon.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.phone_id","title":"phone_id  <code>property</code>","text":"<pre><code>phone_id: str\n</code></pre> <p>Get the ID of the connected phone.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.phone_ip","title":"phone_ip  <code>property</code>","text":"<pre><code>phone_ip: str\n</code></pre> <p>Get the IP address of the connected phone.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.phone_name","title":"phone_name  <code>property</code>","text":"<pre><code>phone_name: str\n</code></pre> <p>Get the name of the connected phone.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.serial_number_glasses","title":"serial_number_glasses  <code>property</code>","text":"<pre><code>serial_number_glasses: str | Literal['default'] | None\n</code></pre> <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> Info <p>Only available on Pupil Invisible.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.serial_number_scene_cam","title":"serial_number_scene_cam  <code>property</code>","text":"<pre><code>serial_number_scene_cam: str | None\n</code></pre> <p>Returns <code>None</code> if no scene camera is connected</p> Info <p>Only available on Pupil Invisible.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.version_glasses","title":"version_glasses  <code>property</code>","text":"<pre><code>version_glasses: str | None\n</code></pre> <p>Get the version of the connected glasses.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str | None</code> )          \u2013            <p>1 -&gt; Pupil Invisible, 2 -&gt; Neon, or None -&gt; No glasses connected.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the device connection and stop all background threads.</p> <p>This method should be called when the device is no longer needed to free up resources.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the device connection and stop all background threads.\n\n    This method should be called when the device is no longer needed\n    to free up resources.\n    \"\"\"\n    if self._event_manager:\n        if self.is_currently_streaming:\n            self.streaming_stop()\n        self._event_manager.trigger_threadsafe(self._EVENT.SHOULD_WORKER_CLOSE)\n        self._auto_update_thread.join()\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.estimate_time_offset","title":"estimate_time_offset","text":"<pre><code>estimate_time_offset(number_of_measurements: int = 100, sleep_between_measurements_seconds: float | None = None) -&gt; TimeEchoEstimates | None\n</code></pre> <p>Estimate the time offset between the host device and the client.</p> <p>This uses the Time Echo protocol to estimate the clock offset between the device and the client.</p> <p>Parameters:</p> <ul> <li> <code>number_of_measurements</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of measurements to take.</p> </li> <li> <code>sleep_between_measurements_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional sleep time between</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TimeEchoEstimates</code> (              <code>TimeEchoEstimates | None</code> )          \u2013            <p>Statistics for roundtrip durations and time offsets, or None if estimation failed or device doesn't support the protocol.</p> </li> </ul> See Also <p>:mod:<code>pupil_labs.realtime_api.time_echo</code> for more details.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def estimate_time_offset(\n    self,\n    number_of_measurements: int = 100,\n    sleep_between_measurements_seconds: float | None = None,\n) -&gt; TimeEchoEstimates | None:\n    \"\"\"Estimate the time offset between the host device and the client.\n\n    This uses the Time Echo protocol to estimate the clock offset between\n    the device and the client.\n\n    Args:\n        number_of_measurements: Number of measurements to take.\n        sleep_between_measurements_seconds: Optional sleep time between\n        measurements.\n\n    Returns:\n        TimeEchoEstimates: Statistics for roundtrip durations and time offsets,\n            or None if estimation failed or device doesn't support the protocol.\n\n    See Also:\n        :mod:`pupil_labs.realtime_api.time_echo` for more details.\n\n    \"\"\"\n    if self._status.phone.time_echo_port is None:\n        logger.warning(\n            \"You Pupil Invisible Companion app is out-of-date and does not yet \"\n            \"support the Time Echo protocol. Upgrade to version 1.4.28 or newer.\"\n        )\n        return None\n    estimator = TimeOffsetEstimator(\n        self.phone_ip, self._status.phone.time_echo_port\n    )\n    return asyncio.run(\n        estimator.estimate(\n            number_of_measurements, sleep_between_measurements_seconds\n        )\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.eye_events_sensor","title":"eye_events_sensor","text":"<pre><code>eye_events_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the eye events sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def eye_events_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the eye events sensor.\"\"\"\n    return self._status.direct_eye_events_sensor()\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.gaze_sensor","title":"gaze_sensor","text":"<pre><code>gaze_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the gaze sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def gaze_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the gaze sensor.\"\"\"\n    return self._status.direct_gaze_sensor()\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.get_calibration","title":"get_calibration","text":"<pre><code>get_calibration() -&gt; Calibration\n</code></pre> <p>Get the current cameras calibration data.</p> <p>Note that Pupil Invisible and Neon are calibration free systems, this refers to the intrinsincs and extrinsics of the cameras.</p> <p>Returns:</p> <ul> <li> <code>Calibration</code>           \u2013            <p>pupil_labs.neon_recording.calib.Calibration: The calibration data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_calibration(self) -&gt; Calibration:\n    \"\"\"Get the current cameras calibration data.\n\n    Note that Pupil Invisible and Neon are calibration free systems, this refers to\n    the intrinsincs and extrinsics of the cameras.\n\n    Returns:\n        pupil_labs.neon_recording.calib.Calibration: The calibration data.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n\n    async def _get_calibration() -&gt; Calibration:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_calibration()\n\n    return asyncio.run(_get_calibration())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.get_errors","title":"get_errors","text":"<pre><code>get_errors() -&gt; list[str]\n</code></pre> <p>Get a list of errors from the device.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of error messages.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_errors(self) -&gt; list[str]:\n    \"\"\"Get a list of errors from the device.\n\n    Returns:\n        list[str]: List of error messages.\n\n    \"\"\"\n    errors = self._errors.copy()\n    self._errors.clear()\n\n    return errors\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.get_template","title":"get_template","text":"<pre><code>get_template() -&gt; Template\n</code></pre> <p>Get the template currently selected on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.get_template</p> <p>Returns:</p> <ul> <li> <code>Template</code> (              <code>Template</code> )          \u2013            <p>The currently selected template.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template can't be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_template(self) -&gt; Template:\n    \"\"\"Get the template currently selected on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.get_template][]\n\n    Returns:\n        Template: The currently selected template.\n\n    Raises:\n        DeviceError: If the template can't be fetched.\n\n    \"\"\"\n\n    async def _get_template() -&gt; Template:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_template()\n\n    return asyncio.run(_get_template())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.get_template_data","title":"get_template_data","text":"<pre><code>get_template_data(template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Get the template data entered on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.get_template_data</p> <p>Parameters:</p> <ul> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the returned data. \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]} \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result from the GET request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template's data could not be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_template_data(self, template_format: TemplateDataFormat = \"simple\") -&gt; Any:\n    \"\"\"Get the template data entered on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.get_template_data][]\n\n    Args:\n        template_format: Format of the returned data.\n            \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]}\n            \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result from the GET request.\n\n    Raises:\n        DeviceError: If the template's data could not be fetched.\n\n    \"\"\"\n\n    async def _get_template_data() -&gt; Any:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_template_data(template_format=template_format)\n\n    return asyncio.run(_get_template_data())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.post_template_data","title":"post_template_data","text":"<pre><code>post_template_data(template_data: dict[str, list[str]], template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Send the data to the currently selected template.</p> <p>Wraps pupil_labs.realtime_api.device.Device.post_template_data</p> <p>Parameters:</p> <ul> <li> <code>template_data</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>The template data to send.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the input data. \"api\" accepts the data as in realtime api format e.g., {\"item_uuid\": [\"42\"]} \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result from the POST request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the data can not be sent.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid data type.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def post_template_data(\n    self,\n    template_data: dict[str, list[str]],\n    template_format: TemplateDataFormat = \"simple\",\n) -&gt; Any:\n    \"\"\"Send the data to the currently selected template.\n\n    Wraps [pupil_labs.realtime_api.device.Device.post_template_data][]\n\n    Args:\n        template_data: The template data to send.\n        template_format: Format of the input data.\n            \"api\" accepts the data as in realtime api format e.g.,\n            {\"item_uuid\": [\"42\"]}\n            \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result from the POST request.\n\n    Raises:\n        DeviceError: If the data can not be sent.\n        ValueError: If invalid data type.\n\n    \"\"\"\n\n    async def _post_template_data() -&gt; Any:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.post_template_data(\n                template_data, template_format=template_format\n            )\n\n    return asyncio.run(_post_template_data())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_eye_events","title":"receive_eye_events","text":"<pre><code>receive_eye_events(timeout_seconds: float | None = None) -&gt; FixationEventData | BlinkEventData | FixationOnsetEventData | None\n</code></pre> <p>Receive an eye event.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new eye event. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FixationEventData | BlinkEventData | FixationOnsetEventData | None</code>           \u2013            <p>FixationEventData | BlinkEventData | FixationOnsetEventData or None:</p> </li> <li> <code>FixationEventData | BlinkEventData | FixationOnsetEventData | None</code>           \u2013            <p>The received eye event, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_eye_events(\n    self, timeout_seconds: float | None = None\n) -&gt; FixationEventData | BlinkEventData | FixationOnsetEventData | None:\n    \"\"\"Receive an eye event.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new eye event.\n            If None, wait indefinitely.\n\n    Returns:\n        FixationEventData | BlinkEventData | FixationOnsetEventData or None:\n        The received eye event, or None if timeout was reached.\n\n    \"\"\"\n    return cast(\n        FixationEventData | BlinkEventData | FixationOnsetEventData,\n        self._receive_item(SensorName.EYE_EVENTS.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_eyes_video_frame","title":"receive_eyes_video_frame","text":"<pre><code>receive_eyes_video_frame(timeout_seconds: float | None = None) -&gt; SimpleVideoFrame | None\n</code></pre> <p>Receive an eye camera video frame.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new frame. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>SimpleVideoFrame or None: The received video frame, or None if timeout</p> </li> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_eyes_video_frame(\n    self, timeout_seconds: float | None = None\n) -&gt; SimpleVideoFrame | None:\n    \"\"\"Receive an eye camera video frame.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new frame.\n            If None, wait indefinitely.\n\n    Returns:\n        SimpleVideoFrame or None: The received video frame, or None if timeout\n        was reached.\n\n    \"\"\"\n    return cast(\n        SimpleVideoFrame,\n        self._receive_item(SensorName.EYES.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_gaze_datum","title":"receive_gaze_datum","text":"<pre><code>receive_gaze_datum(timeout_seconds: float | None = None) -&gt; GazeDataType | None\n</code></pre> <p>Receive a gaze data point.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new gaze datum. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GazeDataType | None</code>           \u2013            <p>GazeDataType or None: The received gaze data, or None if timeout was</p> </li> <li> <code>GazeDataType | None</code>           \u2013            <p>reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_gaze_datum(\n    self, timeout_seconds: float | None = None\n) -&gt; GazeDataType | None:\n    \"\"\"Receive a gaze data point.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new gaze datum.\n            If None, wait indefinitely.\n\n    Returns:\n        GazeDataType or None: The received gaze data, or None if timeout was\n        reached.\n\n    \"\"\"\n    return cast(\n        GazeDataType, self._receive_item(SensorName.GAZE.value, timeout_seconds)\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_imu_datum","title":"receive_imu_datum","text":"<pre><code>receive_imu_datum(timeout_seconds: float | None = None) -&gt; IMUData | None\n</code></pre> <p>Receive an IMU data point.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new IMU datum. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IMUData | None</code>           \u2013            <p>IMUData or None: The received IMU data, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_imu_datum(self, timeout_seconds: float | None = None) -&gt; IMUData | None:\n    \"\"\"Receive an IMU data point.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new IMU datum.\n            If None, wait indefinitely.\n\n    Returns:\n        IMUData or None: The received IMU data, or None if timeout was reached.\n\n    \"\"\"\n    return cast(IMUData, self._receive_item(SensorName.IMU.value, timeout_seconds))\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_matched_scene_and_eyes_video_frames_and_gaze","title":"receive_matched_scene_and_eyes_video_frames_and_gaze","text":"<pre><code>receive_matched_scene_and_eyes_video_frames_and_gaze(timeout_seconds: float | None = None) -&gt; MatchedGazeEyesSceneItem | None\n</code></pre> <p>Receive a matched triplet of scene video frame, eye video frame, and gaze.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a matched triplet. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MatchedGazeEyesSceneItem | None</code>           \u2013            <p>MatchedGazeEyesSceneItem or None: The matched triplet, or None if timeout</p> </li> <li> <code>MatchedGazeEyesSceneItem | None</code>           \u2013            <p>was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_matched_scene_and_eyes_video_frames_and_gaze(\n    self, timeout_seconds: float | None = None\n) -&gt; MatchedGazeEyesSceneItem | None:\n    \"\"\"Receive a matched triplet of scene video frame, eye video frame, and gaze.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a matched triplet.\n            If None, wait indefinitely.\n\n    Returns:\n        MatchedGazeEyesSceneItem or None: The matched triplet, or None if timeout\n        was reached.\n\n    \"\"\"\n    return cast(\n        MatchedGazeEyesSceneItem,\n        self._receive_item(MATCHED_GAZE_EYES_LABEL, timeout_seconds),\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_matched_scene_video_frame_and_gaze","title":"receive_matched_scene_video_frame_and_gaze","text":"<pre><code>receive_matched_scene_video_frame_and_gaze(timeout_seconds: float | None = None) -&gt; MatchedItem | None\n</code></pre> <p>Receive a matched pair of scene video frame and gaze data.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a matched pair. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MatchedItem | None</code>           \u2013            <p>MatchedItem or None: The matched pair, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_matched_scene_video_frame_and_gaze(\n    self, timeout_seconds: float | None = None\n) -&gt; MatchedItem | None:\n    \"\"\"Receive a matched pair of scene video frame and gaze data.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a matched pair.\n            If None, wait indefinitely.\n\n    Returns:\n        MatchedItem or None: The matched pair, or None if timeout was reached.\n\n    \"\"\"\n    return cast(\n        MatchedItem, self._receive_item(MATCHED_ITEM_LABEL, timeout_seconds)\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.receive_scene_video_frame","title":"receive_scene_video_frame","text":"<pre><code>receive_scene_video_frame(timeout_seconds: float | None = None) -&gt; SimpleVideoFrame | None\n</code></pre> <p>Receive a scene (world) video frame.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new frame. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>SimpleVideoFrame or None: The received video frame, or None if timeout was</p> </li> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_scene_video_frame(\n    self, timeout_seconds: float | None = None\n) -&gt; SimpleVideoFrame | None:\n    \"\"\"Receive a scene (world) video frame.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new frame.\n            If None, wait indefinitely.\n\n    Returns:\n        SimpleVideoFrame or None: The received video frame, or None if timeout was\n        reached.\n\n    \"\"\"\n    return cast(\n        SimpleVideoFrame,\n        self._receive_item(SensorName.WORLD.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.recording_cancel","title":"recording_cancel","text":"<pre><code>recording_cancel() -&gt; None\n</code></pre> <p>Cancel the current recording without saving it.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_cancel</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be cancelled. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording not running</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_cancel(self) -&gt; None:\n    \"\"\"Cancel the current recording without saving it.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_cancel][]\n\n    Raises:\n        DeviceError: If the recording could not be cancelled. Possible reasons\n        include:\n            - Recording not running\n\n    \"\"\"\n\n    async def _cancel_recording() -&gt; None:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_cancel()\n\n    return asyncio.run(_cancel_recording())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.recording_start","title":"recording_start","text":"<pre><code>recording_start() -&gt; str\n</code></pre> <p>Start a recording on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_start</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>ID of the started recording.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be started. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording already running</li> <li>Template has required fields</li> <li>Low battery</li> <li>Low storage</li> <li>No wearer selected</li> <li>No workspace selected</li> <li>Setup bottom sheets not completed</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_start(self) -&gt; str:\n    \"\"\"Start a recording on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_start][]\n\n    Returns:\n        str: ID of the started recording.\n\n    Raises:\n        DeviceError: If the recording could not be started. Possible reasons\n        include:\n            - Recording already running\n            - Template has required fields\n            - Low battery\n            - Low storage\n            - No wearer selected\n            - No workspace selected\n            - Setup bottom sheets not completed\n\n    \"\"\"\n\n    async def _start_recording() -&gt; str:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_start()\n\n    return asyncio.run(_start_recording())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.recording_stop_and_save","title":"recording_stop_and_save","text":"<pre><code>recording_stop_and_save() -&gt; None\n</code></pre> <p>Stop and save the current recording.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_stop_and_save</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be stopped. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording not running</li> <li>Template has required fields</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_stop_and_save(self) -&gt; None:\n    \"\"\"Stop and save the current recording.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_stop_and_save][]\n\n    Raises:\n        DeviceError: If the recording could not be stopped. Possible reasons\n        include:\n            - Recording not running\n            - Template has required fields\n\n    \"\"\"\n\n    async def _stop_and_save_recording() -&gt; None:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_stop_and_save()\n\n    return asyncio.run(_stop_and_save_recording())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.send_event","title":"send_event","text":"<pre><code>send_event(event_name: str, event_timestamp_unix_ns: int | None = None) -&gt; Event\n</code></pre> <p>Send an event to the device.</p> <p>Parameters:</p> <ul> <li> <code>event_name</code>               (<code>str</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>event_timestamp_unix_ns</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional timestamp in unix nanoseconds. If None, the current time will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>The created event.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If sending the event fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def send_event(\n    self, event_name: str, event_timestamp_unix_ns: int | None = None\n) -&gt; Event:\n    \"\"\"Send an event to the device.\n\n    Args:\n        event_name: Name of the event.\n        event_timestamp_unix_ns: Optional timestamp in unix nanoseconds.\n            If None, the current time will be used.\n\n    Returns:\n        Event: The created event.\n\n    Raises:\n        DeviceError: If sending the event fails.\n\n    \"\"\"\n\n    async def _send_event() -&gt; Event:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.send_event(event_name, event_timestamp_unix_ns)\n\n    return asyncio.run(_send_event())\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.start_stream_if_needed","title":"start_stream_if_needed","text":"<pre><code>start_stream_if_needed(sensor: str) -&gt; None\n</code></pre> <p>Start streaming if not already streaming.</p> <p>Parameters:</p> <ul> <li> <code>sensor</code>               (<code>str</code>)           \u2013            <p>Sensor name to check.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def start_stream_if_needed(self, sensor: str) -&gt; None:\n    \"\"\"Start streaming if not already streaming.\n\n    Args:\n        sensor: Sensor name to check.\n\n    \"\"\"\n    if not self._is_streaming_flags[sensor].is_set():\n        logger.debug(\"receive_* called without being streaming\")\n        self.streaming_start(sensor)\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.streaming_start","title":"streaming_start","text":"<pre><code>streaming_start(stream_name: str) -&gt; None\n</code></pre> <p>Start streaming data from the specified sensor.</p> <p>Parameters:</p> <ul> <li> <code>stream_name</code>               (<code>str</code>)           \u2013            <p>Name of the sensor to start streaming from. It can be one of</p> </li> <li> <code></code>           \u2013            <p>py:attr:<code>SensorName</code> values or None, which will start all streams.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the stream name is not recognized.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def streaming_start(self, stream_name: str) -&gt; None:\n    \"\"\"Start streaming data from the specified sensor.\n\n    Args:\n        stream_name: Name of the sensor to start streaming from. It can be one of\n        :py:attr:`SensorName` values or None, which will start all streams.\n\n    Raises:\n        ValueError: If the stream name is not recognized.\n\n    \"\"\"\n    if stream_name is None:\n        for event in (\n            self._EVENT.SHOULD_START_GAZE,\n            self._EVENT.SHOULD_START_WORLD,\n            self._EVENT.SHOULD_START_EYES,\n            self._EVENT.SHOULD_START_IMU,\n            self._EVENT.SHOULD_START_EYE_EVENTS,\n        ):\n            self._streaming_trigger_action(event)\n        return\n\n    event = self.stream_name_start_event_map[stream_name]\n    self._streaming_trigger_action(event)\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.streaming_stop","title":"streaming_stop","text":"<pre><code>streaming_stop(stream_name: str | None = None) -&gt; None\n</code></pre> <p>Stop streaming data from the specified sensor.</p> <p>Parameters:</p> <ul> <li> <code>stream_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the sensor to start streaming from. It can be one of</p> </li> <li> <code></code>           \u2013            <p>py:attr:<code>SensorName</code> values or None, which will stop all streams.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the stream name is not recognized.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def streaming_stop(self, stream_name: str | None = None) -&gt; None:\n    \"\"\"Stop streaming data from the specified sensor.\n\n    Args:\n        stream_name: Name of the sensor to start streaming from. It can be one of\n        :py:attr:`SensorName` values or None, which will stop all streams.\n\n    Raises:\n        ValueError: If the stream name is not recognized.\n\n    \"\"\"\n    if stream_name is None:\n        for event in (\n            self._EVENT.SHOULD_STOP_GAZE,\n            self._EVENT.SHOULD_STOP_WORLD,\n            self._EVENT.SHOULD_STOP_EYES,\n            self._EVENT.SHOULD_STOP_IMU,\n            self._EVENT.SHOULD_STOP_EYE_EVENTS,\n        ):\n            self._streaming_trigger_action(event)\n        return\n\n    event = self.stream_name_stop_event_map[stream_name]\n    self._streaming_trigger_action(event)\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.Device.world_sensor","title":"world_sensor","text":"<pre><code>world_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the world sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def world_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the world sensor.\"\"\"\n    return self._status.direct_world_sensor()\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem","title":"MatchedGazeEyesSceneItem","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A matched triplet of scene video frame, eye video frame, and gaze data.</p> <p>This class represents scene and eye video frames along with gaze data that occurred at approximately the same time.</p> <p>Attributes:</p> <ul> <li> <code>eyes</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Eye camera video frame.</p> </li> <li> <code>gaze</code>               (<code>GazeDataType</code>)           \u2013            <p>Corresponding gaze data.</p> </li> <li> <code>scene</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Scene video frame.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.eyes","title":"eyes  <code>instance-attribute</code>","text":"<pre><code>eyes: SimpleVideoFrame\n</code></pre> <p>Eye camera video frame.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.gaze","title":"gaze  <code>instance-attribute</code>","text":"<pre><code>gaze: GazeDataType\n</code></pre> <p>Corresponding gaze data.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.scene","title":"scene  <code>instance-attribute</code>","text":"<pre><code>scene: SimpleVideoFrame\n</code></pre> <p>Scene video frame.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedItem","title":"MatchedItem","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A matched pair of scene video frame and gaze data.</p> <p>This class represents a scene video frame and gaze data point that occurred at approximately the same time.</p> Note <p>The name MatchedItem is maintained for backward compatibility. It represents a matched pair of scene video frame and gaze data.</p> <p>Attributes:</p> <ul> <li> <code>frame</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Scene video frame.</p> </li> <li> <code>gaze</code>               (<code>GazeDataType</code>)           \u2013            <p>Corresponding gaze data.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedItem.frame","title":"frame  <code>instance-attribute</code>","text":"<pre><code>frame: SimpleVideoFrame\n</code></pre> <p>Scene video frame.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.MatchedItem.gaze","title":"gaze  <code>instance-attribute</code>","text":"<pre><code>gaze: GazeDataType\n</code></pre> <p>Corresponding gaze data.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.SimpleVideoFrame","title":"SimpleVideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A simplified video frame representation.</p> <p>This class provides a simplified representation of a video frame with BGR pixel data and timestamp information.</p> <p>Attributes:</p> <ul> <li> <code>bgr_pixels</code>               (<code>BGRBuffer</code>)           \u2013            <p>BGR pixel data as a NumPy array.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.SimpleVideoFrame.bgr_pixels","title":"bgr_pixels  <code>instance-attribute</code>","text":"<pre><code>bgr_pixels: BGRBuffer\n</code></pre> <p>BGR pixel data as a NumPy array.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.SimpleVideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.discover_devices","title":"discover_devices","text":"<pre><code>discover_devices(search_duration_seconds: float) -&gt; list[Device]\n</code></pre> <p>Discover all available devices on the local network.</p> <p>This function searches for devices on the local network for the specified duration and returns all discovered devices.</p> <p>Parameters:</p> <ul> <li> <code>search_duration_seconds</code>               (<code>float</code>)           \u2013            <p>How long to search for devices in seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Device]</code>           \u2013            <p>list[Device]: List of discovered devices.</p> </li> </ul> Example <pre><code># Discover devices for 5 seconds\ndevices = discover_devices(5.0)\nfor device in devices:\n    print(f\"Found device: {device.phone_name}\")\n</code></pre> See Also <p>The asynchronous equivalent :func:<code>pupil_labs.realtime_api.discovery.discover_devices</code></p> Source code in <code>src/pupil_labs/realtime_api/simple/discovery.py</code> <pre><code>def discover_devices(search_duration_seconds: float) -&gt; list[Device]:\n    \"\"\"Discover all available devices on the local network.\n\n    This function searches for devices on the local network for the specified\n    duration and returns all discovered devices.\n\n    Args:\n        search_duration_seconds: How long to search for devices in seconds.\n\n    Returns:\n        list[Device]: List of discovered devices.\n\n    Example:\n        ```python\n        # Discover devices for 5 seconds\n        devices = discover_devices(5.0)\n        for device in devices:\n            print(f\"Found device: {device.phone_name}\")\n        ```\n\n    See Also:\n        The asynchronous equivalent\n        :func:`pupil_labs.realtime_api.discovery.discover_devices`\n\n    \"\"\"\n\n    async def _discover() -&gt; tuple[DiscoveredDeviceInfo, ...]:\n        async with AsyncNetwork() as network:\n            await asyncio.sleep(search_duration_seconds)\n            return network.devices\n\n    return [Device.from_discovered_device(dev) for dev in asyncio.run(_discover())]\n</code></pre>"},{"location":"api/simple/#pupil_labs.realtime_api.simple.discover_one_device","title":"discover_one_device","text":"<pre><code>discover_one_device(max_search_duration_seconds: float | None = 10.0) -&gt; Device | None\n</code></pre> <p>Discover and return the first device found on the local network.</p> <p>This function searches for devices on the local network and returns the first discovered device, or None if no device is found within the specified maximum search duration.</p> <p>Parameters:</p> <ul> <li> <code>max_search_duration_seconds</code>               (<code>float | None</code>, default:                   <code>10.0</code> )           \u2013            <p>Maximum time to search for a device in seconds. If None, search indefinitely. Default is 10.0 seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Device | None</code>           \u2013            <p>Device or None: The first discovered device, or None if no device was found</p> </li> <li> <code>Device | None</code>           \u2013            <p>within the specified time limit.</p> </li> </ul> Example <pre><code># Try to find a device within 5 seconds\ndevice = discover_one_device(5.0)\nif device:\n    print(f\"Found device: {device.phone_name}\")\nelse:\n    print(\"No device found\")\n</code></pre> See Also <p>The asynchronous equivalent :func:<code>pupil_labs.realtime_api.discovery.Network.wait_for_new_device</code></p> Source code in <code>src/pupil_labs/realtime_api/simple/discovery.py</code> <pre><code>def discover_one_device(\n    max_search_duration_seconds: float | None = 10.0,\n) -&gt; Device | None:\n    \"\"\"Discover and return the first device found on the local network.\n\n    This function searches for devices on the local network and returns\n    the first discovered device, or None if no device is found within\n    the specified maximum search duration.\n\n    Args:\n        max_search_duration_seconds: Maximum time to search for a device in seconds.\n            If None, search indefinitely. Default is 10.0 seconds.\n\n    Returns:\n        Device or None: The first discovered device, or None if no device was found\n        within the specified time limit.\n\n    Example:\n        ```python\n        # Try to find a device within 5 seconds\n        device = discover_one_device(5.0)\n        if device:\n            print(f\"Found device: {device.phone_name}\")\n        else:\n            print(\"No device found\")\n        ```\n\n    See Also:\n        The asynchronous equivalent\n        :func:`pupil_labs.realtime_api.discovery.Network.wait_for_new_device`\n\n    \"\"\"\n\n    async def _discover() -&gt; DiscoveredDeviceInfo | None:\n        async with AsyncNetwork() as network:\n            return await network.wait_for_new_device(max_search_duration_seconds)\n\n    device = asyncio.run(_discover())\n    return None if device is None else Device.from_discovered_device(device)\n</code></pre>"},{"location":"cookbook/","title":"Index","text":"<p>In the next section, we will explore how to use the Pupil Labs Realtime API to track your experiment progress using events. This is particularly useful for monitoring the state of your experiment in real-time and can be integrated into various applications.</p> <p>Have a look at Track Your Experiment Progress Using Events</p> <p>There are more complex examples and tools that leverage this package, have a look at:</p> <ul> <li> <p> GPT4-Eyes: AI Vision Assistant</p> <p>Combines eye tracking with AI for real-time scene understanding and assistance.</p> <p> Learn More</p> </li> <li> <p> Gaze Contingency for Assistive Tech</p> <p>Practical guide to using gaze as a cursor and for gaze contingent paradigms.</p> <p> Explore Guide</p> </li> <li> <p> Neon Plugin for PsychoPy</p> <p>Integrate Neon eye tracking into psychophysics experiments with PsychoPy.</p> <p> Read Documentation</p> </li> <li> <p> More Projects and Tools</p> <p>Detect object's with YOLO in RealTime.</p> <p> Explore Further</p> </li> </ul>"},{"location":"cookbook/track-your-experiment-progress-using-events/","title":"Track Your Experiment Progress Using Events","text":"<p>Running a data collection for an experiment can be an organizational challenge. Many experiments are running through different phases and keeping track of what data belongs to what phase can be one of the difficulties.</p> <p>Using events, tracking the progress of an experiment becomes very easy and can often be fully automated though.</p> <p>In this guide, we will demonstrate how to save events at recording time and how to utilize them later during analysis to easily keep track of what phase a certain section of data was recorded in.</p> <p>To this end we are assuming a minimal experiment setup: we want to record subjects while they observe a series of images of animals and analyze how the average fixation duration differs for each image.</p> <p>Example Data</p> <p>You can download the example data used in this guide here.</p>"},{"location":"cookbook/track-your-experiment-progress-using-events/#how-to-use-events-to-keep-track","title":"How To Use Events To Keep Track?","text":"<p>Events are essentially timestamps within a recording that have been marked with a name. We need to keep track of when a specific image is shown during a recording, so we can associate the according fixation data with that image. Thus, we will create an event at the start and end of each image presentation to mark this section.</p> <p>Events can either be created post hoc in the project editor, or at recording time using either the real-time API or the Monitor App. In this example, we are interested in fully automating the event creation and will thus use the real-time API to save events, but depending on your use case you could use either of those methods.</p>"},{"location":"cookbook/track-your-experiment-progress-using-events/#implementation","title":"Implementation","text":"<p>The implementation of stimulus presentation is minimal. The images are loaded using OpenCV and are displayed in a full-screen window for a fixed amount of time.</p> <pre><code>import time\nimport cv2\n\nimage_names = [\"owl\", \"fox\", \"deer\"]\n\ndef prepare_stimulus_presentation():\n    cv2.namedWindow(\"Stimulus\", cv2.WINDOW_NORMAL)\n    cv2.setWindowProperty(\"Stimulus\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n\ndef present_stimulus(img):\n    presentation_time = 5\n    start_time = time.perf_counter()\n    while time.perf_counter() - start_time &lt; presentation_time:\n        cv2.imshow(\"Stimulus\", img)\n        cv2.waitKey(1)\n\ndef cleanup_stimulus_presentation():\n    cv2.destroyAllWindows()\n</code></pre> <p>Using the real-time API, we now have to connect to a Neon device for recording. We can remotely start the recording and save events before and after the stimulus presentation. The names of the events are chosen as <code>&lt;animal name&gt;_start</code> and <code>&lt;animal name&gt;_end</code> depending on the animal that is shown.</p> <p>Once all images have been shown, the recording is stopped remotely.</p> <pre><code># The 2 lines below are only needed when accessing\n# the real-time API from a Jupyter notebook\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom pupil_labs.realtime_api.simple import discover_one_device\n\ndevice = discover_one_device()\ndevice.recording_start()\n\n# Wait for a couple seconds before starting\n# to give all sensors enough time to initialize\ntime.sleep(3)\n\nprepare_stimulus_presentation()\n\nfor name in image_names:\n    img = cv2.imread(name + \".jpg\")\n\n    device.send_event(name + \"_start\")\n    present_stimulus(img)\n    device.send_event(name + \"_end\")\n\ncleanup_stimulus_presentation()\ndevice.recording_stop_and_save()\n</code></pre> <p>That is all we have to do during data collection. Once the recordings have been uploaded to Pupil Cloud we can already see the events in the timeline for every recording. Next, export the timeseries data of all recordings from Pupil Cloud.</p> <pre><code>import pandas as pd\nevents = pd.read_csv(\"raw-data-export/george-49e4a972/events.csv\")\nevents\n</code></pre> recording id timestamp [ns] name type 0 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417853032000000 recording.begin recording 1 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417856195000000 owl_start recording 2 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417861273000000 owl_end recording 3 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417861399000000 fox_start recording 4 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417866475000000 fox_end recording 5 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417866613000000 deer_start recording 6 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417872348000000 deer_end recording 7 49e4a972-7d6b-4b42-b931-bf64b91f952b 1644417872441000000 recording.end recording <pre><code>fixations = pd.read_csv(\"raw-data-export/george-49e4a972/fixations.csv\")\nfixations.head()\n</code></pre> section id recording id fixation id start timestamp [ns] end timestamp [ns] duration [ms] fixation x [px] fixation y [px] 0 5b682999-fb7d-42c3-9da0-2253ece6299b 49e4a972-7d6b-4b42-b931-bf64b91f952b 1 1644417853698031394 1644417853910023394 211 651.751 731.750 1 5b682999-fb7d-42c3-9da0-2253ece6299b 49e4a972-7d6b-4b42-b931-bf64b91f952b 2 1644417853978009394 1644417854338008394 359 528.268 881.095 2 5b682999-fb7d-42c3-9da0-2253ece6299b 49e4a972-7d6b-4b42-b931-bf64b91f952b 3 1644417854410160394 1644417856542104394 2131 655.423 668.033 3 5b682999-fb7d-42c3-9da0-2253ece6299b 49e4a972-7d6b-4b42-b931-bf64b91f952b 4 1644417856590003394 1644417857238017394 648 523.187 677.434 4 5b682999-fb7d-42c3-9da0-2253ece6299b 49e4a972-7d6b-4b42-b931-bf64b91f952b 5 1644417857298000394 1644417858693973394 1395 772.743 640.843 <p>We can now simply iterate through the recordings and filter the fixation data using the start and end timestamps, to calculate the average number of fixations for every image and subject.</p> <pre><code>import os\nimport json\n\nexport_folder = \"raw-data-export/\"\n\nresults = pd.DataFrame(columns=image_names)\n\nfor f in os.listdir(export_folder):\n    rec_folder = os.path.join(export_folder, f)\n    if not os.path.isdir(rec_folder):\n        continue\n\n    # Read all relevant files\n    info_path = os.path.join(rec_folder, \"info.json\")\n    with open(info_path) as info:\n        rec_name = json.load(info)[\"template_data\"][\"recording_name\"]\n\n    events_path = os.path.join(rec_folder, \"events.csv\")\n    events = pd.read_csv(events_path)\n\n    fixations_path = os.path.join(rec_folder, \"fixations.csv\")\n    fixations = pd.read_csv(fixations_path)\n\n    # Calculate average fixation duration per recording and image\n    for name in image_names:\n        start_event = events[events[\"name\"] == name + \"_start\"]\n        start_timestamp = start_event[\"timestamp [ns]\"].values[0]\n\n        end_event = events[events[\"name\"] == name + \"_end\"]\n        end_timestamp = end_event[\"timestamp [ns]\"].values[0]\n\n        condition = (fixations[\"start timestamp [ns]\"] &gt;= start_timestamp) &amp; (fixations[\"end timestamp [ns]\"] &lt;= end_timestamp)\n        image_fixations = fixations[condition]\n\n        results.loc[rec_name, name] = len(image_fixations[\"duration [ms]\"])\n\nresults.loc[\"Mean\"] = results.mean()\n\nresults\n</code></pre> owl fox deer George 7 10 9 Jane 7 5 10 John 10 7 7 Lisa 5 7 9 Steve 7 8 11 Mean 7.2 7.4 9.2 <p>Visualized as a bar chart it looks as follows:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\n\nfor idx, name in enumerate(image_names):\n    plt.bar(idx, results.loc[\"Mean\", name])\n\nplt.xticks(range(len(image_names)), image_names)\nplt.xlabel(\"Image\")\nplt.ylabel(\"Number of Fixations\")\n</code></pre> <pre><code>Text(0, 0.5, 'Number of Fixations')\n</code></pre> <p></p>"},{"location":"cookbook/track-your-experiment-progress-using-events/#conclusion","title":"Conclusion","text":"<p>In this guide, you saw how to use events to track the progress of an experiment. Note that this approach can be generalized to much more complex setups.</p>"},{"location":"guides/simple-vs-async-api/","title":"Simple vs. Async API","text":"<p>The module provides two conceptionally different APIs:</p> <ol> <li> <p>The <code>async</code> interface is using Python's asyncio in order to implement non-blocking asynchronous communication. It provides composable components for each feature of the Network API. This means that you need to select and combine the functionality for your own needs, e.g. [stream_video_with_overlayed_gaze][../methods/async/streaming/scene-camera.md#] example. The <code>async</code> functionality is implemented in the top-level <code>pupil_labs.realtime_api</code> namespace.</p> </li> <li> <p>The <code>simple</code> interface wraps around the <code>async</code> one, sacrificing flexibility for the sake of ease of use. It tries to anticipate and provide a solution for the most common use cases. Note, all <code>simple</code> API functionality can be found in the <code>pupil_labs.realtime_api.simple</code> namespace.</p> </li> </ol> <p>Do not mix async and simple components!</p> <p>The <code>async</code> and <code>simple</code> components are not compatible with each other! Do not mix   them!</p>"},{"location":"guides/simple-vs-async-api/#device-classes","title":"Device Classes","text":"<p>There are three different <code>Device</code> classes that one needs to differentiate:</p> <ol> <li> <p><code>pupil_labs.realtime_api.simple.device.Device</code>    Easy-to-use class that auto-connects to the Companion device on <code>__init__</code> to request the current status and to keep it up-to-date via the Websocket API. The phone's state is mirrored and cached in the corresponding attributes. Accessing the attributes is instant and does not need to perform a request to the phone. The class initiates streaming on demand (pupil_labs.realtime_api.simple.Device.streaming_start) or when needed (any of the <code>simple.Device.receive_*</code>methods being called). Can be initialised with an explicit IP address and port number. Also, returned by the<code>pupil_labs.realtime_api.simple.discover_*</code> functions.</p> </li> <li> <p><code>pupil_labs.realtime_api.models.DiscoveredDeviceInfo</code>    Meta-information about discovered devices, e.g. IP address, port number, etc. Not    able to initiate any connections on its own. Used to configure the following class:</p> </li> <li> <p><code>pupil_labs.realtime_api.device.Device</code>    Does not connect to the Companion device on <code>__init__</code> - only explicitly on calls like pupil_labs.realtime_api.device.Device.get_status. Subscription to the <code>Websocket API</code>, streaming, and <code>clock offset estimation</code> are implemented in separate components.</p> </li> </ol>"},{"location":"guides/simple-vs-async-api/#when-to-use-which-api","title":"When to use which API?","text":"<p>The <code>simple</code> API is the best choice for most users. It is easy to use and provides a high-level interface for common tasks. The <code>async</code> API is more flexible and powerful, but it requires a deeper understanding of asynchronous programming and the underlying components. If you need to implement custom functionality or have specific performance requirements, the <code>async</code> API may be the better choice.</p>"},{"location":"guides/under-the-hood/","title":"Under The Hood","text":"<p>This guide explains how the Pupil Labs' Realtime API works on the wire and how this client library abstracts away some of the complexities of the underlying protocols.</p>"},{"location":"guides/under-the-hood/#http-rest-api","title":"HTTP REST API","text":"<p>The Neon / Pupil Invisible Companion app hosts an HTTP REST API that can be used to query the phone's current state, remote control it, and look up information about available data streams.</p> <p>By default, the API is hosted at <code>http://neon.local:8080/</code> / <code>http://pi.local:8080/</code>, but the app will fallback to a different DNS name and/or port if the default values are taken by another app already.</p> <p>The current connection details can be looked up under the app's main menu \u2192 Streaming or taping on the Streaming icon on Neon. Alternatively, you can use Service discovery in the local network to find available devices.</p> <p>Note</p> <p>The device serves the built-in monitor web app at the document root <code>/</code>. The API is served under the <code>/api</code> path. You can find the full OpenAPI 3 specification of the REST API here.</p>"},{"location":"guides/under-the-hood/#startstopcancel-recordings","title":"Start/stop/cancel recordings","text":"<p>By sending HTTP POST requests to the <code>/api/recording:*</code> endpoints, you can start, stop, and cancel recordings.</p> <ul> <li><code>POST /api/recording:start</code> - Starts a recording if possible</li> <li><code>POST /api/recording:stop_and_save</code> - Stops and saves the running recording if possible</li> <li><code>POST /api/recording:cancel</code> - Stops and discards the running recording if possible</li> </ul> <p>Warning</p> <p>In specific situations, the app will not comply with the request to start a new recording:</p> <ul> <li>the selected template has required fields</li> <li>the available storage amount is too low</li> <li>the device battery is too low</li> <li>no wearer has been selected</li> <li>no workspace has been selected</li> <li>the setup bottom sheets have not been completed</li> </ul> <p>Seealso</p> <p><code>Simple</code> blocking implementations</p> <ul> <li><code>pupil_labs.realtime_api.simple.Device.recording_start</code></li> <li><code>pupil_labs.realtime_api.simple.Device.recording_stop_and_save</code></li> <li><code>pupil_labs.realtime_api.simple.Device.recording_cancel</code></li> </ul> <p>Asynchronous implementations</p> <ul> <li><code>pupil_labs.realtime_api.device.Device.recording_start</code></li> <li><code>pupil_labs.realtime_api.device.Device.recording_stop_and_save</code></li> <li><code>pupil_labs.realtime_api.device.Device.recording_cancel</code></li> </ul>"},{"location":"guides/under-the-hood/#send-events","title":"Send events","text":"<p>By HTTP POSTing requests to the <code>/api/event</code> endpoint, you can send labeled events to the device. Events will be timestamped on reception. Alternatively, you can provide a Unix-epoch timestamp in nanosecond. This is recommended if you want to control the timing of the event.</p> <ul> <li><code>POST /api/event</code> - Sends an event to the device</li> </ul> <p>Seealso</p> <p>Implementations</p> <pre><code>- `Simple` blocking: [`pupil_labs.realtime_api.simple.Device.send_event`][]\n- Asynchronous: [`pupil_labs.realtime_api.device.Device.send_event`][]\n</code></pre>"},{"location":"guides/under-the-hood/#get-current-status","title":"Get Current Status","text":"<p>By sending a HTTP GET request to the <code>/api/status</code> endpoint, you can receive information about the device's current status. This includes information about the battery and storage capacities, connected sensors, and running recordings.</p> <ul> <li><code>GET /api/status</code> - Receive status from device</li> </ul> <p>Seealso</p> <p>Asynchronous implementations: <code>pupil_labs.realtime_api.device.Device.get_status</code></p>"},{"location":"guides/under-the-hood/#websocket-api","title":"Websocket API","text":"<p>In addition to the HTTP REST API above, the Neon / Pupil Invisible Companion device also pushes status updates via a websocket connection. It is hosted on the same port as the REST API. By default, you can connect to it via <code>ws://neon.local:8080/api/status</code> or <code>ws://pi.local:8080/api/status</code>.</p> <p>Tip</p> <p>You can use this website to test the websocket connection.</p> <p>The messages published via this connection have the same format as the Get Current Status endpoint.</p>"},{"location":"guides/under-the-hood/#streaming-api","title":"Streaming API","text":"<p>The Neon / Pupil Invisible Companion app uses the RTSP protocol (RFC 2326) to stream scene video and gaze data. Under the hood, communication is three-fold:</p> <ul> <li>RTSP (RealTime Streaming Protocol) - Provides meta data about the corresponding stream</li> <li>RTP (Realtime Transport Protocol) - Data delivery channel, contains actual payloads</li> <li>RTCP (RTP Control Protocol) - Provides absolute time information to align multiple streams</li> </ul> <p>The necessary connection information is made available via the Sensor model as part of the Get Current Status and Websocket API.</p> <p>The RTSP connection URL follows the following pattern:</p> <pre><code>rtsp://&lt;ip&gt;:&lt;port&gt;/?&lt;params&gt;\n</code></pre> <p>Info</p> <p>Each stream is available via two connection types:</p> <ul> <li><code>DIRECT</code> - direct RTSP connection, as described in this document</li> <li><code>WEBSOCKET</code> - tunneling RTSP over a websocket connection to make it available to web browsers</li> </ul> <p>Seealso</p> <p>The Realtime Network API exposes this information via <code>pupil_labs.realtime_api.models.Status.direct_world_sensor</code> and <code>pupil_labs.realtime_api.models.Status.direct_gaze_sensor</code>, returning <code>pupil_labs.realtime_api.models.Sensor</code> instances.</p>"},{"location":"guides/under-the-hood/#rtsp","title":"RTSP","text":"<p>Abstract</p> <p>The Real Time Streaming Protocol, or RTSP, is an application-level protocol for control over the delivery of data with real-time properties.</p> <p>Source: https://datatracker.ietf.org/doc/html/rfc2326</p> <p>Of the various methods defined in the RTSP protocol, SETUP and DESCRIBE are particularly important for the transmission of the stream's meta and connection information.</p> <p>During the SETUP method, client and server exchange information about their corresponding port numbers for the RTP and RTCP connections.</p> <p>The DESCRIBE response contains SDP(Session Description Protocol) data, describing the following stream attributes (via the media's rtpmap):</p> <ul> <li><code>encoding</code> - The encoding of the stream, e.g. <code>H264</code></li> <li><code>clockRate</code> - The clock rate of the stream's relative clock</li> </ul> <p>For video, it also exposes the sprop-parameter-sets via its format-specific parameters (<code>fmtp</code>). These contain crucial information in order to initialize the corresponding video decoder.</p> <p>Danger</p> <p>Each stream has its own clock rate. For temporal alignment, the clock offset between the stream's relative clock and the absolute NTP clock has to be calculated. See RTCP below.</p> <p>Seealso</p> <p>To encode gaze data, a custom encoding called <code>com.pupillabs.gaze1</code> is used. You can find more information about it below.</p>"},{"location":"guides/under-the-hood/#rtp","title":"RTP","text":"<p>Abstract</p> <p>[The real-time transport protocol] provides end-to-end network transport functions suitable for applications transmitting real-time data, such as audio, video or simulation data, over multicast or unicast network services. [...] The data transport is augmented by a control protocol (RTCP) [...]. RTP and RTCP are designed to be independent of the underlying transport and network layers.</p> <p>Source: https://datatracker.ietf.org/doc/html/rfc3550</p> <p>Payloads can be split across multiple RTP packets. Their order can be identified via the packet header's sequence number. Packets belonging to the same payload have the same timestamp. The payloads can be decoded individually. See Decoding Gaze Data and Decoding Video Data below.</p> <p>Seealso</p> <p>Read more about the RTP timestamp mechanism here.</p> <p>Seealso</p> <p>The Realtime Python API exposes raw RTP data via <code>pupil_labs.realtime_api.streaming.base.RTSPRawStreamer.receive</code> and calculates relative RTP packet timestamps in [<code>pupil_labs.realtime_api.streaming.base._WallclockRTSPReader.relative_timestamp_from_packet</code>][].</p>"},{"location":"guides/under-the-hood/#rtcp","title":"RTCP","text":"<p>The most important role that the RTP control protocol plays for the Pupil Labs Realtime Network API is to provide timestamps in relative stream time and in absolute NTP time (SR RTCP Packet type).</p> <p>Relative timestamps are calculated by dividing the packet timestamp (numerator) by the clock rate (denominator), e.g. a timestamp of 250 at a clock rate of 50 Hz corresponds to <code>250 / 50 = 5</code> seconds.</p> <p>Abstract</p> <p>Wallclock time (absolute date and time) is represented using the timestamp format of the Network Time Protocol (NTP), which is in seconds relative to 1 January 1900 00:00:00 UTC. The full resolution NTP timestamp is a 64-bit unsigned fixed-point number with the integer part in the first 32 bits and the fractional part in the last 32 bits.</p> <p>Source: https://datatracker.ietf.org/doc/html/rfc3550#section-4</p> <p>Knowing time points in both corresponding clocks, relative and absolute one, allows one to calculate the clock offset between the two clocks. This is done by subtracting the one from the other. The offset is then added to new relative timestamps to get the corresponding time.</p> <p>Danger</p> <p>The Realtime Python API converts absolute NTP timestamps to nanoseconds in Unix epoch (time since 1 January 1970 00:00:00 UTC). This corresponds to the same time base and unit returned by <code>time.time_ns</code>.</p>"},{"location":"guides/under-the-hood/#decoding-gaze-data","title":"Decoding Gaze Data","text":"<p>Gaze data is encoded in network byte order (big-endian) and consists of:</p> <ol> <li><code>x</code> - Horizontal component of the gaze location in pixels within the scene camera's    coordinate system. The value is encoded as a 32-bit float.</li> <li><code>y</code> - Vertical component of the gaze location in pixels within the scene camera's    coordinate system. The value is encoded as a 32-bit float.</li> <li><code>worn</code> - Boolean indicating whether the user is wearing the device. The value is    encoded as an unsigned 8-bit integer as either <code>255</code> (device is being worn) or <code>0</code> (device is not being worn).</li> </ol> <p>Eye State Data (Optional)</p> <p>If eye state computation is enabled (not available for Pupil Invisible), additional parameters are included(all encoded as a 32-bit float):</p> <ol> <li><code>pupil_diameter_left</code>: Physical diameter of the left pupil in millimetres.</li> <li><code>eyeball_center_left_x</code>: X-coordinate of the left eyeball centre relative to the scene camera.</li> <li><code>eyeball_center_left_y</code>: Y-coordinate of the left eyeball centre relative to the scene camera.</li> <li><code>eyeball_center_left_z</code>: Z-coordinate of the left eyeball centre relative to the scene camera.</li> <li><code>optical_axis_left_x</code>: X-component of the left eye's optical axis vector.</li> <li><code>optical_axis_left_y</code>: Y-component of the left eye's optical axis vector.</li> <li><code>optical_axis_left_z</code>: Z-component of the left eye's optical axis vector.</li> <li><code>pupil_diameter_right</code>: Physical diameter of the right pupil in millimetres.</li> <li><code>eyeball_center_right_x</code>: X-coordinate of the right eyeball centre relative to the scene camera.</li> <li><code>eyeball_center_right_y</code>: Y-coordinate of the right eyeball centre relative to the scene camera.</li> <li><code>eyeball_center_right_z</code>: Z-coordinate of the right eyeball centre relative to the scene camera.</li> <li><code>optical_axis_right_x</code>: X-component of the right eye's optical axis vector.</li> <li><code>optical_axis_right_y</code>: Y-component of the right eye's optical axis vector.</li> <li><code>optical_axis_right_z</code>: Z-component of the right eye's optical axis vector.</li> <li><code>timestamp_unix_seconds</code>: Unix timestamp representing the time of data capture.</li> </ol> <p>Each RTP packet contains one gaze datum. The payload length varies:</p> <ul> <li>21 bytes: When only gaze data is included. To unpack - (!ffB)</li> <li>77 bytes: When both gaze and eye state data are included. To unpack - (!ffBffffffffffffff)</li> </ul> <p>Tip</p> <p>RTSP packets can be captured and analysed using Wireshark, a comprehensive network protocol analyser. This tool allows detailed inspection of packet data for in-depth analysis and troubleshooting.</p> <p>Seealso</p> <p>The Realtime Python API exposes gaze data via <code>pupil_labs.realtime_api.streaming.gaze.RTSPGazeStreamer.receive</code> and <code>pupil_labs.realtime_api.streaming.gaze</code>.</p>"},{"location":"guides/under-the-hood/#decoding-video-data","title":"Decoding Video Data","text":"<p>Video frames are split across multiple RTP packets. The payload is wrapped in the additional Network Abstraction Layer (NAL). This allows finding frame boundaries across fragmented payloads without relying on the RTP meta information.</p> <p>Once the data is unpacked from the NAL, it can be passed to a corresponding video decoder, e.g. <code>pyav's av.CodecContext</code>.</p> <p>Important</p> <p>The video decoder needs to be initialized with the sprop-parameter-sets exposed via the RTSP DESCRIBE method.</p> <p>Seealso</p> <p>The Realtime Python API implements the NAL unpacking here: <code>pupil_labs.realtime_api.streaming.nal_unit.extract_payload_from_nal_unit</code></p>"},{"location":"guides/under-the-hood/#service-discovery-in-the-local-network","title":"Service discovery in the local network","text":"<p>To avoid having to manually copy the IP address from the Neon / Pupil Invisible Companion user interface, the application announces its REST API endpoint via multicast DNS service discovery. Specifically, it announces a service of type <code>_http._tcp.local.</code> and uses the following naming pattern:</p> <pre><code>PI monitor:&lt;phone name&gt;:&lt;phone hardware id&gt;._http._tcp.local.\n</code></pre> <p>Seealso</p> <p>The service name is exposed via - <code>pupil_labs.realtime_api.models.DiscoveredDeviceInfo.name</code> and - [<code>pupil_labs.realtime_api.base.DeviceBase.full_name</code>[]].</p> <p>The phone name component is exposed via - <code>pupil_labs.realtime_api.models.Phone.device_name</code> and - <code>pupil_labs.realtime_api.simple.Device.phone_name</code>.</p> <p>The phone hardware id component is exposed via - <code>pupil_labs.realtime_api.models.Phone.device_id</code> and - <code>pupil_labs.realtime_api.simple.Device.phone_id</code>.</p> <p>The client's <code>pupil_labs.realtime_api.discovery</code> module uses the <code>zeroconf</code> Python package under the hood to discover services.</p>"},{"location":"methods/","title":"Code Examples","text":"<ul> <li>Simple API Examples</li> <li>Async API Examples</li> </ul>"},{"location":"methods/async/","title":"Asynchronous Examples","text":"<p>Code examples and methods that use the async api mode. Includes descriptions of how to use them.</p>"},{"location":"methods/async/#connect-to-a-device","title":"Connect to a Device","text":"<ul> <li>Discover Devices</li> <li>Device Information</li> <li>Automatic Status Updates</li> </ul>"},{"location":"methods/async/#remote-control-devices","title":"Remote Control Devices","text":"<ul> <li>Start, Stop, Save and Cancel Recordings</li> <li>Send Event</li> </ul>"},{"location":"methods/async/#streaming","title":"Streaming","text":"<ul> <li>Gaze Data</li> <li>Scene Camera Video</li> <li>Scene Camera Video with Overlayed Gaze</li> <li>Scene Camera Video with Overlayed Eyes Video and Gaze Circle</li> <li>Eyes Camera Video</li> <li>IMU Data</li> <li>Blinks, Fixations &amp; Saccades</li> </ul>"},{"location":"methods/async/#templates","title":"Templates","text":"<ul> <li>Fetch, fill and save templates</li> </ul>"},{"location":"methods/async/#others","title":"Others","text":"<ul> <li>Camera Calibration</li> <li>Time Offset</li> </ul>"},{"location":"methods/simple/","title":"Simple Examples","text":"<p>Code examples and methods that use the simple api mode. Includes descriptions of how to use them.</p>"},{"location":"methods/simple/#connect-to-a-device","title":"Connect to a Device","text":"<ul> <li>Discover Devices</li> <li>Device Information &amp; Updates</li> </ul>"},{"location":"methods/simple/#remote-control-devices","title":"Remote Control Devices","text":"<ul> <li>Start, Stop, Save, and Cancel Recordings</li> <li>Save Events</li> </ul>"},{"location":"methods/simple/#streaming","title":"Streaming","text":"<ul> <li>Gaze Data</li> <li>Scene Camera Video</li> <li>Scene Camera Video with Overlayed Gaze</li> <li>Scene Camera Video with Overlayed Eyes Video and Gaze Circle</li> <li>Eyes Camera Video</li> <li>IMU Data</li> <li>Blinks, Fixations &amp; Saccades</li> </ul>"},{"location":"methods/simple/#templates","title":"Templates","text":"<ul> <li>Fetch, Fill and Save Templates</li> </ul>"},{"location":"methods/simple/#others","title":"Others","text":"<ul> <li>Camera Calibration</li> <li>Time Offset</li> </ul>"},{"location":"methods/async/connect-to-a-device/","title":"Connect to a Device","text":"<p>One of the first steps you need to carry to leverage the API is to connect to one device. This library offers you ways to find one or multiple devices connected to your local network. See below the different ways to connect to a device:</p> Discover DevicesConnect Directly via IP Address <p>Using the <code>discover_devices</code>:</p> discover_devices.py<pre><code>import asyncio\nimport contextlib\n\nfrom pupil_labs.realtime_api.discovery import Network, discover_devices\n\n\nasync def main():\n    async with Network() as network:\n        print(\"Looking for the next best device...\\n\\t\", end=\"\")\n        print(await network.wait_for_new_device(timeout_seconds=5))\n\n        print(\"---\")\n        print(\"All devices after searching for additional 5 seconds:\")\n        await asyncio.sleep(5)\n        print(network.devices)\n\n    print(\"---\")\n    print(\"Starting new, indefinitive search... hit ctrl-c to stop.\")\n    # optionally set timeout_seconds argument to limit search duration\n    async for device_info in discover_devices():\n        print(f\"\\t{device_info}\")\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre> <pre><code>from pupil_labs.realtime_api.device import Device\n\n# This address is just an example. Find out the actual IP address of your device!\nip = \"192.168.1.169\"\ndevice = Device(address=ip, port=\"8080\")\n</code></pre> <p>Make sure the Companion App is running! If no device can be found, please have a look at the troubleshooting section.</p> <p>Below you can find a link to the full code example and the API referece for the returned Device object.</p>"},{"location":"methods/async/connect-to-a-device/#device-information-automatic-status-updates","title":"Device Information &amp; Automatic Status Updates","text":"<p>Once connected, the <code>Device</code> object, alows you to retrieve <code>Status</code> updates by calling <code>get_status</code>.</p> <p>This <code>Status</code> represents the full Companion's Device state, including sub-classes representing:</p> <ul> <li> <p>Phone</p> </li> <li> <p>Hardware</p> </li> <li> <p>Sensors</p> </li> <li> <p>Recording</p> </li> </ul> Get Current StatusUpdate via Callback <p>get_status.py<pre><code>    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n\n        print(f\"Device IP address: {status.phone.ip}\")\n        print(f\"Battery level: {status.phone.battery_level} %\")\n\n        print(f\"Connected glasses: SN {status.hardware.glasses_serial}\")\n        print(f\"Connected scene camera: SN {status.hardware.world_camera_serial}\")\n\n        world = status.direct_world_sensor()\n        print(f\"World sensor: connected={world.connected} url={world.url}\")\n\n        gaze = status.direct_gaze_sensor()\n        print(f\"Gaze sensor: connected={gaze.connected} url={gaze.url}\")\n</code></pre> <pre><code>Device IP address: 192.168.1.60\nBattery level: 78 %\nConnected glasses: SN -1\nConnected scene camera: SN -1\nWorld sensor: connected=True url=rtsp://192.168.1.60:8086/?camera=world&amp;audioenable=on\nGaze sensor: connected=True url=rtsp://192.168.1.60:8086/?camera=gaze&amp;audioenable=on\n</code></pre></p> <p><pre><code>    async with Device.from_discovered_device(dev_info) as device:\n        duration = 20\n        print(f\"Starting auto-update for {duration} seconds\")\n        # callbacks can be awaitable, too\n        notifier = StatusUpdateNotifier(device, callbacks=[print_component])\n        await notifier.receive_updates_start()\n        await asyncio.sleep(duration)\n        print(\"Stopping auto-update\")\n        await notifier.receive_updates_stop()\n</code></pre> <pre><code>Starting auto-update for 20 seconds\nPhone(battery_level=77, battery_state='OK', device_id='d55a33b5ab845785', device_name='Neon Companion', ip='192.168.1.60', memory=99877777408, memory_state='OK', time_echo_port=12321)\nHardware(version='2.0', glasses_serial='-1', world_camera_serial='-1', module_serial='841684')\nSensor(sensor='imu', conn_type='WEBSOCKET', connected=True, ip='192.168.1.60', params='camera=imu&amp;audioenable=off', port=8686, protocol='rtsp', stream_error=False)\nSensor(sensor='imu', conn_type='DIRECT', connected=True, ip='192.168.1.60', params='camera=imu&amp;audioenable=on', port=8086, protocol='rtsp', stream_error=False)\nSensor(sensor='world', conn_type='WEBSOCKET', connected=True, ip='192.168.1.60', params='camera=world&amp;audioenable=off', port=8686, protocol='rtsp', stream_error=False)\n</code></pre></p> <p>Refer to the Device API documentation for more details.</p> Device DeviceBase Status"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device","title":"Device","text":"<pre><code>Device(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>DeviceBase</code></p> <p>Class representing a Pupil Labs device.</p> <p>This class provides methods to interact with the device, such as starting and stopping recordings, sending events, and fetching device status. It also provides a context manager for automatically closing the device session.</p> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>close</code>             \u2013              <p>Close the connection to the device.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> <li> <code>get_calibration</code>             \u2013              <p>Get the current cameras calibration data.</p> </li> <li> <code>get_status</code>             \u2013              <p>Get the current status of the device.</p> </li> <li> <code>get_template</code>             \u2013              <p>Get the template currently selected on device.</p> </li> <li> <code>get_template_data</code>             \u2013              <p>Get the template data entered on device.</p> </li> <li> <code>post_template_data</code>             \u2013              <p>Set the data for the currently selected template.</p> </li> <li> <code>recording_cancel</code>             \u2013              <p>Cancel the current recording without saving it.</p> </li> <li> <code>recording_start</code>             \u2013              <p>Start a recording on the device.</p> </li> <li> <code>recording_stop_and_save</code>             \u2013              <p>Stop and save the current recording.</p> </li> <li> <code>send_event</code>             \u2013              <p>Send an event to the device.</p> </li> <li> <code>status_updates</code>             \u2013              <p>Stream status updates from the device.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>active_session</code>               (<code>ClientSession</code>)           \u2013            <p>Returns the active session, raising an error if it's None.</p> </li> <li> <code>session</code>               (<code>ClientSession | None</code>)           \u2013            <p>The HTTP session used for making requests.</p> </li> <li> <code>template_definition</code>               (<code>Template | None</code>)           \u2013            <p>The template definition currently selected on the device.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the Device class.\"\"\"\n    super().__init__(*args, **kwargs)\n    self._create_client_session()\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.active_session","title":"active_session  <code>property</code>","text":"<pre><code>active_session: ClientSession\n</code></pre> <p>Returns the active session, raising an error if it's None.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.session","title":"session  <code>instance-attribute</code>","text":"<pre><code>session: ClientSession | None\n</code></pre> <p>The HTTP session used for making requests.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.template_definition","title":"template_definition  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>template_definition: Template | None = None\n</code></pre> <p>The template definition currently selected on the device.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the connection to the device.</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the connection to the device.\"\"\"\n    await self.active_session.close()\n    self.session = None\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.get_calibration","title":"get_calibration  <code>async</code>","text":"<pre><code>get_calibration() -&gt; Calibration\n</code></pre> <p>Get the current cameras calibration data.</p> <p>Note that Pupil Invisible and Neon are calibration free systems, this refers to the intrinsincs and extrinsics of the cameras and is only available for Neon.</p> <p>Returns:</p> <ul> <li> <code>Calibration</code>           \u2013            <p>pupil_labs.neon_recording.calib.Calibration: The calibration data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_calibration(self) -&gt; Calibration:\n    \"\"\"Get the current cameras calibration data.\n\n    Note that Pupil Invisible and Neon are calibration free systems, this refers to\n    the intrinsincs and extrinsics of the cameras and is only available for Neon.\n\n    Returns:\n        pupil_labs.neon_recording.calib.Calibration: The calibration data.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.CALIBRATION)\n    ) as response:\n        if response.status != 200:\n            raise DeviceError(response.status, \"Failed to fetch calibration\")\n\n        raw_data = await response.read()\n        return cast(Calibration, Calibration.from_buffer(raw_data))\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status() -&gt; Status\n</code></pre> <p>Get the current status of the device.</p> <p>Returns:</p> <ul> <li> <code>Status</code> (              <code>Status</code> )          \u2013            <p>The current device status.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_status(self) -&gt; Status:\n    \"\"\"Get the current status of the device.\n\n    Returns:\n        Status: The current device status.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n    async with self.active_session.get(self.api_url(APIPath.STATUS)) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_status] Received status: {result}\")\n        return Status.from_dict(result)\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.get_template","title":"get_template  <code>async</code>","text":"<pre><code>get_template() -&gt; Template\n</code></pre> <p>Get the template currently selected on device.</p> <p>Returns:</p> <ul> <li> <code>Template</code> (              <code>Template</code> )          \u2013            <p>The currently selected template.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template can't be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template(self) -&gt; Template:\n    \"\"\"Get the template currently selected on device.\n\n    Returns:\n        Template: The currently selected template.\n\n    Raises:\n        DeviceError: If the template can't be fetched.\n\n    \"\"\"\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DEFINITION)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_def] Received template def: {result}\")\n        self.template_definition = Template(**result)\n        return self.template_definition\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.get_template_data","title":"get_template_data  <code>async</code>","text":"<pre><code>get_template_data(template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Get the template data entered on device.</p> <p>Parameters:</p> <ul> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the returned data. - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]} - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The template data in the requested format.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template's data could not be fetched.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def get_template_data(\n    self, template_format: TemplateDataFormat = \"simple\"\n) -&gt; Any:\n    \"\"\"Get the template data entered on device.\n\n    Args:\n        template_format (TemplateDataFormat): Format of the returned data.\n            - \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]}\n            - \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The template data in the requested format.\n\n    Raises:\n        DeviceError: If the template's data could not be fetched.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    async with self.active_session.get(\n        self.api_url(APIPath.TEMPLATE_DATA)\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(\n            f\"[{self}.get_template_data] Received data's template: {result}\"\n        )\n        if template_format == \"api\":\n            return result\n        elif template_format == \"simple\":\n            template = await self.get_template()\n            return template.convert_from_api_to_simple_format(result)\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.post_template_data","title":"post_template_data  <code>async</code>","text":"<pre><code>post_template_data(template_answers: dict[str, list[str]], template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Set the data for the currently selected template.</p> <p>Parameters:</p> <ul> <li> <code>template_answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>The template data to send.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the input data. - \"api\" accepts the data as in realtime api format e.g.,     {\"item_uuid\": [\"42\"]} - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result of the operation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the data can not be sent.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid data type.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If an invalid format is provided.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def post_template_data(\n    self,\n    template_answers: dict[str, list[str]],\n    template_format: TemplateDataFormat = \"simple\",\n) -&gt; Any:\n    \"\"\"Set the data for the currently selected template.\n\n    Args:\n        template_answers: The template data to send.\n        template_format (TemplateDataFormat): Format of the input data.\n            - \"api\" accepts the data as in realtime api format e.g.,\n                {\"item_uuid\": [\"42\"]}\n            - \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result of the operation.\n\n    Raises:\n        DeviceError: If the data can not be sent.\n        ValueError: If invalid data type.\n        AssertionError: If an invalid format is provided.\n\n    \"\"\"\n    assert template_format in get_args(TemplateDataFormat), (\n        f\"format should be one of {TemplateDataFormat}\"\n    )\n\n    self.template_definition = await self.get_template()\n\n    if template_format == \"simple\":\n        template_answers = (\n            self.template_definition.convert_from_simple_to_api_format(\n                template_answers\n            )\n        )\n\n    pre_populated_data = await self.get_template_data(template_format=\"api\")\n    errors = self.template_definition.validate_answers(\n        pre_populated_data | template_answers, template_format=\"api\"\n    )\n    if errors:\n        raise ValueError(errors)\n\n    # workaround for issue with api as it fails when passing in an empty list\n    # ie. it wants [\"\"] instead of []\n    template_answers = {\n        key: value or [\"\"] for key, value in template_answers.items()\n    }\n\n    async with self.active_session.post(\n        self.api_url(APIPath.TEMPLATE_DATA), json=template_answers\n    ) as response:\n        confirmation = await response.json()\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        result = confirmation[\"result\"]\n        logger.debug(f\"[{self}.get_template_data] Send data's template: {result}\")\n        return result\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.recording_cancel","title":"recording_cancel  <code>async</code>","text":"<pre><code>recording_cancel() -&gt; None\n</code></pre> <p>Cancel the current recording without saving it.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be cancelled. Possible reasons include: - Recording not running</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_cancel(self) -&gt; None:\n    \"\"\"Cancel the current recording without saving it.\n\n    Raises:\n        DeviceError: If the recording could not be cancelled.\n            Possible reasons include:\n            - Recording not running\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_CANCEL)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.recording_start","title":"recording_start  <code>async</code>","text":"<pre><code>recording_start() -&gt; str\n</code></pre> <p>Start a recording on the device.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>ID of the started recording.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be started. Possible reasons include: - Recording already running - Template has required fields - Low battery - Low storage - No wearer selected - No workspace selected - Setup bottom sheets not completed</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_start(self) -&gt; str:\n    \"\"\"Start a recording on the device.\n\n    Returns:\n        str: ID of the started recording.\n\n    Raises:\n        DeviceError: If recording could not be started. Possible reasons include:\n            - Recording already running\n            - Template has required fields\n            - Low battery\n            - Low storage\n            - No wearer selected\n            - No workspace selected\n            - Setup bottom sheets not completed\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_START)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.start_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        return cast(str, confirmation[\"result\"][\"id\"])\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.recording_stop_and_save","title":"recording_stop_and_save  <code>async</code>","text":"<pre><code>recording_stop_and_save() -&gt; None\n</code></pre> <p>Stop and save the current recording.</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If recording could not be stopped. Possible reasons include: - Recording not running - Template has required fields</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def recording_stop_and_save(self) -&gt; None:\n    \"\"\"Stop and save the current recording.\n\n    Raises:\n        DeviceError: If recording could not be stopped. Possible reasons include:\n            - Recording not running\n            - Template has required fields\n\n    \"\"\"\n    async with self.active_session.post(\n        self.api_url(APIPath.RECORDING_STOP_AND_SAVE)\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.stop_recording] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.send_event","title":"send_event  <code>async</code>","text":"<pre><code>send_event(event_name: str, event_timestamp_unix_ns: int | None = None) -&gt; Event\n</code></pre> <p>Send an event to the device.</p> <p>Parameters:</p> <ul> <li> <code>event_name</code>               (<code>str</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>event_timestamp_unix_ns</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional timestamp in unix nanoseconds. If None, the current time will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>The created event.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If sending the event fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def send_event(\n    self, event_name: str, event_timestamp_unix_ns: int | None = None\n) -&gt; Event:\n    \"\"\"Send an event to the device.\n\n    Args:\n        event_name: Name of the event.\n        event_timestamp_unix_ns: Optional timestamp in unix nanoseconds.\n            If None, the current time will be used.\n\n    Returns:\n        Event: The created event.\n\n    Raises:\n        DeviceError: If sending the event fails.\n\n    \"\"\"\n    event: dict[str, Any] = {\"name\": event_name}\n    if event_timestamp_unix_ns is not None:\n        event[\"timestamp\"] = event_timestamp_unix_ns\n\n    async with self.active_session.post(\n        self.api_url(APIPath.EVENT), json=event\n    ) as response:\n        confirmation = await response.json()\n        logger.debug(f\"[{self}.send_event] Received response: {confirmation}\")\n        if response.status != 200:\n            raise DeviceError(response.status, confirmation[\"message\"])\n        confirmation[\"result\"][\"name\"] = (\n            event_name  # As the API does not return the name yet\n        )\n        return Event.from_dict(confirmation[\"result\"])\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.device.Device.status_updates","title":"status_updates  <code>async</code>","text":"<pre><code>status_updates() -&gt; AsyncIterator[Component]\n</code></pre> <p>Stream status updates from the device.</p> <p>Yields:</p> <ul> <li> <code>Component</code> (              <code>AsyncIterator[Component]</code> )          \u2013            <p>Status update components as they arrive.</p> </li> </ul> <p>Auto-reconnect, see:     https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect</p> Source code in <code>src/pupil_labs/realtime_api/device.py</code> <pre><code>async def status_updates(self) -&gt; AsyncIterator[Component]:\n    \"\"\"Stream status updates from the device.\n\n    Yields:\n        Component: Status update components as they arrive.\n\n    Auto-reconnect, see:\n        https://websockets.readthedocs.io/en/stable/reference/asyncio/client.html#websockets.asyncio.client.connect\n\n    \"\"\"\n    websocket_status_endpoint = self.api_url(APIPath.STATUS, protocol=\"ws\")\n    async for websocket in websockets.connect(websocket_status_endpoint):\n        try:\n            async for message_raw in websocket:\n                message_json = json.loads(message_raw)\n                try:\n                    component = parse_component(message_json)\n                except UnknownComponentError:\n                    logger.warning(f\"Dropping unknown component: {component}\")\n                    continue\n                yield component\n        except websockets.ConnectionClosed:\n            logger.debug(\"Websocket connection closed. Reconnecting...\")\n            continue\n        except asyncio.CancelledError:\n            logger.debug(\"status_updates() cancelled\")\n            break\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.base.DeviceBase","title":"DeviceBase","text":"<pre><code>DeviceBase(address: str, port: int, full_name: str | None = None, dns_name: str | None = None, suppress_decoding_warnings: bool = True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class representing Realtime API host devices.</p> <p>This class provides the foundation for device implementations that connect to the Realtime API.</p> <p>Attributes:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>REST API server address.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>REST API server port.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>)           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>)           \u2013            <p>REST API server DNS name, e.g.<code>neon.local / pi.local.</code>.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>REST API server address.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>REST API server port.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>REST API server DNS name, e.g.<code>neon.local / pi.local.</code>.</p> </li> <li> <code>suppress_decoding_warnings</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to suppress libav decoding warnings.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    port: int,\n    full_name: str | None = None,\n    dns_name: str | None = None,\n    suppress_decoding_warnings: bool = True,\n):\n    \"\"\"Initialize the DeviceBase instance.\n\n    Args:\n        address (str): REST API server address.\n        port (int): REST API server port.\n        full_name (str | None): Full service discovery name.\n        dns_name (str | None): REST API server DNS name,\n            e.g.``neon.local / pi.local.``.\n        suppress_decoding_warnings: Whether to suppress libav decoding warnings.\n\n    \"\"\"\n    self.address: str = address\n    self.port: int = port\n    self.full_name: str | None = full_name\n    self.dns_name: str | None = dns_name\n    if suppress_decoding_warnings:\n        # suppress decoding warnings due to incomplete data transmissions\n        logging.getLogger(\"libav.h264\").setLevel(logging.CRITICAL)\n        logging.getLogger(\"libav.swscaler\").setLevel(logging.ERROR)\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.base.DeviceBase.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.base.DeviceBase.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.base.DeviceBase.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status","title":"Status  <code>dataclass</code>","text":"<pre><code>Status(phone: Phone, hardware: Hardware, sensors: list[Sensor], recording: Recording | None)\n</code></pre> <p>Represents the Companion's Device full status</p> <p>Methods:</p> <ul> <li> <code>direct_eye_events_sensor</code>             \u2013              <p>Get blinks, fixations sensor.</p> </li> <li> <code>direct_eyes_sensor</code>             \u2013              <p>Get the eye camera sensor with direct connection. Only available on Neon.</p> </li> <li> <code>direct_gaze_sensor</code>             \u2013              <p>Get the gaze sensor with direct connection.</p> </li> <li> <code>direct_imu_sensor</code>             \u2013              <p>Get the IMU sensor with direct connection.</p> </li> <li> <code>direct_world_sensor</code>             \u2013              <p>Get the scene camera sensor with direct connection.</p> </li> <li> <code>from_dict</code>             \u2013              <p>Create a Status from a list of raw components.</p> </li> <li> <code>matching_sensors</code>             \u2013              <p>Find sensors matching specified criteria.</p> </li> <li> <code>update</code>             \u2013              <p>Update Component.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>hardware</code>               (<code>Hardware</code>)           \u2013            <p>Information about glasses connected, won't be present if not connected</p> </li> <li> <code>phone</code>               (<code>Phone</code>)           \u2013            <p>Information about the connected phone. Always present.</p> </li> <li> <code>recording</code>               (<code>Recording | None</code>)           \u2013            <p>Current recording, if any.</p> </li> <li> <code>sensors</code>               (<code>list[Sensor]</code>)           \u2013            <p>\"List of sensor information.</p> </li> </ul>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.hardware","title":"hardware  <code>instance-attribute</code>","text":"<pre><code>hardware: Hardware\n</code></pre> <p>Information about glasses connected, won't be present if not connected</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.phone","title":"phone  <code>instance-attribute</code>","text":"<pre><code>phone: Phone\n</code></pre> <p>Information about the connected phone. Always present.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.recording","title":"recording  <code>instance-attribute</code>","text":"<pre><code>recording: Recording | None\n</code></pre> <p>Current recording, if any.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.sensors","title":"sensors  <code>instance-attribute</code>","text":"<pre><code>sensors: list[Sensor]\n</code></pre> <p>\"List of sensor information.</p>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.direct_eye_events_sensor","title":"direct_eye_events_sensor","text":"<pre><code>direct_eye_events_sensor() -&gt; Sensor | None\n</code></pre> <p>Get blinks, fixations sensor.</p> <p>Only available on Neon with Companion App version 2.9 or newer.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_eye_events_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get blinks, fixations _sensor_.\n\n    Only available on Neon with Companion App version 2.9 or newer.\n    \"\"\"\n    return next(\n        self.matching_sensors(SensorName.EYE_EVENTS, ConnectionType.DIRECT),\n        Sensor(\n            sensor=SensorName.EYE_EVENTS.value,\n            conn_type=ConnectionType.DIRECT.value,\n        ),\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.direct_eyes_sensor","title":"direct_eyes_sensor","text":"<pre><code>direct_eyes_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the eye camera sensor with direct connection. Only available on Neon.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_eyes_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the eye camera sensor with direct connection. Only available on Neon.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.EYES, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.EYES.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.direct_gaze_sensor","title":"direct_gaze_sensor","text":"<pre><code>direct_gaze_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the gaze sensor with direct connection.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_gaze_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the gaze sensor with direct connection.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.GAZE, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.GAZE.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.direct_imu_sensor","title":"direct_imu_sensor","text":"<pre><code>direct_imu_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the IMU sensor with direct connection.</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_imu_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the IMU sensor with direct connection.\"\"\"\n    return next(\n        self.matching_sensors(SensorName.IMU, ConnectionType.DIRECT),\n        Sensor(sensor=SensorName.IMU.value, conn_type=ConnectionType.DIRECT.value),\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.direct_world_sensor","title":"direct_world_sensor","text":"<pre><code>direct_world_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the scene camera sensor with direct connection.</p> Note <p>Pupil Invisible devices, the world camera can be detached</p> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def direct_world_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the scene camera sensor with direct connection.\n\n    Note:\n        Pupil Invisible devices, the world camera can be detached\n\n    \"\"\"\n    return next(\n        self.matching_sensors(SensorName.WORLD, ConnectionType.DIRECT),\n        Sensor(\n            sensor=SensorName.WORLD.value, conn_type=ConnectionType.DIRECT.value\n        ),\n    )\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(status_json_result: list[ComponentRaw]) -&gt; Status\n</code></pre> <p>Create a Status from a list of raw components.</p> <p>Parameters:</p> <ul> <li> <code>status_json_result</code>               (<code>list[ComponentRaw]</code>)           \u2013            <p>List of raw component dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Status</code> (              <code>Status</code> )          \u2013            <p>New Status instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>@classmethod\ndef from_dict(cls, status_json_result: list[ComponentRaw]) -&gt; \"Status\":\n    \"\"\"Create a Status from a list of raw components.\n\n    Args:\n        status_json_result: List of raw component dictionaries.\n\n    Returns:\n        Status: New Status instance.\n\n    \"\"\"\n    phone = None\n    recording = None\n    hardware = Hardware()\n    sensors = []\n    for dct in status_json_result:\n        try:\n            component = parse_component(dct)\n        except UnknownComponentError:\n            logger.warning(f\"Dropping unknown component: {dct}\")\n            continue\n        if isinstance(component, Phone):\n            phone = component\n        elif isinstance(component, Hardware):\n            hardware = component\n        elif isinstance(component, Sensor):\n            sensors.append(component)\n        elif isinstance(component, Recording):\n            recording = component\n        elif isinstance(component, NetworkDevice):\n            pass  # no need to handle NetworkDevice updates here\n        else:\n            logger.warning(f\"Unknown model class: {type(component).__name__}\")\n    sensors.sort(key=lambda s: (not s.connected, s.conn_type, s.sensor))\n    if not phone:\n        raise ValueError(\"Status data must include a 'Phone' component.\")\n    return cls(phone, hardware, sensors, recording)\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.matching_sensors","title":"matching_sensors","text":"<pre><code>matching_sensors(name: SensorName, connection: ConnectionType) -&gt; Iterator[Sensor]\n</code></pre> <p>Find sensors matching specified criteria.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>SensorName</code>)           \u2013            <p>Sensor name to match, or ANY to match any name.</p> </li> <li> <code>connection</code>               (<code>ConnectionType</code>)           \u2013            <p>Connection type to match, or ANY to match any type.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Sensor</code> (              <code>Sensor</code> )          \u2013            <p>Sensors matching the criteria.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def matching_sensors(\n    self, name: SensorName, connection: ConnectionType\n) -&gt; Iterator[Sensor]:\n    \"\"\"Find sensors matching specified criteria.\n\n    Args:\n        name: Sensor name to match, or ANY to match any name.\n        connection: Connection type to match, or ANY to match any type.\n\n    Yields:\n        Sensor: Sensors matching the criteria.\n\n    \"\"\"\n    for sensor in self.sensors:\n        if name is not SensorName.ANY and sensor.sensor != name.value:\n            continue\n        if (\n            connection is not ConnectionType.ANY\n            and sensor.conn_type != connection.value\n        ):\n            continue\n        yield sensor\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#pupil_labs.realtime_api.models.Status.update","title":"update","text":"<pre><code>update(component: Component) -&gt; None\n</code></pre> <p>Update Component.</p> <p>Parameters:</p> <ul> <li> <code>component</code>               (<code>Component</code>)           \u2013            <p>Component to update.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def update(self, component: Component) -&gt; None:\n    \"\"\"Update Component.\n\n    Args:\n        component: Component to update.\n\n    \"\"\"\n    if isinstance(component, Phone):\n        self.phone = component\n    elif isinstance(component, Hardware):\n        self.hardware = component\n    elif isinstance(component, Recording):\n        self.recording = component\n    elif isinstance(component, Sensor):\n        for idx, sensor in enumerate(self.sensors):\n            if (\n                sensor.sensor == component.sensor\n                and sensor.conn_type == component.conn_type\n            ):\n                self.sensors[idx] = component\n                break\n</code></pre>"},{"location":"methods/async/connect-to-a-device/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here <p>discover_devices.py<pre><code>import asyncio\nimport contextlib\n\nfrom pupil_labs.realtime_api.discovery import Network, discover_devices\n\n\nasync def main():\n    async with Network() as network:\n        print(\"Looking for the next best device...\\n\\t\", end=\"\")\n        print(await network.wait_for_new_device(timeout_seconds=5))\n\n        print(\"---\")\n        print(\"All devices after searching for additional 5 seconds:\")\n        await asyncio.sleep(5)\n        print(network.devices)\n\n    print(\"---\")\n    print(\"Starting new, indefinitive search... hit ctrl-c to stop.\")\n    # optionally set timeout_seconds argument to limit search duration\n    async for device_info in discover_devices():\n        print(f\"\\t{device_info}\")\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre> device_status_get_current.py<pre><code>import asyncio\n\nfrom pupil_labs.realtime_api import Device, Network\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n\n        print(f\"Device IP address: {status.phone.ip}\")\n        print(f\"Battery level: {status.phone.battery_level} %\")\n\n        print(f\"Connected glasses: SN {status.hardware.glasses_serial}\")\n        print(f\"Connected scene camera: SN {status.hardware.world_camera_serial}\")\n\n        world = status.direct_world_sensor()\n        print(f\"World sensor: connected={world.connected} url={world.url}\")\n\n        gaze = status.direct_gaze_sensor()\n        print(f\"Gaze sensor: connected={gaze.connected} url={gaze.url}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre></p> device_status_update_via_callback.py<pre><code>import asyncio\n\nfrom pupil_labs.realtime_api import Device, Network, StatusUpdateNotifier\n\n\ndef print_component(component):\n    print(component)\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        duration = 20\n        print(f\"Starting auto-update for {duration} seconds\")\n        # callbacks can be awaitable, too\n        notifier = StatusUpdateNotifier(device, callbacks=[print_component])\n        await notifier.receive_updates_start()\n        await asyncio.sleep(duration)\n        print(\"Stopping auto-update\")\n        await notifier.receive_updates_stop()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"methods/async/others/","title":"Others","text":""},{"location":"methods/async/others/#time-offset-estimation","title":"Time Offset Estimation","text":"<p>1.1.0</p> <p>Additionally, we offer you a convenient function to estimate the time offset between the device and your computer using the <code>TimeOffsetEstimator.estimate</code> method.</p> <p>See <code>time_echo</code> for details.</p> <pre><code>        time_offset_estimator = TimeOffsetEstimator(\n            status.phone.ip, status.phone.time_echo_port\n        )\n</code></pre> <pre><code>Mean time offset: -37.53 ms\nMean roundtrip duration: 12.91 ms\n</code></pre> Check the whole example code here device_time_offset.py<pre><code>import asyncio\n\nfrom pupil_labs.realtime_api import Device, Network\nfrom pupil_labs.realtime_api.time_echo import TimeOffsetEstimator\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n\n        print(f\"Device IP address: {status.phone.ip}\")\n        print(f\"Device Time Echo port: {status.phone.time_echo_port}\")\n\n        if status.phone.time_echo_port is None:\n            print(\n                \"You Pupil Invisible Companion app is out-of-date and does not yet \"\n                \"support the Time Echo protocol. Upgrade to version 1.4.28 or newer.\"\n            )\n            return\n\n        time_offset_estimator = TimeOffsetEstimator(\n            status.phone.ip, status.phone.time_echo_port\n        )\n        estimated_offset = await time_offset_estimator.estimate()\n        print(f\"Mean time offset: {estimated_offset.time_offset_ms.mean} ms\")\n        print(\n            f\"Mean roundtrip duration: {estimated_offset.roundtrip_duration_ms.mean} ms\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Wanna get super precise time sync?</p> <p>Have a look at our tutorial here.</p>"},{"location":"methods/async/others/#camera-calibration","title":"Camera calibration","text":"<p>Neon</p> <p>Getting the camera calibration coefficients can be extremely useful for undistorting the video. You can receive camera calibration parameters using the get_calibration method.</p> <pre><code>await device.get_calibration()\n</code></pre> <p>Returns a <code>pupil_labs.neon_recording.calib.Calibration</code> object.</p> <pre><code>Calibration(\n    version = np.uint8(1)\n    serial = '841684'\n    scene_camera_matrix = array([[896.23068471,   0.        , 790.8950718 ],\n                [  0.        , 895.99428647, 593.30938736],\n                [  0.        ,   0.        ,   1.        ]])\n    scene_distortion_coefficients = array([-0.13185823,  0.11141446, -0.00072215, -0.00019211, -0.00102044,\n                0.17091784,  0.05497444,  0.02371847])\n    scene_extrinsics_affine_matrix = array([[1., 0., 0., 0.],\n                [0., 1., 0., 0.],\n                [0., 0., 1., 0.],\n                [0., 0., 0., 1.]])\n    right_camera_matrix = array([[140.03968681,   0.        ,  99.07925009],\n                [  0.        , 140.16685902,  96.21073359],\n                [  0.        ,   0.        ,   1.        ]])\n    right_distortion_coefficients = array([ 4.88968666e-02, -1.28678179e-01, -2.42854366e-04,  6.16360859e-04,\n                -6.13765032e-01, -4.34790467e-02,  3.41057533e-02, -6.83627299e-01])\n    right_extrinsics_affine_matrix = array([[-0.8363896 ,  0.14588414,  0.52836567, 16.93598175],\n                [ 0.05819079,  0.98211712, -0.17905241, 19.64488983],\n                [-0.54503787, -0.11901156, -0.82992166, -7.03995514],\n                [ 0.        ,  0.        ,  0.        ,  1.        ]])\n    left_camera_matrix = array([[139.60850687,   0.        ,  93.21881139],\n                [  0.        , 139.73659663,  95.43463863],\n                [  0.        ,   0.        ,   1.        ]])\n    left_distortion_coefficients = array([ 4.95496340e-02, -1.27421933e-01,  6.92379886e-04,  4.98479011e-04,\n                -6.26153622e-01, -4.43117940e-02,  3.31060602e-02, -6.91888536e-01])\n    left_extrinsics_affine_matrix = array([[ -0.83850485,  -0.13447338,  -0.52804023, -17.65301514],\n                [ -0.05493483,   0.98499447,  -0.16360955,  19.88935852],\n                [  0.54211783,  -0.10817961,  -0.83330995,  -7.48944855],\n                [  0.        ,   0.        ,   0.        ,   1.        ]])\n    crc = np.uint32(734156985)\n)\n</code></pre> <p>Wanna know how to undistort the video?\"</p> <p>Get a look at our tutorial.</p>"},{"location":"methods/async/remote-control/","title":"Remote Control","text":"<p>With the device connected, you can remotely control your device, similarly to the Monitor App and start, stop, save and annotate recordings.</p>"},{"location":"methods/async/remote-control/#start-a-recording","title":"Start a Recording","text":"<p>Use <code>device.recording_start</code> to start a recording on the device and return the recording ID.</p> start_stop_recordings.py<pre><code>    async with Device.from_discovered_device(dev_info) as device:\n        # get update when recording is fully started\n        notifier = StatusUpdateNotifier(device, callbacks=[on_status_update])\n        await notifier.receive_updates_start()\n        recording_id = await device.recording_start()\n        print(f\"Initiated recording with id {recording_id}\")\n        await asyncio.sleep(5)\n        print(\"Stopping recording\")\n        await device.recording_stop_and_save()\n        # await control.recording_cancel()  # uncomment to cancel recording\n        await asyncio.sleep(2)  # wait for confirmation via auto-update\n        await notifier.receive_updates_stop()\n</code></pre> <pre><code>Initiated recording with id 54e7d0bb-5b4c-40e6-baed-f2e11eb4f53b\n</code></pre> <p>With a recording ongoing, you can:</p>"},{"location":"methods/async/remote-control/#send-event","title":"Send event","text":"<p>Save events using the <code>device.send_event</code> method. By default, the Neon device receiving the event will assign a timestamp to it, using the time of arrival. Optionally, you can set a custom nanosecond timestamp for your event instead.</p> Timestamped on Arrival (Host/Companion Device)With Explicit Timestamp <p>send_event.py<pre><code>await device.send_event(\"test event\")\n</code></pre> <pre><code>Event(name=test event recording_id=None timestamp_unix_ns=1744271292116000000 datetime=2025-04-10 09:48:12.116000)\n</code></pre></p> <p>send_event.py<pre><code> await device.send_event(\n    \"test event\", event_timestamp_unix_ns=time.time_ns()\n)\n</code></pre> <pre><code>Event(name=test event recording_id=None timestamp_unix_ns=1744271291692745000 datetime=2025-04-10 09:48:11.692745)\n</code></pre></p> <p>Events will only be saved if the recording is running. If you send an event while there is no recording, it will be discarded.</p>"},{"location":"methods/async/remote-control/#check-for-errors","title":"Check for Errors","text":"<p>Neon +2.9.0 +1.5.0</p> <p>Errors can happen, but now you can also monitor them remotely, by looking for stream_errors on the <code>component</code> while updating the status, you can get notified of WatchDog errors during your recording.</p> start_stop_recordings.py<pre><code>def on_status_update(component):\n    if isinstance(component, Recording):\n        if component.action == \"ERROR\":\n            print(f\"Error : {component.message}\")\n\n    elif isinstance(component, Sensor) and component.stream_error:\n        print(f\"Stream error in sensor {component.sensor}\")\n\n            ...\n\n        # get update when recording is fully started\n        notifier = StatusUpdateNotifier(device, callbacks=[on_status_update])\n</code></pre> <pre><code>Error: Recording Watchdog failure\n</code></pre>"},{"location":"methods/async/remote-control/#stop-save-a-recording","title":"Stop &amp; Save a Recording","text":"<p>Likewise you can stop and save a recording using <code>device.recording_stop_and_save</code>. Note that if you have a mandatory question that is not filled, the recording will not be saved until that question is answered.</p> start_stop_recordings.py<pre><code>await device.recording_stop_and_save()\n</code></pre> <pre><code>Recording stopped and saved\n</code></pre>"},{"location":"methods/async/remote-control/#cancel-a-recording","title":"Cancel a Recording","text":"<p>You can also cancel(<code>device.recording_cancel</code>) a recording if you don't want to save it. This will delete the recording and all its data.</p> start_stop_recordings.py<pre><code>await device.recording_cancel()\n</code></pre>"},{"location":"methods/async/remote-control/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here start_stop_recordings.py<pre><code>import asyncio\n\nfrom pupil_labs.realtime_api import Device, Network, StatusUpdateNotifier\nfrom pupil_labs.realtime_api.models import Recording, Sensor\n\n\ndef on_status_update(component):\n    if isinstance(component, Recording):\n        if component.action == \"ERROR\":\n            print(f\"Error : {component.message}\")\n\n    elif isinstance(component, Sensor) and component.stream_error:\n        print(f\"Stream error in sensor {component.sensor}\")\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        # get update when recording is fully started\n        notifier = StatusUpdateNotifier(device, callbacks=[on_status_update])\n        await notifier.receive_updates_start()\n        recording_id = await device.recording_start()\n        print(f\"Initiated recording with id {recording_id}\")\n        await asyncio.sleep(5)\n        print(\"Stopping recording\")\n        await device.recording_stop_and_save()\n        # await control.recording_cancel()  # uncomment to cancel recording\n        await asyncio.sleep(2)  # wait for confirmation via auto-update\n        await notifier.receive_updates_stop()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> send_event.py<pre><code>import asyncio\nimport time\n\nfrom pupil_labs.realtime_api import Device, Network\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        # send event without timestamp\n        print(await device.send_event(\"test event\"))\n\n        # send event with current timestamp\n        print(\n            await device.send_event(\n                \"test event\", event_timestamp_unix_ns=time.time_ns()\n            )\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"methods/async/templates/","title":"Templates","text":"<p>Neon +1.3.0 +2.8.25</p> <p>You can access the response data entered into the template questionnaire on the phone and also set those responses remotely. If the template is properly configured, this allows you to also define the recording name.</p>"},{"location":"methods/async/templates/#get-template-definition","title":"Get Template Definition","text":"<p>Using the <code>device.get_template</code> method, you can receive the definition of the template containing all questions and sections.</p> <pre><code>        template = await device.get_template()\n</code></pre> Template"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template","title":"Template","text":"<p>Template Class for data collection.</p> <p>Methods:</p> <ul> <li> <code>convert_from_api_to_simple_format</code>             \u2013              <p>Convert data from API format to simple format.</p> </li> <li> <code>convert_from_simple_to_api_format</code>             \u2013              <p>Convert data from simple format to API format.</p> </li> <li> <code>get_question_by_id</code>             \u2013              <p>Get a template item by ID.</p> </li> <li> <code>validate_answers</code>             \u2013              <p>Validate answers for this Template.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>archived_at</code>               (<code>datetime | None</code>)           \u2013            <p>Archival timestamp (if archived).</p> </li> <li> <code>created_at</code>               (<code>datetime</code>)           \u2013            <p>Creation timestamp.</p> </li> <li> <code>description</code>               (<code>str | None</code>)           \u2013            <p>Template description.</p> </li> <li> <code>id</code>               (<code>UUID</code>)           \u2013            <p>Unique identifier.</p> </li> <li> <code>is_default_template</code>               (<code>bool</code>)           \u2013            <p>Whether this is the default template for the Workspace</p> </li> <li> <code>items</code>               (<code>list[TemplateItem]</code>)           \u2013            <p>List of template items.</p> </li> <li> <code>label_ids</code>               (<code>list[UUID]</code>)           \u2013            <p>Associated label IDs.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Template name.</p> </li> <li> <code>published_at</code>               (<code>datetime | None</code>)           \u2013            <p>Publication timestamp.</p> </li> <li> <code>recording_ids</code>               (<code>list[UUID] | None</code>)           \u2013            <p>Associated recording IDs.</p> </li> <li> <code>recording_name_format</code>               (<code>list[str]</code>)           \u2013            <p>Format for recording name.</p> </li> <li> <code>updated_at</code>               (<code>datetime</code>)           \u2013            <p>Last update timestamp.</p> </li> </ul>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.archived_at","title":"archived_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>archived_at: datetime | None = None\n</code></pre> <p>Archival timestamp (if archived).</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre> <p>Creation timestamp.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Template description.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: UUID\n</code></pre> <p>Unique identifier.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.is_default_template","title":"is_default_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_default_template: bool = True\n</code></pre> <p>Whether this is the default template for the Workspace</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.items","title":"items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>items: list[TemplateItem] = field(default_factory=list)\n</code></pre> <p>List of template items.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.label_ids","title":"label_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_ids: list[UUID] = field(default_factory=list, metadata={'readonly': True})\n</code></pre> <p>Associated label IDs.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Template name.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.published_at","title":"published_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>published_at: datetime | None = None\n</code></pre> <p>Publication timestamp.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.recording_ids","title":"recording_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>recording_ids: list[UUID] | None = None\n</code></pre> <p>Associated recording IDs.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.recording_name_format","title":"recording_name_format  <code>instance-attribute</code>","text":"<pre><code>recording_name_format: list[str]\n</code></pre> <p>Format for recording name.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: datetime\n</code></pre> <p>Last update timestamp.</p>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.convert_from_api_to_simple_format","title":"convert_from_api_to_simple_format","text":"<pre><code>convert_from_api_to_simple_format(data: dict[str, list[str]]) -&gt; dict[str, Any]\n</code></pre> <p>Convert data from API format to simple format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Data in API format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, Any]</code> )          \u2013            <p>Data in simple format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_api_to_simple_format(\n    self, data: dict[str, list[str]]\n) -&gt; dict[str, Any]:\n    \"\"\"Convert data from API format to simple format.\n\n    Args:\n        data: Data in API format.\n\n    Returns:\n        dict: Data in simple format.\n\n    \"\"\"\n    simple_format = {}\n    for question_id, value in data.items():\n        question = self.get_question_by_id(question_id)\n        if question is None:\n            logger.warning(\n                f\"Skipping unknown question ID '{question_id}' during API to \"\n                f\"simple conversion.\"\n            )\n            continue\n        processed_value: Any\n        if question.widget_type in {\"CHECKBOX_LIST\", \"RADIO_LIST\"}:\n            if question.choices is None:\n                logger.warning(\n                    f\"Question {question_id} (type {question.widget_type}) \"\n                    f\"has no choices defined.\"\n                )\n                processed_value = []\n            elif value == [\"\"] and \"\" not in question.choices:\n                processed_value = []\n        else:\n            if not value:\n                value = [\"\"]\n\n            value_str = value[0]\n            if question.input_type != \"any\":\n                processed_value = (\n                    None if value_str == \"\" else question._value_type(value_str)\n                )\n            else:\n                processed_value = value\n\n        simple_format[question_id] = processed_value\n    return simple_format\n</code></pre>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.convert_from_simple_to_api_format","title":"convert_from_simple_to_api_format","text":"<pre><code>convert_from_simple_to_api_format(data: dict[str, Any]) -&gt; dict[str, list[Any]]\n</code></pre> <p>Convert data from simple format to API format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Data in simple format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, list[Any]]</code> )          \u2013            <p>Data in API format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_simple_to_api_format(\n    self, data: dict[str, Any]\n) -&gt; dict[str, list[Any]]:\n    \"\"\"Convert data from simple format to API format.\n\n    Args:\n        data: Data in simple format.\n\n    Returns:\n        dict: Data in API format.\n\n    \"\"\"\n    api_format = {}\n    for question_id, value in data.items():\n        if value is None:\n            value = \"\"\n        if not isinstance(value, list):\n            value = [value]\n\n        api_format[question_id] = value\n    return api_format\n</code></pre>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.get_question_by_id","title":"get_question_by_id","text":"<pre><code>get_question_by_id(question_id: str | UUID) -&gt; TemplateItem | None\n</code></pre> <p>Get a template item by ID.</p> <p>Parameters:</p> <ul> <li> <code>question_id</code>               (<code>str | UUID</code>)           \u2013            <p>ID of the template item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TemplateItem | None</code>           \u2013            <p>TemplateItem | None: The template item, or None if not found.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def get_question_by_id(self, question_id: str | UUID) -&gt; TemplateItem | None:\n    \"\"\"Get a template item by ID.\n\n    Args:\n        question_id: ID of the template item.\n\n    Returns:\n        TemplateItem | None: The template item, or None if not found.\n\n    \"\"\"\n    for item in self.items:\n        if str(item.id) == str(question_id):\n            return item\n    return None\n</code></pre>"},{"location":"methods/async/templates/#pupil_labs.realtime_api.models.Template.validate_answers","title":"validate_answers","text":"<pre><code>validate_answers(answers: dict[str, list[str]], template_format: TemplateDataFormat, raise_exception: bool = True) -&gt; list[ErrorDetails]\n</code></pre> <p>Validate answers for this Template.</p> <p>Parameters:</p> <ul> <li> <code>answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Answers to validate.</p> </li> <li> <code>raise_exception</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to raise an exception on validation failure.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>)           \u2013            <p>Format of the answers (\"simple\" or \"api\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[ErrorDetails]</code> )          \u2013            <p>List of validation errors, or empty list if validation succeeded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidTemplateAnswersError</code>             \u2013            <p>If validation fails and raise_exception is</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def validate_answers(\n    self,\n    answers: dict[str, list[str]],\n    template_format: TemplateDataFormat,\n    raise_exception: bool = True,\n) -&gt; list[ErrorDetails]:\n    \"\"\"Validate answers for this Template.\n\n    Args:\n        answers: Answers to validate.\n        raise_exception: Whether to raise an exception on validation failure.\n        template_format: Format of the answers (\"simple\" or \"api\").\n\n    Returns:\n        list: List of validation errors, or empty list if validation succeeded.\n\n    Raises:\n        InvalidTemplateAnswersError: If validation fails and raise_exception is\n        True.\n\n    \"\"\"\n    AnswerModel = self._create_answer_model(template_format=template_format)\n    errors = []\n    try:\n        AnswerModel(**answers)\n    except ValidationError as e:\n        errors = e.errors()\n\n    for error in errors:\n        question_id = error[\"loc\"][0]\n        question = self.get_question_by_id(str(question_id))\n        if question:\n            error[\"question\"] = asdict(question)  # type: ignore[typeddict-unknown-key]\n\n    if errors and raise_exception:\n        raise InvalidTemplateAnswersError(self, answers, errors)\n    return errors\n</code></pre>"},{"location":"methods/async/templates/#get-template-data","title":"Get Template Data","text":"<p>Using the <code>device.get_template_data</code> method, you can receive the responses currently saved in the template.</p> <pre><code>        data = await device.get_template_data(template_format=\"simple\")\n</code></pre>"},{"location":"methods/async/templates/#set-template-data","title":"Set Template Data","text":"<p>Using the <code>device.post_template_data</code> method, you can set the template responses remotely.</p> <pre><code>            await device.post_template_data(questionnaire)\n</code></pre>"},{"location":"methods/async/templates/#get-template-questions-validate-them","title":"Get Template Questions &amp; Validate them","text":"<p>You can also retrieve individual questions by their ID using the <code>template.get_question_by_id</code> method and check the validity of a response using the <code>template.validate_answers</code> method.</p>"},{"location":"methods/async/templates/#see-it-in-action","title":"See it in action","text":"Check the whole example code here templates.py<pre><code>import asyncio\nimport os\n\nimport beaupy\n\nfrom pupil_labs.realtime_api import Device, Network\nfrom pupil_labs.realtime_api.models import InvalidTemplateAnswersError, TemplateItem\n\nLINE = \"\\u2500\" * os.get_terminal_size().columns\nRED = \"\\033[31m\"\nRESET = \"\\033[0m\"\n\n\ndef prompt_checkbox_answer(item: TemplateItem, current_value):\n    ticked = []\n    for i, choice in enumerate(item.choices):\n        current_value: list\n        if choice in (current_value or []):\n            current_value.remove(choice)\n            ticked.append(i)\n    choices = beaupy.select_multiple(\n        item.choices,\n        ticked_indices=ticked,\n    )\n    return choices\n\n\ndef prompt_radio_answer(item: TemplateItem, current_value):\n    cursor_index = 0\n    if current_value and current_value in item.choices:\n        cursor_index = item.choices.index(current_value[0])\n\n    choice = beaupy.select(item.choices, cursor_index=cursor_index)\n    template_input = []\n    if choice is not None:\n        template_input = [choice]\n    return template_input\n\n\ndef prompt_string_answer(item: TemplateItem, current_value):\n    placeholder = item.help_text if item.help_text and item.help_text != [\"\"] else None\n    current_value = (\n        placeholder if not current_value or current_value == [\"\"] else current_value\n    )\n    return beaupy.prompt(\n        f\"Enter value for '{item.title}': \",\n        initial_value=\"\" if current_value is None else str(current_value),\n    )\n\n\nasync def main():  # noqa: C901\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        # Fetch current template definition\n        template = await device.get_template()\n        # Fetch data filled on the template\n        data = await device.get_template_data(template_format=\"simple\")\n\n        print(f\"[{template.name}] Data pre-filled:\")\n        print(LINE)\n        print(\"\\n\".join(f\"{k}\\t{v}\" for k, v in data.items()))\n\n        # Filling a template\n        questionnaire = {}\n        if template:\n            try:\n                for item in template.items:\n                    if item.widget_type in (\"SECTION_HEADER\", \"PAGE_BREAK\"):\n                        continue\n                    print(LINE)\n                    print(\n                        f\"{'* ' if item.required else ''}\"\n                        f\"ID: {item.id} - Title: {item.title} \"\n                        f\"- Input Type: {item.input_type}\"\n                    )\n                    current_value = data.get(str(item.id))\n                    while True:\n                        question = template.get_question_by_id(item.id)\n                        if item.widget_type == \"CHECKBOX_LIST\":\n                            template_input = prompt_checkbox_answer(item, current_value)\n                        elif item.widget_type == \"RADIO_LIST\":\n                            template_input = prompt_radio_answer(item, current_value)\n                        else:\n                            template_input = prompt_string_answer(item, current_value)\n\n                        try:\n                            print(template_input)\n                            errors = question.validate_answer(template_input)\n                            if not errors:\n                                questionnaire[str(item.id)] = template_input\n                                break\n                            else:\n                                print(f\"Errors: {errors}\")\n                        except InvalidTemplateAnswersError as e:\n                            print(f\"{RED}Validation failed for: {template_input}\")\n                            for error in e.errors:\n                                print(f\"    {error['msg']}\")\n                            print(LINE + RESET)\n            except KeyboardInterrupt:\n                print(\"\\nKeyboardInterrupt detected. Skipping line.\")\n\n        print(LINE)\n\n        # Sending the template\n        if questionnaire:\n            await device.post_template_data(questionnaire)\n\n        # Fetch new data filled on the template\n        data = await device.get_template_data(template_format=\"api\")\n\n        # Iterate to check filled data\n        print(f\"[{template.name}] Data post:\")\n        print(LINE)\n        print(\"\\n\".join(f\"{k}\\t{v}\" for k, v in data.items()))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/eye-cameras/","title":"Stream Eye Cameras","text":"<p>Neon +1.1.2</p> <p>Neon allows you to receive the eye cameras video stream with timestamps. Using the same <code>receive_video_frames</code> method used for the scene camera, but using the <code>sensor.eyes</code> that you can withdraw from direct_eyes_sensor.</p> <pre><code>status = await device.get_status()\nsensor_eyes = status.direct_eyes_sensor()\nasync for frame in receive_video_frames(\n    sensor_eyes.url, run_loop=restart_on_disconnect\n    ):\n    bgr_buffer = frame.bgr_buffer()\n</code></pre> VideoFrame Check the whole example code here stream_eyes_camera_video.py<pre><code>import asyncio\nimport contextlib\nimport time\n\nimport cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api import Device, Network, receive_video_frames  # noqa: E402\n\n\nasync def main(preview_frame_rate=30):\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n        sensor_eyes = status.direct_eyes_sensor()\n        if not sensor_eyes.connected:\n            print(f\"Eyes camera is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n        last_update = time.perf_counter()\n        async for frame in receive_video_frames(\n            sensor_eyes.url, run_loop=restart_on_disconnect\n        ):\n            bgr_buffer = frame.bgr_buffer()\n            draw_time(bgr_buffer, frame.datetime)\n            cv2.imshow(\"Eye Cameras - Press ESC to quit\", bgr_buffer)\n\n            time_since_last_update = time.perf_counter() - last_update\n            if time_since_last_update &gt; 1 / preview_frame_rate:\n                if cv2.waitKey(1) &amp; 0xFF == 27:\n                    return\n                last_update = time.perf_counter()\n\n\ndef draw_time(frame, time):\n    frame_txt_font_name = cv2.FONT_HERSHEY_SIMPLEX\n    frame_txt_font_scale = 0.5\n    frame_txt_thickness = 1\n\n    # first line: frame index\n    frame_txt = str(time)\n\n    cv2.putText(\n        frame,\n        frame_txt,\n        (20, 50),\n        frame_txt_font_name,\n        frame_txt_font_scale,\n        (255, 255, 255),\n        thickness=frame_txt_thickness,\n        lineType=cv2.LINE_8,\n    )\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame","title":"VideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A video frame with timestamp information.</p> <p>This class represents a video frame from the scene camera with associated timestamp information. The Class inherits VideoFrame from py.av library.</p> <p>Methods:</p> <ul> <li> <code>bgr_buffer</code>             \u2013              <p>Convert the video frame to a BGR buffer.</p> </li> <li> <code>to_ndarray</code>             \u2013              <p>Convert the video frame to a NumPy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>VideoFrame</code>)           \u2013            <p>The video frame.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: VideoFrame\n</code></pre> <p>The video frame.</p>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.bgr_buffer","title":"bgr_buffer","text":"<pre><code>bgr_buffer() -&gt; BGRBuffer\n</code></pre> <p>Convert the video frame to a BGR buffer.</p> <p>This method converts the video frame to a BGR buffer, which is a NumPy array with the shape (height, width, 3) and dtype uint8. The BGR format is commonly used in computer vision applications.</p> <p>Returns:</p> <ul> <li> <code>BGRBuffer</code> (              <code>BGRBuffer</code> )          \u2013            <p>The BGR buffer as a NumPy array.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def bgr_buffer(self) -&gt; BGRBuffer:\n    \"\"\"Convert the video frame to a BGR buffer.\n\n    This method converts the video frame to a BGR buffer, which is a\n    NumPy array with the shape (height, width, 3) and dtype uint8.\n    The BGR format is commonly used in computer vision applications.\n\n    Returns:\n        BGRBuffer: The BGR buffer as a NumPy array.\n\n    \"\"\"\n    return self.to_ndarray(format=\"bgr24\")\n</code></pre>"},{"location":"methods/async/streaming/eye-cameras/#pupil_labs.realtime_api.streaming.video.VideoFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray(*args: Any, **kwargs: Any) -&gt; NDArray\n</code></pre> <p>Convert the video frame to a NumPy array.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def to_ndarray(self, *args: Any, **kwargs: Any) -&gt; npt.NDArray:\n    \"\"\"Convert the video frame to a NumPy array.\"\"\"\n    return self.av_frame.to_ndarray(*args, **kwargs)\n</code></pre>"},{"location":"methods/async/streaming/eye-events/","title":"Blinks, fixations &amp; saccades","text":"<p>Neon +2.9.0 +1.5.0</p> <p>Using the <code>receive_eye_events_data</code> method, you can receive eye events such as blinks, saccades or fixations. The data returned is either an instance of:</p> <p>Warnign</p> <p>Requires the \"Compute fixations\" setting to be enabled in the Companion Device.</p>"},{"location":"methods/async/streaming/eye-events/#fixationeventdata","title":"<code>FixationEventData</code>","text":"<p>Defines a full fixation or saccade event. They would look as follow, with event_type being either <code>0</code> for saccades or <code>1</code> for fixations.</p> SaccadeFixation <pre><code>FixationEventData(\n    event_type=0,\n    start_time_ns=1744625900502677306,\n    end_time_ns=1744625900562676306,\n    start_gaze_x=768.2272338867188,\n    start_gaze_y=685.6964721679688,\n    end_gaze_x=716.1095581054688,\n    end_gaze_y=493.5322570800781,\n    mean_gaze_x=747.7811279296875,\n    mean_gaze_y=597.7672119140625,\n    amplitude_pixels=199.10633850097656,\n    amplitude_angle_deg=12.716423988342285,\n    mean_velocity=3318.313232421875,\n    max_velocity=7444.6396484375,\n    rtp_ts_unix_seconds=1744626471.955861\n)\n</code></pre> <pre><code>FixationEventData(\n    event_type=1,\n    start_time_ns=1744625967695094306,\n    end_time_ns=1744625968135465306,\n    start_gaze_x=870.0199584960938,\n    start_gaze_y=311.0625,\n    end_gaze_x=730.7664794921875,\n    end_gaze_y=264.4870300292969,\n    mean_gaze_x=839.43115234375,\n    mean_gaze_y=280.5098876953125,\n    amplitude_pixels=146.83596801757812,\n    amplitude_angle_deg=9.18490982055664,\n    mean_velocity=272.82110595703125,\n    max_velocity=1415.25048828125,\n    rtp_ts_unix_seconds=1744626539.528702\n)\n</code></pre> FixationEventData"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData","title":"FixationEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade event.</p> <p>Represents a completed fixation or saccade event with detailed information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a FixationEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>amplitude_angle_deg</code>               (<code>float</code>)           \u2013            <p>Amplitude in degrees.</p> </li> <li> <code>amplitude_pixels</code>               (<code>float</code>)           \u2013            <p>Amplitude in pixels.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_gaze_x</code>               (<code>float</code>)           \u2013            <p>End gaze x-coordinate in pixels.</p> </li> <li> <code>end_gaze_y</code>               (<code>float</code>)           \u2013            <p>End gaze y-coordinate in pixels.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the event in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (0 for saccade, 1 for fixation).</p> </li> <li> <code>max_velocity</code>               (<code>float</code>)           \u2013            <p>Maximum velocity in pixels per degree.</p> </li> <li> <code>mean_gaze_x</code>               (<code>float</code>)           \u2013            <p>Mean gaze x-coordinate in pixels.</p> </li> <li> <code>mean_gaze_y</code>               (<code>float</code>)           \u2013            <p>Mean gaze y-coordinate in pixels.</p> </li> <li> <code>mean_velocity</code>               (<code>float</code>)           \u2013            <p>Mean velocity in pixels per degree.</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_gaze_x</code>               (<code>float</code>)           \u2013            <p>Start gaze x-coordinate in pixels.</p> </li> <li> <code>start_gaze_y</code>               (<code>float</code>)           \u2013            <p>Start gaze y-coordinate in pixels.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_angle_deg","title":"amplitude_angle_deg  <code>instance-attribute</code>","text":"<pre><code>amplitude_angle_deg: float\n</code></pre> <p>Amplitude in degrees.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_pixels","title":"amplitude_pixels  <code>instance-attribute</code>","text":"<pre><code>amplitude_pixels: float\n</code></pre> <p>Amplitude in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_x","title":"end_gaze_x  <code>instance-attribute</code>","text":"<pre><code>end_gaze_x: float\n</code></pre> <p>End gaze x-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_y","title":"end_gaze_y  <code>instance-attribute</code>","text":"<pre><code>end_gaze_y: float\n</code></pre> <p>End gaze y-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the event in nanoseconds.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (0 for saccade, 1 for fixation).</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.max_velocity","title":"max_velocity  <code>instance-attribute</code>","text":"<pre><code>max_velocity: float\n</code></pre> <p>Maximum velocity in pixels per degree.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_x","title":"mean_gaze_x  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_x: float\n</code></pre> <p>Mean gaze x-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_y","title":"mean_gaze_y  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_y: float\n</code></pre> <p>Mean gaze y-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_velocity","title":"mean_velocity  <code>instance-attribute</code>","text":"<pre><code>mean_velocity: float\n</code></pre> <p>Mean velocity in pixels per degree.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_x","title":"start_gaze_x  <code>instance-attribute</code>","text":"<pre><code>start_gaze_x: float\n</code></pre> <p>Start gaze x-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_y","title":"start_gaze_y  <code>instance-attribute</code>","text":"<pre><code>start_gaze_y: float\n</code></pre> <p>Start gaze y-coordinate in pixels.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; FixationEventData\n</code></pre> <p>Create a FixationEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"FixationEventData\":\n    \"\"\"Create a FixationEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n    ) = struct.unpack(\"!iqqffffffffff\", data.raw)\n    return cls(\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"methods/async/streaming/eye-events/#fixationonseteventdata","title":"<code>FixationOnsetEventData</code>","text":"<p>This defines a fixation or saccade onset event, and they would look as follows:</p> Saccade OnsetFixation Onset <pre><code>FixationOnsetEventData(event_type=2, start_time_ns=1744626187872119306, rtp_ts_unix_seconds=1744626759.2655792)\n</code></pre> <pre><code>FixationOnsetEventData(event_type=3, start_time_ns=1744626187872119306, rtp_ts_unix_seconds=1744626759.2655792)\n</code></pre> FixationOnsetEventData"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData","title":"FixationOnsetEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade onset event.</p> <p>Represents the beginning of a fixation or saccade event.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (2 for saccade onset, 3 for fixation onset).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (2 for saccade onset, 3 for fixation onset).</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#blinkeventdata","title":"<code>BlinkEventData</code>","text":"<p>Finally, BlinkEventData determines a blink event.</p> <pre><code>BlinkEventData(\n    event_type=4,\n    start_time_ns=1744626029708811306,\n    end_time_ns=1744626029919061306,\n    rtp_ts_unix_seconds=1744626601.1020627\n)\n</code></pre> BlinkEventData"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData","title":"BlinkEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a blink event.</p> <p>Represents a detected blink event with timing information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a BlinkEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the blink in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (4 -&gt; blink events).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the blink in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the blink in nanoseconds.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (4 -&gt; blink events).</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the blink in nanoseconds.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/async/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; BlinkEventData\n</code></pre> <p>Create a BlinkEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"BlinkEventData\":\n    \"\"\"Create a BlinkEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n    ) = struct.unpack(\"!iqq\", data.raw)\n    return cls(event_type, start_time_ns, end_time_ns, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"methods/async/streaming/eye-events/#example","title":"Example","text":"<p>If you ran the example you will get an output simmilar to this:</p> <pre><code>[FIXATION] event with duration of 3.93 seconds.\n[SACCADE] event with 43\u00b0 amplitude.\n[BLINK] blinked at 10:35:07 UTC\n</code></pre> Check the whole example code here stream_eye_events<pre><code>import asyncio\nimport contextlib\nfrom datetime import datetime, timezone\n\nfrom pupil_labs.realtime_api import Device, Network, receive_eye_events_data\nfrom pupil_labs.realtime_api.streaming.eye_events import (\n    BlinkEventData,\n    FixationEventData,\n)\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n        sensor_eye_events = status.direct_eye_events_sensor()\n        if not sensor_eye_events.connected:\n            print(f\"Eye events sensor is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n        async for eye_event in receive_eye_events_data(\n            sensor_eye_events.url, run_loop=restart_on_disconnect\n        ):\n            if isinstance(eye_event, BlinkEventData):\n                time_sec = eye_event.start_time_ns // 1e9\n                blink_time = datetime.fromtimestamp(time_sec, timezone.utc)\n                print(f\"[BLINK] blinked at {blink_time.strftime('%H:%M:%S')} UTC\")\n\n            elif isinstance(eye_event, FixationEventData) and eye_event.event_type == 0:\n                angle = eye_event.amplitude_angle_deg\n                print(f\"[SACCADE] event with {angle:.0f}\u00b0 amplitude.\")\n\n            elif isinstance(eye_event, FixationEventData) and eye_event.event_type == 1:\n                duration = (eye_event.end_time_ns - eye_event.start_time_ns) / 1e9\n                print(f\"[FIXATION] event with duration of {duration:.2f} seconds.\")\n\n            # print(eye_event) # This will print all the fields of the eye event\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/gaze/","title":"Streaming Gaze Data","text":"<p>Use <code>receive_gaze_data</code> to subscribe to gaze data.</p> <p>This function can return different types of gaze data (GazeDataType) depending on the device and its configuration:</p> <ul> <li>GazeData object for Pupil Invisible or Neon without \"Compute eye state\" enabled.</li> <li>EyestateGazeData or EyestateEyelidGazeData for Neon with \"Compute eye state\" enabled, depending on the version of the Neon Companion app.</li> </ul> <p>See below samples for each type of gaze data.</p> Gaze dataWith Eye StateWith Eye Lid data <pre><code>GazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre> <p>Neon +2.8.8 +1.2.0</p> <p><pre><code>EyestateGazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    pupil_diameter_left=4.306737899780273,\n    eyeball_center_left_x=-29.3125,\n    eyeball_center_left_y=11.6875,\n    eyeball_center_left_z=-42.15625,\n    optical_axis_left_x=0.09871648252010345,\n    optical_axis_left_y=0.15512824058532715,\n    optical_axis_left_z=0.9829498529434204,\n    pupil_diameter_right=3.2171919345855713,\n    eyeball_center_right_x=33.21875,\n    eyeball_center_right_y=12.84375,\n    eyeball_center_right_z=-45.34375,\n    optical_axis_right_x=-0.20461124181747437,\n    optical_axis_right_y=0.1512681096792221,\n    optical_axis_right_z=0.9670844078063965,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre>  This method also provides pupil diameter and eye poses.</p> <p>Neon +2.9.0 +1.3.6</p> <p><pre><code>EyestateEyelidGazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    pupil_diameter_left=4.306737899780273,\n    eyeball_center_left_x=-29.3125,\n    eyeball_center_left_y=11.6875,\n    eyeball_center_left_z=-42.15625,\n    optical_axis_left_x=0.09871648252010345,\n    optical_axis_left_y=0.15512824058532715,\n    optical_axis_left_z=0.9829498529434204,\n    pupil_diameter_right=3.2171919345855713,\n    eyeball_center_right_x=33.21875,\n    eyeball_center_right_y=12.84375,\n    eyeball_center_right_z=-45.34375,\n    optical_axis_right_x=-0.20461124181747437,\n    optical_axis_right_y=0.1512681096792221,\n    optical_axis_right_z=0.9670844078063965,\n    eyelid_angle_top_left=-1.1484375,\n    eyelid_angle_bottom_left=-1.2763671875,\n    eyelid_aperture_left=1.6408717632293701,\n    eyelid_angle_top_right=-0.6259765625,\n    eyelid_angle_bottom_right=-1.2216796875,\n    eyelid_aperture_right=7.2039408683776855,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre> This method also provides eye openness data.</p> <p>You can learn more about the payload in the Under the Hood guide.</p>"},{"location":"methods/async/streaming/gaze/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here stream_gaze.py<pre><code>import asyncio\nimport contextlib\n\nfrom pupil_labs.realtime_api import Device, Network, receive_gaze_data\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n        sensor_gaze = status.direct_gaze_sensor()\n        if not sensor_gaze.connected:\n            print(f\"Gaze sensor is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n        async for gaze in receive_gaze_data(\n            sensor_gaze.url, run_loop=restart_on_disconnect\n        ):\n            print(gaze)\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/imu-data/","title":"Stream IMU Data","text":"<p>Neon +1.1.2</p> <p>Data generated by the IMU can be received using the <code>receive_imu_data</code> method. It returns a UTC timestamp in seconds, the head pose as a quaternion, gyro data, and accelerometer data as follows.</p> <pre><code>IMUData(\n    gyro_data=Data3D(x=-0.1659393310546875, y=-0.1964569091796875, z=0.1735687255859375),\n    accel_data=Data3D(x=-0.02880859375, y=-0.224609375, z=1.0068359375),\n    quaternion=Quaternion(x=-0.06114135682582855, y=-0.090116947889328, z=0.8700147271156311, w=0.48084819316864014),\n    timestamp_unix_seconds=1744297102.1941311\n)\n</code></pre> Check the whole example code here stream_imu.py<pre><code>import asyncio\nimport contextlib\n\nfrom pupil_labs.realtime_api import Device, Network, receive_imu_data\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n        sensor_imu = status.direct_imu_sensor()\n        if not sensor_imu.connected:\n            print(f\"Imu sensor is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n        async for imu_pack in receive_imu_data(\n            sensor_imu.url, run_loop=restart_on_disconnect\n        ):\n            print(imu_pack)\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/scene-camera/","title":"Scene Camera","text":""},{"location":"methods/async/streaming/scene-camera/#scene-camera-video","title":"Scene Camera Video","text":"<p>You can receive the scene camera video stream with timestamps, using the <code>receive_video_frames</code> method.</p> <pre><code>async for frame in receive_video_frames(\n    sensor_world.url, run_loop=restart_on_disconnect\n):\n    bgr_buffer = frame.bgr_buffer()\n</code></pre> VideoFrame Check the whole example code here stream_scene_camera_video.py<pre><code>import asyncio\nimport contextlib\n\nimport cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api import Device, Network, receive_video_frames  # noqa: E402\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        status = await device.get_status()\n        sensor_world = status.direct_world_sensor()\n        if not sensor_world.connected:\n            print(f\"Scene camera is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n        async for frame in receive_video_frames(\n            sensor_world.url, run_loop=restart_on_disconnect\n        ):\n            bgr_buffer = frame.bgr_buffer()\n            draw_time(bgr_buffer, frame.datetime)\n            cv2.imshow(\"Scene Camera - Press ESC to quit\", bgr_buffer)\n            if cv2.waitKey(1) &amp; 0xFF == 27:\n                return\n\n\ndef draw_time(frame, time):\n    frame_txt_font_name = cv2.FONT_HERSHEY_SIMPLEX\n    frame_txt_font_scale = 1.0\n    frame_txt_thickness = 1\n\n    # first line: frame index\n    frame_txt = str(time)\n\n    cv2.putText(\n        frame,\n        frame_txt,\n        (20, 50),\n        frame_txt_font_name,\n        frame_txt_font_scale,\n        (255, 255, 255),\n        thickness=frame_txt_thickness,\n        lineType=cv2.LINE_8,\n    )\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame","title":"VideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A video frame with timestamp information.</p> <p>This class represents a video frame from the scene camera with associated timestamp information. The Class inherits VideoFrame from py.av library.</p> <p>Methods:</p> <ul> <li> <code>bgr_buffer</code>             \u2013              <p>Convert the video frame to a BGR buffer.</p> </li> <li> <code>to_ndarray</code>             \u2013              <p>Convert the video frame to a NumPy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>VideoFrame</code>)           \u2013            <p>The video frame.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: VideoFrame\n</code></pre> <p>The video frame.</p>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.bgr_buffer","title":"bgr_buffer","text":"<pre><code>bgr_buffer() -&gt; BGRBuffer\n</code></pre> <p>Convert the video frame to a BGR buffer.</p> <p>This method converts the video frame to a BGR buffer, which is a NumPy array with the shape (height, width, 3) and dtype uint8. The BGR format is commonly used in computer vision applications.</p> <p>Returns:</p> <ul> <li> <code>BGRBuffer</code> (              <code>BGRBuffer</code> )          \u2013            <p>The BGR buffer as a NumPy array.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def bgr_buffer(self) -&gt; BGRBuffer:\n    \"\"\"Convert the video frame to a BGR buffer.\n\n    This method converts the video frame to a BGR buffer, which is a\n    NumPy array with the shape (height, width, 3) and dtype uint8.\n    The BGR format is commonly used in computer vision applications.\n\n    Returns:\n        BGRBuffer: The BGR buffer as a NumPy array.\n\n    \"\"\"\n    return self.to_ndarray(format=\"bgr24\")\n</code></pre>"},{"location":"methods/async/streaming/scene-camera/#pupil_labs.realtime_api.streaming.video.VideoFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray(*args: Any, **kwargs: Any) -&gt; NDArray\n</code></pre> <p>Convert the video frame to a NumPy array.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/video.py</code> <pre><code>def to_ndarray(self, *args: Any, **kwargs: Any) -&gt; npt.NDArray:\n    \"\"\"Convert the video frame to a NumPy array.\"\"\"\n    return self.av_frame.to_ndarray(*args, **kwargs)\n</code></pre>"},{"location":"methods/async/streaming/scene-camera/#scene-camera-video-with-overlayed-gaze","title":"Scene Camera Video with Overlayed Gaze","text":"<p>The following example shows how you can match multiple sensors streams by qeueing (<code>asyncio.Queue()</code>) and matching the data.</p> Check the whole example code here stream_video_with_overlayed_gaze.py<pre><code>import asyncio\nimport contextlib\nimport typing as T\n\nimport cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api import (  # noqa: E402\n    Device,\n    Network,\n    receive_gaze_data,\n    receive_video_frames,\n)\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        print(f\"Getting status information from {device}\")\n        status = await device.get_status()\n\n        sensor_gaze = status.direct_gaze_sensor()\n        if not sensor_gaze.connected:\n            print(f\"Gaze sensor is not connected to {device}\")\n            return\n\n        sensor_world = status.direct_world_sensor()\n        if not sensor_world.connected:\n            print(f\"Scene camera is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n\n        queue_video = asyncio.Queue()\n        queue_gaze = asyncio.Queue()\n\n        process_video = asyncio.create_task(\n            enqueue_sensor_data(\n                receive_video_frames(sensor_world.url, run_loop=restart_on_disconnect),\n                queue_video,\n            )\n        )\n        process_gaze = asyncio.create_task(\n            enqueue_sensor_data(\n                receive_gaze_data(sensor_gaze.url, run_loop=restart_on_disconnect),\n                queue_gaze,\n            )\n        )\n        try:\n            await match_and_draw(queue_video, queue_gaze)\n        finally:\n            process_video.cancel()\n            process_gaze.cancel()\n\n\nasync def enqueue_sensor_data(sensor: T.AsyncIterator, queue: asyncio.Queue) -&gt; None:\n    async for datum in sensor:\n        try:\n            queue.put_nowait((datum.datetime, datum))\n        except asyncio.QueueFull:\n            print(f\"Queue is full, dropping {datum}\")\n\n\nasync def match_and_draw(queue_video, queue_gaze):\n    while True:\n        video_datetime, video_frame = await get_most_recent_item(queue_video)\n        _, gaze_datum = await get_closest_item(queue_gaze, video_datetime)\n\n        bgr_buffer = video_frame.to_ndarray(format=\"bgr24\")\n\n        cv2.circle(\n            bgr_buffer,\n            (int(gaze_datum.x), int(gaze_datum.y)),\n            radius=80,\n            color=(0, 0, 255),\n            thickness=15,\n        )\n\n        cv2.imshow(\"Scene camera with gaze overlay\", bgr_buffer)\n        cv2.waitKey(1)\n\n\nasync def get_most_recent_item(queue):\n    item = await queue.get()\n    while True:\n        try:\n            next_item = queue.get_nowait()\n        except asyncio.QueueEmpty:\n            return item\n        else:\n            item = next_item\n\n\nasync def get_closest_item(queue, timestamp):\n    item_ts, item = await queue.get()\n    # assumes monotonically increasing timestamps\n    if item_ts &gt; timestamp:\n        return item_ts, item\n    while True:\n        try:\n            next_item_ts, next_item = queue.get_nowait()\n        except asyncio.QueueEmpty:\n            return item_ts, item\n        else:\n            if next_item_ts &gt; timestamp:\n                return next_item_ts, next_item\n            item_ts, item = next_item_ts, next_item\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/async/streaming/scene-camera/#scene-camera-video-with-overlayed-fixations-or-other-streams","title":"Scene Camera Video with Overlayed Fixations or Other Streams","text":"<p>Neon +2.9.0 +1.5.0</p> <p>You can do this with any streams, eye cameras or including eye events (blinks, or fixations data).</p> Check the whole example code here stream_video_with_overlayed_fixations.py<pre><code>import asyncio\nimport contextlib\nimport typing as T\nfrom collections import deque\n\nimport cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api import (  # noqa: E402\n    Device,\n    Network,\n    receive_eye_events_data,\n    receive_video_frames,\n)\nfrom pupil_labs.realtime_api.streaming import (  # noqa: E402\n    BlinkEventData,\n    FixationEventData,\n)\n\n\nasync def main():\n    async with Network() as network:\n        dev_info = await network.wait_for_new_device(timeout_seconds=5)\n    if dev_info is None:\n        print(\"No device could be found! Abort\")\n        return\n\n    async with Device.from_discovered_device(dev_info) as device:\n        print(f\"Getting status information from {device}\")\n        status = await device.get_status()\n\n        sensor_eye_events = status.direct_eye_events_sensor()\n        if not sensor_eye_events.connected:\n            print(f\"Eye events sensor is not connected to {device}\")\n            return\n\n        sensor_world = status.direct_world_sensor()\n        if not sensor_world.connected:\n            print(f\"Scene camera is not connected to {device}\")\n            return\n\n        restart_on_disconnect = True\n\n        queue_video = asyncio.Queue()\n        queue_eye_events = asyncio.Queue()\n\n        process_video = asyncio.create_task(\n            enqueue_sensor_data(\n                receive_video_frames(sensor_world.url, run_loop=restart_on_disconnect),\n                queue_video,\n            )\n        )\n        process_gaze = asyncio.create_task(\n            enqueue_sensor_data(\n                receive_eye_events_data(\n                    sensor_eye_events.url, run_loop=restart_on_disconnect\n                ),\n                queue_eye_events,\n            )\n        )\n        try:\n            await match_and_draw(queue_video, queue_eye_events)\n        finally:\n            process_video.cancel()\n            process_gaze.cancel()\n\n\nasync def enqueue_sensor_data(sensor: T.AsyncIterator, queue: asyncio.Queue) -&gt; None:\n    async for datum in sensor:\n        try:\n            queue.put_nowait((datum.datetime, datum))\n        except asyncio.QueueFull:\n            print(f\"Queue is full, dropping {datum}\")\n\n\nasync def match_and_draw(queue_video, queue_eye_events):\n    fixation_history = deque(maxlen=10)\n    fixation_counter = 0\n\n    blink = None\n    blink_counter = 0\n\n    while True:\n        _video_datetime, video_frame = await get_most_recent_item(queue_video)\n        bgr_buffer = video_frame.to_ndarray(format=\"bgr24\")\n\n        while not queue_eye_events.empty():\n            _, eye_event = await queue_eye_events.get()\n            if isinstance(eye_event, FixationEventData):\n                if eye_event.event_type == 0:\n                    continue\n\n                fixation_history.append({\n                    \"id\": fixation_counter,\n                    \"fixation\": eye_event,\n                })\n                fixation_counter += 1\n\n            elif isinstance(eye_event, BlinkEventData):\n                blink = eye_event\n                blink_counter += 1\n\n        for fixation_meta in fixation_history:\n            fixation_id = fixation_meta[\"id\"]\n            fixation = fixation_meta[\"fixation\"]\n\n            age = video_frame.timestamp_unix_seconds - fixation.end_time_ns * 1e-9\n            duration = (fixation.end_time_ns - fixation.start_time_ns) * 1e-9\n\n            overlay = bgr_buffer.copy()\n            cv2.circle(\n                overlay,\n                (int(fixation.mean_gaze_x), int(fixation.mean_gaze_y)),\n                radius=40 + int(duration * 10),\n                color=(255, 32, 32),\n                thickness=5,\n            )\n            cv2.putText(\n                overlay,\n                str(fixation_id),\n                (int(fixation.mean_gaze_x) - 10, int(fixation.mean_gaze_y) + 5),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                1,\n                (255, 255, 255),\n                2,\n                cv2.LINE_AA,\n            )\n            alpha = min(max(0, 1.0 - age / 5.0), 1.0)\n            cv2.addWeighted(overlay, alpha, bgr_buffer, 1 - alpha, 0, bgr_buffer)\n\n        if blink is not None:\n            overlay = bgr_buffer.copy()\n            cv2.putText(\n                overlay,\n                f\"Blink {blink_counter}\",\n                (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                1,\n                (0, 0, 255),\n                2,\n                cv2.LINE_AA,\n            )\n            age = video_frame.timestamp_unix_seconds - blink.end_time_ns * 1e-9\n            alpha = min(max(0, 1.0 - age / 5.0), 1.0)\n            cv2.addWeighted(overlay, alpha, bgr_buffer, 1 - alpha, 0, bgr_buffer)\n\n        cv2.imshow(\"Scene camera with eye events\", bgr_buffer)\n        cv2.waitKey(1)\n\n\nasync def get_most_recent_item(queue):\n    item = await queue.get()\n    while True:\n        try:\n            next_item = queue.get_nowait()\n        except asyncio.QueueEmpty:\n            return item\n        else:\n            item = next_item\n\n\nif __name__ == \"__main__\":\n    with contextlib.suppress(KeyboardInterrupt):\n        asyncio.run(main())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/","title":"Connect to a Device","text":"<p>The first step when using the Real-time API is to connect your client to a Companion device, enabling communication between them.</p> <p>The examples below demonstrate how to connect to one or more devices using our device discovery functions, as well as how to connect directly via a device\u2019s IP address.</p> <p>Your device and the computer running the client must be connected to the same network, and the Companion app must be running. If no device can be found, please refer to the troubleshooting section.</p> Discover One DeviceDiscover Multiple DevicesConnect Directly via IP Address <p>Using the <code>discover_one_device()</code>, you will get access to the first device it detects.</p> discover_devices.py<pre><code>from pupil_labs.realtime_api.simple import discover_devices, discover_one_device\n\nprint(\"Looking for the next best device...\\n\\t\", end=\"\")\nprint(discover_one_device(max_search_duration_seconds=10.0))\n</code></pre> <p>Using the <code>discover_devices()</code>, you can connect to more than one device.</p> discover_devices.py<pre><code>from pupil_labs.realtime_api.simple import discover_devices, discover_one_device\n\nprint(\"Starting 10 second search...\\n\\t\", end=\"\")\nprint(discover_devices(search_duration_seconds=10.0))\n</code></pre> <p>There might be instances where devices can't be found due to the network, but you can always specify the IP address and port to simplify the process.</p> <pre><code>from pupil_labs.realtime_api.simple import Device\n\n# This address is just an example. Find out the actual IP address of your device!\nip = \"192.168.1.169\"\ndevice = Device(address=ip, port=\"8080\")\n</code></pre> <p>Make sure the Companion App is running! If no device can be found, please have a look at the troubleshooting section.</p> <p>Below you can find a link to the full code example and the API referece for the returned Device object.</p>"},{"location":"methods/simple/connect-to-a-device/#device-information-automatic-status-updates","title":"Device Information &amp; Automatic Status Updates","text":"<p>Once connected, the <code>Device</code> object provides access to various properties of he Companion device and connected glasses, such as battery level, free storage, and serial numbers.</p> <p>This class automatically monitors the Companion device in the background and mirrors its state accordingly.</p> Get a snap of StatusContinuously monitor Status Updates <p>get_status.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# Device status is fetched on initialization and kept up-to-date in the background\n\nprint(f\"Phone IP address: {device.phone_ip}\")\nprint(f\"Phone name: {device.phone_name}\")\nprint(f\"Phone unique ID: {device.phone_id}\")\n\nprint(f\"Battery level: {device.battery_level_percent}%\")\nprint(f\"Battery state: {device.battery_state}\")\n\nprint(f\"Free storage: {device.memory_num_free_bytes / 1024**3}GB\")\nprint(f\"Storage level: {device.memory_state}\")\n\nprint(f\"Connected glasses: SN {device.serial_number_glasses}\")\nprint(f\"Connected scene camera: SN {device.serial_number_scene_cam}\")\n\ndevice.close()  # explicitly stop auto-update\n</code></pre> Output<pre><code>Phone IP address: 192.168.1.168\nPhone name: OnePlus8\nBattery level: 43%\nFree storage: 92.4 GB\nSerial number of connected glasses: h4gcf\n</code></pre></p> <pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\nscene_camera = device.world_sensor()\nconnected = False if scene_camera is None else scene_camera.connected\nprint(\"Scene camera connected:\", connected)\n\ninput(\"(Dis)connect the scene camera and hit enter...\")\n\nscene_camera = device.world_sensor()\nconnected = False if scene_camera is None else scene_camera.connected\nprint(\"Scene camera connected:\", connected)\n\ndevice.close()\n</code></pre> Output<pre><code>Looking for the next best device...\nScene camera connected: True\n(Dis)connect the scene camera and hit enter...\nScene camera connected: False\n</code></pre> Device Reference"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device","title":"Device","text":"<pre><code>Device(address: str, port: int, full_name: str | None = None, dns_name: str | None = None, start_streaming_by_default: bool = False, suppress_decoding_warnings: bool = True)\n</code></pre> <p>               Bases: <code>DeviceBase</code></p> <p>Simple synchronous API for interacting with Pupil Labs devices.</p> <p>This class provides a simplified, synchronous interface to Pupil Labs devices, wrapping the asynchronous API with a more user-friendly interface.</p> Important <p>Use [discover_devices][pupil_labs.realtime_api.simple.discovery. discover_devices] instead of initializing the class manually. See the simple_discovery_example example.</p> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>IP address of the device.</p> </li> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>Port number of the device.</p> </li> <li> <code>full_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Full service discovery name.</p> </li> <li> <code>dns_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>DNS name of the device.</p> </li> <li> <code>start_streaming_by_default</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to start streaming automatically.</p> </li> <li> <code>suppress_decoding_warnings</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to suppress decoding warnings.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>api_url</code>             \u2013              <p>Construct a full API URL for the given path.</p> </li> <li> <code>close</code>             \u2013              <p>Close the device connection and stop all background threads.</p> </li> <li> <code>convert_from</code>             \u2013              <p>Convert another device instance to this type.</p> </li> <li> <code>estimate_time_offset</code>             \u2013              <p>Estimate the time offset between the host device and the client.</p> </li> <li> <code>eye_events_sensor</code>             \u2013              <p>Get the eye events sensor.</p> </li> <li> <code>from_discovered_device</code>             \u2013              <p>Create a device instance from discovery information.</p> </li> <li> <code>gaze_sensor</code>             \u2013              <p>Get the gaze sensor.</p> </li> <li> <code>get_calibration</code>             \u2013              <p>Get the current cameras calibration data.</p> </li> <li> <code>get_errors</code>             \u2013              <p>Get a list of errors from the device.</p> </li> <li> <code>get_template</code>             \u2013              <p>Get the template currently selected on the device.</p> </li> <li> <code>get_template_data</code>             \u2013              <p>Get the template data entered on the device.</p> </li> <li> <code>post_template_data</code>             \u2013              <p>Send the data to the currently selected template.</p> </li> <li> <code>receive_eye_events</code>             \u2013              <p>Receive an eye event.</p> </li> <li> <code>receive_eyes_video_frame</code>             \u2013              <p>Receive an eye camera video frame.</p> </li> <li> <code>receive_gaze_datum</code>             \u2013              <p>Receive a gaze data point.</p> </li> <li> <code>receive_imu_datum</code>             \u2013              <p>Receive an IMU data point.</p> </li> <li> <code>receive_matched_scene_and_eyes_video_frames_and_gaze</code>             \u2013              <p>Receive a matched triplet of scene video frame, eye video frame, and gaze.</p> </li> <li> <code>receive_matched_scene_video_frame_and_gaze</code>             \u2013              <p>Receive a matched pair of scene video frame and gaze data.</p> </li> <li> <code>receive_scene_video_frame</code>             \u2013              <p>Receive a scene (world) video frame.</p> </li> <li> <code>recording_cancel</code>             \u2013              <p>Cancel the current recording without saving it.</p> </li> <li> <code>recording_start</code>             \u2013              <p>Start a recording on the device.</p> </li> <li> <code>recording_stop_and_save</code>             \u2013              <p>Stop and save the current recording.</p> </li> <li> <code>send_event</code>             \u2013              <p>Send an event to the device.</p> </li> <li> <code>start_stream_if_needed</code>             \u2013              <p>Start streaming if not already streaming.</p> </li> <li> <code>streaming_start</code>             \u2013              <p>Start streaming data from the specified sensor.</p> </li> <li> <code>streaming_stop</code>             \u2013              <p>Stop streaming data from the specified sensor.</p> </li> <li> <code>world_sensor</code>             \u2013              <p>Get the world sensor.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>battery_level_percent</code>               (<code>int</code>)           \u2013            <p>Get the battery level of the connected phone in percentage.</p> </li> <li> <code>battery_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Get the battery state of the connected phone.</p> </li> <li> <code>is_currently_streaming</code>               (<code>bool</code>)           \u2013            <p>Check if data streaming is currently active.</p> </li> <li> <code>memory_num_free_bytes</code>               (<code>int</code>)           \u2013            <p>Get the available memory of the connected phone in bytes.</p> </li> <li> <code>memory_state</code>               (<code>Literal['OK', 'LOW', 'CRITICAL']</code>)           \u2013            <p>Get the memory state of the connected phone.</p> </li> <li> <code>module_serial</code>               (<code>str | Literal['default'] | None</code>)           \u2013            <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> </li> <li> <code>phone_id</code>               (<code>str</code>)           \u2013            <p>Get the ID of the connected phone.</p> </li> <li> <code>phone_ip</code>               (<code>str</code>)           \u2013            <p>Get the IP address of the connected phone.</p> </li> <li> <code>phone_name</code>               (<code>str</code>)           \u2013            <p>Get the name of the connected phone.</p> </li> <li> <code>serial_number_glasses</code>               (<code>str | Literal['default'] | None</code>)           \u2013            <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> </li> <li> <code>serial_number_scene_cam</code>               (<code>str | None</code>)           \u2013            <p>Returns <code>None</code> if no scene camera is connected</p> </li> <li> <code>version_glasses</code>               (<code>str | None</code>)           \u2013            <p>Get the version of the connected glasses.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def __init__(\n    self,\n    address: str,\n    port: int,\n    full_name: str | None = None,\n    dns_name: str | None = None,\n    start_streaming_by_default: bool = False,\n    suppress_decoding_warnings: bool = True,\n) -&gt; None:\n    \"\"\"Initialize a Device instance.\n\n    Args:\n        address: IP address of the device.\n        port: Port number of the device.\n        full_name: Full service discovery name.\n        dns_name: DNS name of the device.\n        start_streaming_by_default: Whether to start streaming automatically.\n        suppress_decoding_warnings: Whether to suppress decoding warnings.\n\n    \"\"\"\n    super().__init__(\n        address,\n        port,\n        full_name=full_name,\n        dns_name=dns_name,\n        suppress_decoding_warnings=suppress_decoding_warnings,\n    )\n    self._status = self._get_status()\n    self._start_background_worker(start_streaming_by_default)\n\n    self._errors: list[str] = []\n\n    self.stream_name_start_event_map = {\n        SensorName.GAZE.value: self._EVENT.SHOULD_START_GAZE,\n        SensorName.WORLD.value: self._EVENT.SHOULD_START_WORLD,\n        SensorName.EYES.value: self._EVENT.SHOULD_START_EYES,\n        SensorName.IMU.value: self._EVENT.SHOULD_START_IMU,\n        SensorName.EYE_EVENTS.value: self._EVENT.SHOULD_START_EYE_EVENTS,\n    }\n\n    self.stream_name_stop_event_map = {\n        SensorName.GAZE.value: self._EVENT.SHOULD_STOP_GAZE,\n        SensorName.WORLD.value: self._EVENT.SHOULD_STOP_WORLD,\n        SensorName.EYES.value: self._EVENT.SHOULD_STOP_EYES,\n        SensorName.IMU.value: self._EVENT.SHOULD_STOP_IMU,\n        SensorName.EYE_EVENTS.value: self._EVENT.SHOULD_STOP_EYE_EVENTS,\n    }\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.battery_level_percent","title":"battery_level_percent  <code>property</code>","text":"<pre><code>battery_level_percent: int\n</code></pre> <p>Get the battery level of the connected phone in percentage.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.battery_state","title":"battery_state  <code>property</code>","text":"<pre><code>battery_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Get the battery state of the connected phone.</p> <p>Possible values are \"OK\", \"LOW\", or \"CRITICAL\".</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.is_currently_streaming","title":"is_currently_streaming  <code>property</code>","text":"<pre><code>is_currently_streaming: bool\n</code></pre> <p>Check if data streaming is currently active.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if streaming is active, False otherwise.</p> </li> </ul>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.memory_num_free_bytes","title":"memory_num_free_bytes  <code>property</code>","text":"<pre><code>memory_num_free_bytes: int\n</code></pre> <p>Get the available memory of the connected phone in bytes.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.memory_state","title":"memory_state  <code>property</code>","text":"<pre><code>memory_state: Literal['OK', 'LOW', 'CRITICAL']\n</code></pre> <p>Get the memory state of the connected phone.</p> <p>Possible values are \"OK\", \"LOW\", or \"CRITICAL\".</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.module_serial","title":"module_serial  <code>property</code>","text":"<pre><code>module_serial: str | Literal['default'] | None\n</code></pre> <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> Info <p>Only available on Neon.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.phone_id","title":"phone_id  <code>property</code>","text":"<pre><code>phone_id: str\n</code></pre> <p>Get the ID of the connected phone.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.phone_ip","title":"phone_ip  <code>property</code>","text":"<pre><code>phone_ip: str\n</code></pre> <p>Get the IP address of the connected phone.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.phone_name","title":"phone_name  <code>property</code>","text":"<pre><code>phone_name: str\n</code></pre> <p>Get the name of the connected phone.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.serial_number_glasses","title":"serial_number_glasses  <code>property</code>","text":"<pre><code>serial_number_glasses: str | Literal['default'] | None\n</code></pre> <p>Returns <code>None</code> or <code>\"default\"</code> if no glasses are connected</p> Info <p>Only available on Pupil Invisible.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.serial_number_scene_cam","title":"serial_number_scene_cam  <code>property</code>","text":"<pre><code>serial_number_scene_cam: str | None\n</code></pre> <p>Returns <code>None</code> if no scene camera is connected</p> Info <p>Only available on Pupil Invisible.</p>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.version_glasses","title":"version_glasses  <code>property</code>","text":"<pre><code>version_glasses: str | None\n</code></pre> <p>Get the version of the connected glasses.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str | None</code> )          \u2013            <p>1 -&gt; Pupil Invisible, 2 -&gt; Neon, or None -&gt; No glasses connected.</p> </li> </ul>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.api_url","title":"api_url","text":"<pre><code>api_url(path: APIPath, protocol: str = 'http', prefix: str = '/api') -&gt; str\n</code></pre> <p>Construct a full API URL for the given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>APIPath</code>)           \u2013            <p>API path to access.</p> </li> <li> <code>protocol</code>               (<code>str</code>, default:                   <code>'http'</code> )           \u2013            <p>Protocol to use (http).</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'/api'</code> )           \u2013            <p>API URL prefix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Complete URL for the API endpoint.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>def api_url(\n    self, path: APIPath, protocol: str = \"http\", prefix: str = \"/api\"\n) -&gt; str:\n    \"\"\"Construct a full API URL for the given path.\n\n    Args:\n        path: API path to access.\n        protocol: Protocol to use (http).\n        prefix: API URL prefix.\n\n    Returns:\n        Complete URL for the API endpoint.\n\n    \"\"\"\n    return path.full_address(\n        self.address, self.port, protocol=protocol, prefix=prefix\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the device connection and stop all background threads.</p> <p>This method should be called when the device is no longer needed to free up resources.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the device connection and stop all background threads.\n\n    This method should be called when the device is no longer needed\n    to free up resources.\n    \"\"\"\n    if self._event_manager:\n        if self.is_currently_streaming:\n            self.streaming_stop()\n        self._event_manager.trigger_threadsafe(self._EVENT.SHOULD_WORKER_CLOSE)\n        self._auto_update_thread.join()\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.convert_from","title":"convert_from  <code>classmethod</code>","text":"<pre><code>convert_from(other: T) -&gt; DeviceType\n</code></pre> <p>Convert another device instance to this type.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>T</code>)           \u2013            <p>Device instance to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Converted device instance.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef convert_from(cls: type[DeviceType], other: T) -&gt; DeviceType:\n    \"\"\"Convert another device instance to this type.\n\n    Args:\n        other: Device instance to convert.\n\n    Returns:\n        Converted device instance.\n\n    \"\"\"\n    return cls(\n        other.address,\n        other.port,\n        full_name=other.full_name,\n        dns_name=other.dns_name,\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.estimate_time_offset","title":"estimate_time_offset","text":"<pre><code>estimate_time_offset(number_of_measurements: int = 100, sleep_between_measurements_seconds: float | None = None) -&gt; TimeEchoEstimates | None\n</code></pre> <p>Estimate the time offset between the host device and the client.</p> <p>This uses the Time Echo protocol to estimate the clock offset between the device and the client.</p> <p>Parameters:</p> <ul> <li> <code>number_of_measurements</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of measurements to take.</p> </li> <li> <code>sleep_between_measurements_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional sleep time between</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TimeEchoEstimates</code> (              <code>TimeEchoEstimates | None</code> )          \u2013            <p>Statistics for roundtrip durations and time offsets, or None if estimation failed or device doesn't support the protocol.</p> </li> </ul> See Also <p>:mod:<code>pupil_labs.realtime_api.time_echo</code> for more details.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def estimate_time_offset(\n    self,\n    number_of_measurements: int = 100,\n    sleep_between_measurements_seconds: float | None = None,\n) -&gt; TimeEchoEstimates | None:\n    \"\"\"Estimate the time offset between the host device and the client.\n\n    This uses the Time Echo protocol to estimate the clock offset between\n    the device and the client.\n\n    Args:\n        number_of_measurements: Number of measurements to take.\n        sleep_between_measurements_seconds: Optional sleep time between\n        measurements.\n\n    Returns:\n        TimeEchoEstimates: Statistics for roundtrip durations and time offsets,\n            or None if estimation failed or device doesn't support the protocol.\n\n    See Also:\n        :mod:`pupil_labs.realtime_api.time_echo` for more details.\n\n    \"\"\"\n    if self._status.phone.time_echo_port is None:\n        logger.warning(\n            \"You Pupil Invisible Companion app is out-of-date and does not yet \"\n            \"support the Time Echo protocol. Upgrade to version 1.4.28 or newer.\"\n        )\n        return None\n    estimator = TimeOffsetEstimator(\n        self.phone_ip, self._status.phone.time_echo_port\n    )\n    return asyncio.run(\n        estimator.estimate(\n            number_of_measurements, sleep_between_measurements_seconds\n        )\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.eye_events_sensor","title":"eye_events_sensor","text":"<pre><code>eye_events_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the eye events sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def eye_events_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the eye events sensor.\"\"\"\n    return self._status.direct_eye_events_sensor()\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.from_discovered_device","title":"from_discovered_device  <code>classmethod</code>","text":"<pre><code>from_discovered_device(device: DiscoveredDeviceInfo) -&gt; DeviceType\n</code></pre> <p>Create a device instance from discovery information.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>DiscoveredDeviceInfo</code>)           \u2013            <p>Discovered device information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceType</code>           \u2013            <p>Device instance</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/base.py</code> <pre><code>@classmethod\ndef from_discovered_device(\n    cls: type[DeviceType], device: DiscoveredDeviceInfo\n) -&gt; DeviceType:\n    \"\"\"Create a device instance from discovery information.\n\n    Args:\n        device: Discovered device information.\n\n    Returns:\n        Device instance\n\n    \"\"\"\n    return cls(\n        device.addresses[0],\n        device.port,\n        full_name=device.name,\n        dns_name=device.server,\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.gaze_sensor","title":"gaze_sensor","text":"<pre><code>gaze_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the gaze sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def gaze_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the gaze sensor.\"\"\"\n    return self._status.direct_gaze_sensor()\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.get_calibration","title":"get_calibration","text":"<pre><code>get_calibration() -&gt; Calibration\n</code></pre> <p>Get the current cameras calibration data.</p> <p>Note that Pupil Invisible and Neon are calibration free systems, this refers to the intrinsincs and extrinsics of the cameras.</p> <p>Returns:</p> <ul> <li> <code>Calibration</code>           \u2013            <p>pupil_labs.neon_recording.calib.Calibration: The calibration data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the request fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_calibration(self) -&gt; Calibration:\n    \"\"\"Get the current cameras calibration data.\n\n    Note that Pupil Invisible and Neon are calibration free systems, this refers to\n    the intrinsincs and extrinsics of the cameras.\n\n    Returns:\n        pupil_labs.neon_recording.calib.Calibration: The calibration data.\n\n    Raises:\n        DeviceError: If the request fails.\n\n    \"\"\"\n\n    async def _get_calibration() -&gt; Calibration:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_calibration()\n\n    return asyncio.run(_get_calibration())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.get_errors","title":"get_errors","text":"<pre><code>get_errors() -&gt; list[str]\n</code></pre> <p>Get a list of errors from the device.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of error messages.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_errors(self) -&gt; list[str]:\n    \"\"\"Get a list of errors from the device.\n\n    Returns:\n        list[str]: List of error messages.\n\n    \"\"\"\n    errors = self._errors.copy()\n    self._errors.clear()\n\n    return errors\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.get_template","title":"get_template","text":"<pre><code>get_template() -&gt; Template\n</code></pre> <p>Get the template currently selected on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.get_template</p> <p>Returns:</p> <ul> <li> <code>Template</code> (              <code>Template</code> )          \u2013            <p>The currently selected template.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template can't be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_template(self) -&gt; Template:\n    \"\"\"Get the template currently selected on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.get_template][]\n\n    Returns:\n        Template: The currently selected template.\n\n    Raises:\n        DeviceError: If the template can't be fetched.\n\n    \"\"\"\n\n    async def _get_template() -&gt; Template:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_template()\n\n    return asyncio.run(_get_template())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.get_template_data","title":"get_template_data","text":"<pre><code>get_template_data(template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Get the template data entered on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.get_template_data</p> <p>Parameters:</p> <ul> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the returned data. \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]} \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result from the GET request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the template's data could not be fetched.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def get_template_data(self, template_format: TemplateDataFormat = \"simple\") -&gt; Any:\n    \"\"\"Get the template data entered on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.get_template_data][]\n\n    Args:\n        template_format: Format of the returned data.\n            \"api\" returns the data as is from the api e.g., {\"item_uuid\": [\"42\"]}\n            \"simple\" returns the data parsed e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result from the GET request.\n\n    Raises:\n        DeviceError: If the template's data could not be fetched.\n\n    \"\"\"\n\n    async def _get_template_data() -&gt; Any:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.get_template_data(template_format=template_format)\n\n    return asyncio.run(_get_template_data())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.post_template_data","title":"post_template_data","text":"<pre><code>post_template_data(template_data: dict[str, list[str]], template_format: TemplateDataFormat = 'simple') -&gt; Any\n</code></pre> <p>Send the data to the currently selected template.</p> <p>Wraps pupil_labs.realtime_api.device.Device.post_template_data</p> <p>Parameters:</p> <ul> <li> <code>template_data</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>The template data to send.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>, default:                   <code>'simple'</code> )           \u2013            <p>Format of the input data. \"api\" accepts the data as in realtime api format e.g., {\"item_uuid\": [\"42\"]} \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result from the POST request.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the data can not be sent.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid data type.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def post_template_data(\n    self,\n    template_data: dict[str, list[str]],\n    template_format: TemplateDataFormat = \"simple\",\n) -&gt; Any:\n    \"\"\"Send the data to the currently selected template.\n\n    Wraps [pupil_labs.realtime_api.device.Device.post_template_data][]\n\n    Args:\n        template_data: The template data to send.\n        template_format: Format of the input data.\n            \"api\" accepts the data as in realtime api format e.g.,\n            {\"item_uuid\": [\"42\"]}\n            \"simple\" accepts the data in parsed format e.g., {\"item_uuid\": 42}\n\n    Returns:\n        The result from the POST request.\n\n    Raises:\n        DeviceError: If the data can not be sent.\n        ValueError: If invalid data type.\n\n    \"\"\"\n\n    async def _post_template_data() -&gt; Any:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.post_template_data(\n                template_data, template_format=template_format\n            )\n\n    return asyncio.run(_post_template_data())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_eye_events","title":"receive_eye_events","text":"<pre><code>receive_eye_events(timeout_seconds: float | None = None) -&gt; FixationEventData | BlinkEventData | FixationOnsetEventData | None\n</code></pre> <p>Receive an eye event.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new eye event. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FixationEventData | BlinkEventData | FixationOnsetEventData | None</code>           \u2013            <p>FixationEventData | BlinkEventData | FixationOnsetEventData or None:</p> </li> <li> <code>FixationEventData | BlinkEventData | FixationOnsetEventData | None</code>           \u2013            <p>The received eye event, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_eye_events(\n    self, timeout_seconds: float | None = None\n) -&gt; FixationEventData | BlinkEventData | FixationOnsetEventData | None:\n    \"\"\"Receive an eye event.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new eye event.\n            If None, wait indefinitely.\n\n    Returns:\n        FixationEventData | BlinkEventData | FixationOnsetEventData or None:\n        The received eye event, or None if timeout was reached.\n\n    \"\"\"\n    return cast(\n        FixationEventData | BlinkEventData | FixationOnsetEventData,\n        self._receive_item(SensorName.EYE_EVENTS.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_eyes_video_frame","title":"receive_eyes_video_frame","text":"<pre><code>receive_eyes_video_frame(timeout_seconds: float | None = None) -&gt; SimpleVideoFrame | None\n</code></pre> <p>Receive an eye camera video frame.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new frame. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>SimpleVideoFrame or None: The received video frame, or None if timeout</p> </li> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_eyes_video_frame(\n    self, timeout_seconds: float | None = None\n) -&gt; SimpleVideoFrame | None:\n    \"\"\"Receive an eye camera video frame.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new frame.\n            If None, wait indefinitely.\n\n    Returns:\n        SimpleVideoFrame or None: The received video frame, or None if timeout\n        was reached.\n\n    \"\"\"\n    return cast(\n        SimpleVideoFrame,\n        self._receive_item(SensorName.EYES.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_gaze_datum","title":"receive_gaze_datum","text":"<pre><code>receive_gaze_datum(timeout_seconds: float | None = None) -&gt; GazeDataType | None\n</code></pre> <p>Receive a gaze data point.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new gaze datum. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GazeDataType | None</code>           \u2013            <p>GazeDataType or None: The received gaze data, or None if timeout was</p> </li> <li> <code>GazeDataType | None</code>           \u2013            <p>reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_gaze_datum(\n    self, timeout_seconds: float | None = None\n) -&gt; GazeDataType | None:\n    \"\"\"Receive a gaze data point.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new gaze datum.\n            If None, wait indefinitely.\n\n    Returns:\n        GazeDataType or None: The received gaze data, or None if timeout was\n        reached.\n\n    \"\"\"\n    return cast(\n        GazeDataType, self._receive_item(SensorName.GAZE.value, timeout_seconds)\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_imu_datum","title":"receive_imu_datum","text":"<pre><code>receive_imu_datum(timeout_seconds: float | None = None) -&gt; IMUData | None\n</code></pre> <p>Receive an IMU data point.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new IMU datum. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IMUData | None</code>           \u2013            <p>IMUData or None: The received IMU data, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_imu_datum(self, timeout_seconds: float | None = None) -&gt; IMUData | None:\n    \"\"\"Receive an IMU data point.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new IMU datum.\n            If None, wait indefinitely.\n\n    Returns:\n        IMUData or None: The received IMU data, or None if timeout was reached.\n\n    \"\"\"\n    return cast(IMUData, self._receive_item(SensorName.IMU.value, timeout_seconds))\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_matched_scene_and_eyes_video_frames_and_gaze","title":"receive_matched_scene_and_eyes_video_frames_and_gaze","text":"<pre><code>receive_matched_scene_and_eyes_video_frames_and_gaze(timeout_seconds: float | None = None) -&gt; MatchedGazeEyesSceneItem | None\n</code></pre> <p>Receive a matched triplet of scene video frame, eye video frame, and gaze.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a matched triplet. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MatchedGazeEyesSceneItem | None</code>           \u2013            <p>MatchedGazeEyesSceneItem or None: The matched triplet, or None if timeout</p> </li> <li> <code>MatchedGazeEyesSceneItem | None</code>           \u2013            <p>was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_matched_scene_and_eyes_video_frames_and_gaze(\n    self, timeout_seconds: float | None = None\n) -&gt; MatchedGazeEyesSceneItem | None:\n    \"\"\"Receive a matched triplet of scene video frame, eye video frame, and gaze.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a matched triplet.\n            If None, wait indefinitely.\n\n    Returns:\n        MatchedGazeEyesSceneItem or None: The matched triplet, or None if timeout\n        was reached.\n\n    \"\"\"\n    return cast(\n        MatchedGazeEyesSceneItem,\n        self._receive_item(MATCHED_GAZE_EYES_LABEL, timeout_seconds),\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_matched_scene_video_frame_and_gaze","title":"receive_matched_scene_video_frame_and_gaze","text":"<pre><code>receive_matched_scene_video_frame_and_gaze(timeout_seconds: float | None = None) -&gt; MatchedItem | None\n</code></pre> <p>Receive a matched pair of scene video frame and gaze data.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a matched pair. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MatchedItem | None</code>           \u2013            <p>MatchedItem or None: The matched pair, or None if timeout was reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_matched_scene_video_frame_and_gaze(\n    self, timeout_seconds: float | None = None\n) -&gt; MatchedItem | None:\n    \"\"\"Receive a matched pair of scene video frame and gaze data.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a matched pair.\n            If None, wait indefinitely.\n\n    Returns:\n        MatchedItem or None: The matched pair, or None if timeout was reached.\n\n    \"\"\"\n    return cast(\n        MatchedItem, self._receive_item(MATCHED_ITEM_LABEL, timeout_seconds)\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.receive_scene_video_frame","title":"receive_scene_video_frame","text":"<pre><code>receive_scene_video_frame(timeout_seconds: float | None = None) -&gt; SimpleVideoFrame | None\n</code></pre> <p>Receive a scene (world) video frame.</p> <p>Parameters:</p> <ul> <li> <code>timeout_seconds</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time to wait for a new frame. If None, wait indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>SimpleVideoFrame or None: The received video frame, or None if timeout was</p> </li> <li> <code>SimpleVideoFrame | None</code>           \u2013            <p>reached.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def receive_scene_video_frame(\n    self, timeout_seconds: float | None = None\n) -&gt; SimpleVideoFrame | None:\n    \"\"\"Receive a scene (world) video frame.\n\n    Args:\n        timeout_seconds: Maximum time to wait for a new frame.\n            If None, wait indefinitely.\n\n    Returns:\n        SimpleVideoFrame or None: The received video frame, or None if timeout was\n        reached.\n\n    \"\"\"\n    return cast(\n        SimpleVideoFrame,\n        self._receive_item(SensorName.WORLD.value, timeout_seconds),\n    )\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.recording_cancel","title":"recording_cancel","text":"<pre><code>recording_cancel() -&gt; None\n</code></pre> <p>Cancel the current recording without saving it.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_cancel</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be cancelled. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording not running</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_cancel(self) -&gt; None:\n    \"\"\"Cancel the current recording without saving it.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_cancel][]\n\n    Raises:\n        DeviceError: If the recording could not be cancelled. Possible reasons\n        include:\n            - Recording not running\n\n    \"\"\"\n\n    async def _cancel_recording() -&gt; None:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_cancel()\n\n    return asyncio.run(_cancel_recording())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.recording_start","title":"recording_start","text":"<pre><code>recording_start() -&gt; str\n</code></pre> <p>Start a recording on the device.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_start</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>ID of the started recording.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be started. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording already running</li> <li>Template has required fields</li> <li>Low battery</li> <li>Low storage</li> <li>No wearer selected</li> <li>No workspace selected</li> <li>Setup bottom sheets not completed</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_start(self) -&gt; str:\n    \"\"\"Start a recording on the device.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_start][]\n\n    Returns:\n        str: ID of the started recording.\n\n    Raises:\n        DeviceError: If the recording could not be started. Possible reasons\n        include:\n            - Recording already running\n            - Template has required fields\n            - Low battery\n            - Low storage\n            - No wearer selected\n            - No workspace selected\n            - Setup bottom sheets not completed\n\n    \"\"\"\n\n    async def _start_recording() -&gt; str:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_start()\n\n    return asyncio.run(_start_recording())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.recording_stop_and_save","title":"recording_stop_and_save","text":"<pre><code>recording_stop_and_save() -&gt; None\n</code></pre> <p>Stop and save the current recording.</p> <p>Wraps pupil_labs.realtime_api.device.Device.recording_stop_and_save</p> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If the recording could not be stopped. Possible reasons</p> </li> <li> <code>include</code>             \u2013            <ul> <li>Recording not running</li> <li>Template has required fields</li> </ul> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def recording_stop_and_save(self) -&gt; None:\n    \"\"\"Stop and save the current recording.\n\n    Wraps [pupil_labs.realtime_api.device.Device.recording_stop_and_save][]\n\n    Raises:\n        DeviceError: If the recording could not be stopped. Possible reasons\n        include:\n            - Recording not running\n            - Template has required fields\n\n    \"\"\"\n\n    async def _stop_and_save_recording() -&gt; None:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.recording_stop_and_save()\n\n    return asyncio.run(_stop_and_save_recording())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.send_event","title":"send_event","text":"<pre><code>send_event(event_name: str, event_timestamp_unix_ns: int | None = None) -&gt; Event\n</code></pre> <p>Send an event to the device.</p> <p>Parameters:</p> <ul> <li> <code>event_name</code>               (<code>str</code>)           \u2013            <p>Name of the event.</p> </li> <li> <code>event_timestamp_unix_ns</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional timestamp in unix nanoseconds. If None, the current time will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Event</code> (              <code>Event</code> )          \u2013            <p>The created event.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DeviceError</code>             \u2013            <p>If sending the event fails.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def send_event(\n    self, event_name: str, event_timestamp_unix_ns: int | None = None\n) -&gt; Event:\n    \"\"\"Send an event to the device.\n\n    Args:\n        event_name: Name of the event.\n        event_timestamp_unix_ns: Optional timestamp in unix nanoseconds.\n            If None, the current time will be used.\n\n    Returns:\n        Event: The created event.\n\n    Raises:\n        DeviceError: If sending the event fails.\n\n    \"\"\"\n\n    async def _send_event() -&gt; Event:\n        async with _DeviceAsync.convert_from(self) as control:\n            return await control.send_event(event_name, event_timestamp_unix_ns)\n\n    return asyncio.run(_send_event())\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.start_stream_if_needed","title":"start_stream_if_needed","text":"<pre><code>start_stream_if_needed(sensor: str) -&gt; None\n</code></pre> <p>Start streaming if not already streaming.</p> <p>Parameters:</p> <ul> <li> <code>sensor</code>               (<code>str</code>)           \u2013            <p>Sensor name to check.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def start_stream_if_needed(self, sensor: str) -&gt; None:\n    \"\"\"Start streaming if not already streaming.\n\n    Args:\n        sensor: Sensor name to check.\n\n    \"\"\"\n    if not self._is_streaming_flags[sensor].is_set():\n        logger.debug(\"receive_* called without being streaming\")\n        self.streaming_start(sensor)\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.streaming_start","title":"streaming_start","text":"<pre><code>streaming_start(stream_name: str) -&gt; None\n</code></pre> <p>Start streaming data from the specified sensor.</p> <p>Parameters:</p> <ul> <li> <code>stream_name</code>               (<code>str</code>)           \u2013            <p>Name of the sensor to start streaming from. It can be one of</p> </li> <li> <code></code>           \u2013            <p>py:attr:<code>SensorName</code> values or None, which will start all streams.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the stream name is not recognized.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def streaming_start(self, stream_name: str) -&gt; None:\n    \"\"\"Start streaming data from the specified sensor.\n\n    Args:\n        stream_name: Name of the sensor to start streaming from. It can be one of\n        :py:attr:`SensorName` values or None, which will start all streams.\n\n    Raises:\n        ValueError: If the stream name is not recognized.\n\n    \"\"\"\n    if stream_name is None:\n        for event in (\n            self._EVENT.SHOULD_START_GAZE,\n            self._EVENT.SHOULD_START_WORLD,\n            self._EVENT.SHOULD_START_EYES,\n            self._EVENT.SHOULD_START_IMU,\n            self._EVENT.SHOULD_START_EYE_EVENTS,\n        ):\n            self._streaming_trigger_action(event)\n        return\n\n    event = self.stream_name_start_event_map[stream_name]\n    self._streaming_trigger_action(event)\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.streaming_stop","title":"streaming_stop","text":"<pre><code>streaming_stop(stream_name: str | None = None) -&gt; None\n</code></pre> <p>Stop streaming data from the specified sensor.</p> <p>Parameters:</p> <ul> <li> <code>stream_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the sensor to start streaming from. It can be one of</p> </li> <li> <code></code>           \u2013            <p>py:attr:<code>SensorName</code> values or None, which will stop all streams.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the stream name is not recognized.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def streaming_stop(self, stream_name: str | None = None) -&gt; None:\n    \"\"\"Stop streaming data from the specified sensor.\n\n    Args:\n        stream_name: Name of the sensor to start streaming from. It can be one of\n        :py:attr:`SensorName` values or None, which will stop all streams.\n\n    Raises:\n        ValueError: If the stream name is not recognized.\n\n    \"\"\"\n    if stream_name is None:\n        for event in (\n            self._EVENT.SHOULD_STOP_GAZE,\n            self._EVENT.SHOULD_STOP_WORLD,\n            self._EVENT.SHOULD_STOP_EYES,\n            self._EVENT.SHOULD_STOP_IMU,\n            self._EVENT.SHOULD_STOP_EYE_EVENTS,\n        ):\n            self._streaming_trigger_action(event)\n        return\n\n    event = self.stream_name_stop_event_map[stream_name]\n    self._streaming_trigger_action(event)\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#pupil_labs.realtime_api.simple.Device.world_sensor","title":"world_sensor","text":"<pre><code>world_sensor() -&gt; Sensor | None\n</code></pre> <p>Get the world sensor.</p> Source code in <code>src/pupil_labs/realtime_api/simple/device.py</code> <pre><code>def world_sensor(self) -&gt; Sensor | None:\n    \"\"\"Get the world sensor.\"\"\"\n    return self._status.direct_world_sensor()\n</code></pre>"},{"location":"methods/simple/connect-to-a-device/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here discover_devices.py<pre><code>from pupil_labs.realtime_api.simple import discover_devices, discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\\n\\t\", end=\"\")\nprint(discover_one_device(max_search_duration_seconds=10.0))\n\n# List all devices that could be found within 10 seconds\nprint(\"Starting 10 second search...\\n\\t\", end=\"\")\nprint(discover_devices(search_duration_seconds=10.0))\n</code></pre> get_status.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# Device status is fetched on initialization and kept up-to-date in the background\n\nprint(f\"Phone IP address: {device.phone_ip}\")\nprint(f\"Phone name: {device.phone_name}\")\nprint(f\"Phone unique ID: {device.phone_id}\")\n\nprint(f\"Battery level: {device.battery_level_percent}%\")\nprint(f\"Battery state: {device.battery_state}\")\n\nprint(f\"Free storage: {device.memory_num_free_bytes / 1024**3}GB\")\nprint(f\"Storage level: {device.memory_state}\")\n\nprint(f\"Connected glasses: SN {device.serial_number_glasses}\")\nprint(f\"Connected scene camera: SN {device.serial_number_scene_cam}\")\n\ndevice.close()  # explicitly stop auto-update\n</code></pre> status_auto_update.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\nscene_camera = device.world_sensor()\nconnected = False if scene_camera is None else scene_camera.connected\nprint(\"Scene camera connected:\", connected)\n\ninput(\"(Dis)connect the scene camera and hit enter...\")\n\nscene_camera = device.world_sensor()\nconnected = False if scene_camera is None else scene_camera.connected\nprint(\"Scene camera connected:\", connected)\n\ndevice.close()\n</code></pre>"},{"location":"methods/simple/others/","title":"Others","text":""},{"location":"methods/simple/others/#time-offset-estimation","title":"Time Offset Estimation","text":"<p>1.1.0</p> <p>Additionally, we offer you a convenient function to estimate the time offset between the device and your computer using the <code>device.estimate_time_offset</code> method.</p> <p>See <code>time_echo</code> for details.</p> <pre><code>estimate = device.estimate_time_offset()\nif estimate is None:\n    device.close()\n    raise SystemExit(\"Pupil Companion app is too old\")\n\nprint(f\"Mean time offset: {estimate.time_offset_ms.mean} ms\")\nprint(f\"Mean roundtrip duration: {estimate.roundtrip_duration_ms.mean} ms\")\n</code></pre> <pre><code>Mean time offset: -37.53 ms\nMean roundtrip duration: 12.91 ms\n</code></pre> Check the whole example code here device_time_offset.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    raise SystemExit(\"No device found.\")\n\nestimate = device.estimate_time_offset()\nif estimate is None:\n    device.close()\n    raise SystemExit(\"Pupil Companion app is too old\")\n\nprint(f\"Mean time offset: {estimate.time_offset_ms.mean} ms\")\nprint(f\"Mean roundtrip duration: {estimate.roundtrip_duration_ms.mean} ms\")\n\ndevice.close()\n</code></pre> <p>Wanna get super precise time sync?</p> <p>Have a look at our tutorial here.</p>"},{"location":"methods/simple/others/#camera-calibration","title":"Camera calibration","text":"<p>Neon</p> <p>Getting the camera calibration coefficients can be extremely useful for undistorting the video. You can receive camera calibration parameters using the get_calibration method.</p> <pre><code>calibration = device.get_calibration()\n</code></pre> <p>Returns a <code>pupil_labs.neon_recording.calib.Calibration</code> object.</p> <pre><code>Calibration(\n    version = np.uint8(1)\n    serial = '841684'\n    scene_camera_matrix = array([[896.23068471,   0.        , 790.8950718 ],\n                [  0.        , 895.99428647, 593.30938736],\n                [  0.        ,   0.        ,   1.        ]])\n    scene_distortion_coefficients = array([-0.13185823,  0.11141446, -0.00072215, -0.00019211, -0.00102044,\n                0.17091784,  0.05497444,  0.02371847])\n    scene_extrinsics_affine_matrix = array([[1., 0., 0., 0.],\n                [0., 1., 0., 0.],\n                [0., 0., 1., 0.],\n                [0., 0., 0., 1.]])\n    right_camera_matrix = array([[140.03968681,   0.        ,  99.07925009],\n                [  0.        , 140.16685902,  96.21073359],\n                [  0.        ,   0.        ,   1.        ]])\n    right_distortion_coefficients = array([ 4.88968666e-02, -1.28678179e-01, -2.42854366e-04,  6.16360859e-04,\n                -6.13765032e-01, -4.34790467e-02,  3.41057533e-02, -6.83627299e-01])\n    right_extrinsics_affine_matrix = array([[-0.8363896 ,  0.14588414,  0.52836567, 16.93598175],\n                [ 0.05819079,  0.98211712, -0.17905241, 19.64488983],\n                [-0.54503787, -0.11901156, -0.82992166, -7.03995514],\n                [ 0.        ,  0.        ,  0.        ,  1.        ]])\n    left_camera_matrix = array([[139.60850687,   0.        ,  93.21881139],\n                [  0.        , 139.73659663,  95.43463863],\n                [  0.        ,   0.        ,   1.        ]])\n    left_distortion_coefficients = array([ 4.95496340e-02, -1.27421933e-01,  6.92379886e-04,  4.98479011e-04,\n                -6.26153622e-01, -4.43117940e-02,  3.31060602e-02, -6.91888536e-01])\n    left_extrinsics_affine_matrix = array([[ -0.83850485,  -0.13447338,  -0.52804023, -17.65301514],\n                [ -0.05493483,   0.98499447,  -0.16360955,  19.88935852],\n                [  0.54211783,  -0.10817961,  -0.83330995,  -7.48944855],\n                [  0.        ,   0.        ,   0.        ,   1.        ]])\n    crc = np.uint32(734156985)\n)\n</code></pre> <p>Wanna know how to undistort the video?\"</p> <p>Get a look at our tutorial.</p> Check the whole example code here camera_calibration.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# Device status is fetched on initialization and kept up-to-date in the background\n\ncalibration = device.get_calibration()\n\nprint(\"Scene camera matrix:\")\nprint(calibration[\"scene_camera_matrix\"][0])\nprint(\"\\nScene distortion coefficients:\")\nprint(calibration[\"scene_distortion_coefficients\"][0])\n\nprint(\"\\nRight camera matrix:\")\nprint(calibration[\"right_camera_matrix\"][0])\nprint(\"\\nRight distortion coefficients:\")\nprint(calibration[\"right_distortion_coefficients\"][0])\n\nprint(\"\\nLeft camera matrix:\")\nprint(calibration[\"left_camera_matrix\"][0])\nprint(\"\\nLeft distortion coefficients:\")\nprint(calibration[\"left_distortion_coefficients\"][0])\n\ndevice.close()\n</code></pre>"},{"location":"methods/simple/remote-control/","title":"Remote Control","text":"<p>With the device connected, you can remotely control your device, similarly to the Monitor App and start, stop, save and annotate recordings.</p>"},{"location":"methods/simple/remote-control/#start-a-recording","title":"Start a Recording","text":"<p>It is possible to remotely control your device. Use <code>device.recording_start</code> to start a recording on the device and return the recording ID.</p> start_stop_recordings.py<pre><code>print(\"Starting recording\")\nrecording_id = device.recording_start()\n</code></pre> <pre><code>Started recording with id 54e7d0bb-5b4c-40e6-baed-f2e11eb4f53b\n</code></pre>"},{"location":"methods/simple/remote-control/#stop-save-a-recording","title":"Stop &amp; Save a Recording","text":"<p>You can stop and save a recording using <code>device.recording_stop_and_save</code>. Note that if you have a mandatory question that is not filled, the recording will not be saved until that question is answered.</p> start_stop_recordings.py<pre><code>device.recording_stop_and_save()\n</code></pre> <pre><code>Recording stopped and saved\n</code></pre>"},{"location":"methods/simple/remote-control/#cancel-a-recording","title":"Cancel a Recording","text":"<p>You can also cancel (<code>device.recording_cancel</code>) a recording if you don't want to save it, just be aware that this will delete the recording and all its data.</p> start_stop_recordings.py<pre><code>device.recording_cancel()\n</code></pre>"},{"location":"methods/simple/remote-control/#save-events","title":"Save Events","text":"<p>A common requirement is to save events with your recording. This is made possible using the <code>device.send_event</code> method. By default, the Neon device receiving the event will assign a timestamp to it, using the time of arrival. Optionally, you can set a custom nanosecond timestamp for your event instead. We also show you how to measure the clock offset between the Neon device and client computer, which may be useful for synchronising third-party clocks with your Neon device.</p> Timestamped on Arrival (Host/Companion Device)With Explicit TimestampWith Manual Clock Offset Correction <p>send_event.py<pre><code>print(device.send_event(\"test event; timestamped at arrival\"))\n</code></pre> <pre><code>Event(name=test event; timestamped at arrival recording_id=None timestamp_unix_ns=1744271292116000000 datetime=2025-04-10 09:48:12.116000)\n</code></pre></p> <p>send_event.py<pre><code># send event with current timestamp\nprint(\n    device.send_event(\n        \"test event; timestamped by the client, relying on NTP for sync\",\n        event_timestamp_unix_ns=time.time_ns(),\n    )\n</code></pre> <pre><code>Event(name=test event; timestamped by the client, relying on NTP for sync recording_id=None timestamp_unix_ns=1744271291692745000 datetime=2025-04-10 09:48:11.692745)\n</code></pre></p> <p>send_event.py<pre><code># Estimate clock offset between Companion device and client script\n# (only needs to be done once)\nestimate = device.estimate_time_offset()\nclock_offset_ns = round(estimate.time_offset_ms.mean * 1_000_000)\nprint(f\"Clock offset: {clock_offset_ns:_d} ns\")\n\n# send event with current timestamp, but correct it manual for possible clock offset\ncurrent_time_ns_in_client_clock = time.time_ns()\ncurrent_time_ns_in_companion_clock = current_time_ns_in_client_clock - clock_offset_ns\nprint(\n    device.send_event(\n        \"test event; timestamped by the client, manual clock offset correction\",\n        event_timestamp_unix_ns=current_time_ns_in_companion_clock,\n    )\n)\n</code></pre> <pre><code>Clock offset: -437_790_000 ns\nEvent(name=test event; timestamped by the client, manual clock offset correction recording_id=None timestamp_unix_ns=1744271293119536000 datetime=2025-04-10 09:48:13.119536)\n</code></pre></p> <p>Events will only be saved if a recording is running. If you send an event while the Companion app is not recording, it will be discarded.</p>"},{"location":"methods/simple/remote-control/#check-for-errors","title":"Check for Errors","text":"<p>You can also monitor the recording for potential errors. Add the <code>device.get_errors</code> method to your code to get notified of errors (if they happen) during your recording.</p> <p>Neon +2.9.0 +1.5.0</p> start_stop_recordings.py<pre><code># Check for errors while recording runs\nstart_time = time.time()\nwhile time.time() - start_time &lt; 5:\n    for e in device.get_errors():\n        print(\"Error:\", e)\n</code></pre> <pre><code>Error: Recording Watchdog failure\n</code></pre>"},{"location":"methods/simple/remote-control/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here start_stop_recordings.py<pre><code>import time\n\nfrom pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\nprint(\"Starting recording\")\nrecording_id = device.recording_start()\nprint(f\"Started recording with id {recording_id}\")\n\n# Check for errors while recording runs\nstart_time = time.time()\nwhile time.time() - start_time &lt; 5:\n    for e in device.get_errors():\n        print(\"Error:\", e)\n\n    time.sleep(1)\n\ndevice.recording_stop_and_save()\n\nprint(\"Recording stopped and saved\")\n# device.recording_cancel()  # uncomment to cancel recording\n\ndevice.close()\n</code></pre> send_event.py<pre><code>import time\n\nfrom pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\nprint(device.send_event(\"test event; timestamped at arrival\"))\n\n# send event with current timestamp\nprint(\n    device.send_event(\n        \"test event; timestamped by the client, relying on NTP for sync\",\n        event_timestamp_unix_ns=time.time_ns(),\n    )\n)\n\n# Estimate clock offset between Companion device and client script\n# (only needs to be done once)\nestimate = device.estimate_time_offset()\nclock_offset_ns = round(estimate.time_offset_ms.mean * 1_000_000)\nprint(f\"Clock offset: {clock_offset_ns:_d} ns\")\n\n# send event with current timestamp, but correct it manual for possible clock offset\ncurrent_time_ns_in_client_clock = time.time_ns()\ncurrent_time_ns_in_companion_clock = current_time_ns_in_client_clock - clock_offset_ns\nprint(\n    device.send_event(\n        \"test event; timestamped by the client, manual clock offset correction\",\n        event_timestamp_unix_ns=current_time_ns_in_companion_clock,\n    )\n)\n\n\ndevice.close()\n</code></pre>"},{"location":"methods/simple/templates/","title":"Templates","text":"<p>Neon +1.3.0 +2.8.25</p> <p>You can access the response data entered into the template questionnaire on the phone and also set those responses remotely. If the template is properly configured, this allows you to also define the recording name.</p>"},{"location":"methods/simple/templates/#get-template-definition","title":"Get Template Definition","text":"<p>Using the <code>device.get_template</code> method, you can receive the definition of the template containing all questions and sections.</p> <pre><code>template = device.get_template()\n</code></pre> Template"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template","title":"Template","text":"<p>Template Class for data collection.</p> <p>Methods:</p> <ul> <li> <code>convert_from_api_to_simple_format</code>             \u2013              <p>Convert data from API format to simple format.</p> </li> <li> <code>convert_from_simple_to_api_format</code>             \u2013              <p>Convert data from simple format to API format.</p> </li> <li> <code>get_question_by_id</code>             \u2013              <p>Get a template item by ID.</p> </li> <li> <code>validate_answers</code>             \u2013              <p>Validate answers for this Template.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>archived_at</code>               (<code>datetime | None</code>)           \u2013            <p>Archival timestamp (if archived).</p> </li> <li> <code>created_at</code>               (<code>datetime</code>)           \u2013            <p>Creation timestamp.</p> </li> <li> <code>description</code>               (<code>str | None</code>)           \u2013            <p>Template description.</p> </li> <li> <code>id</code>               (<code>UUID</code>)           \u2013            <p>Unique identifier.</p> </li> <li> <code>is_default_template</code>               (<code>bool</code>)           \u2013            <p>Whether this is the default template for the Workspace</p> </li> <li> <code>items</code>               (<code>list[TemplateItem]</code>)           \u2013            <p>List of template items.</p> </li> <li> <code>label_ids</code>               (<code>list[UUID]</code>)           \u2013            <p>Associated label IDs.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Template name.</p> </li> <li> <code>published_at</code>               (<code>datetime | None</code>)           \u2013            <p>Publication timestamp.</p> </li> <li> <code>recording_ids</code>               (<code>list[UUID] | None</code>)           \u2013            <p>Associated recording IDs.</p> </li> <li> <code>recording_name_format</code>               (<code>list[str]</code>)           \u2013            <p>Format for recording name.</p> </li> <li> <code>updated_at</code>               (<code>datetime</code>)           \u2013            <p>Last update timestamp.</p> </li> </ul>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.archived_at","title":"archived_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>archived_at: datetime | None = None\n</code></pre> <p>Archival timestamp (if archived).</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre> <p>Creation timestamp.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Template description.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: UUID\n</code></pre> <p>Unique identifier.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.is_default_template","title":"is_default_template  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_default_template: bool = True\n</code></pre> <p>Whether this is the default template for the Workspace</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.items","title":"items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>items: list[TemplateItem] = field(default_factory=list)\n</code></pre> <p>List of template items.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.label_ids","title":"label_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_ids: list[UUID] = field(default_factory=list, metadata={'readonly': True})\n</code></pre> <p>Associated label IDs.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Template name.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.published_at","title":"published_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>published_at: datetime | None = None\n</code></pre> <p>Publication timestamp.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.recording_ids","title":"recording_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>recording_ids: list[UUID] | None = None\n</code></pre> <p>Associated recording IDs.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.recording_name_format","title":"recording_name_format  <code>instance-attribute</code>","text":"<pre><code>recording_name_format: list[str]\n</code></pre> <p>Format for recording name.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.updated_at","title":"updated_at  <code>instance-attribute</code>","text":"<pre><code>updated_at: datetime\n</code></pre> <p>Last update timestamp.</p>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.convert_from_api_to_simple_format","title":"convert_from_api_to_simple_format","text":"<pre><code>convert_from_api_to_simple_format(data: dict[str, list[str]]) -&gt; dict[str, Any]\n</code></pre> <p>Convert data from API format to simple format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Data in API format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, Any]</code> )          \u2013            <p>Data in simple format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_api_to_simple_format(\n    self, data: dict[str, list[str]]\n) -&gt; dict[str, Any]:\n    \"\"\"Convert data from API format to simple format.\n\n    Args:\n        data: Data in API format.\n\n    Returns:\n        dict: Data in simple format.\n\n    \"\"\"\n    simple_format = {}\n    for question_id, value in data.items():\n        question = self.get_question_by_id(question_id)\n        if question is None:\n            logger.warning(\n                f\"Skipping unknown question ID '{question_id}' during API to \"\n                f\"simple conversion.\"\n            )\n            continue\n        processed_value: Any\n        if question.widget_type in {\"CHECKBOX_LIST\", \"RADIO_LIST\"}:\n            if question.choices is None:\n                logger.warning(\n                    f\"Question {question_id} (type {question.widget_type}) \"\n                    f\"has no choices defined.\"\n                )\n                processed_value = []\n            elif value == [\"\"] and \"\" not in question.choices:\n                processed_value = []\n        else:\n            if not value:\n                value = [\"\"]\n\n            value_str = value[0]\n            if question.input_type != \"any\":\n                processed_value = (\n                    None if value_str == \"\" else question._value_type(value_str)\n                )\n            else:\n                processed_value = value\n\n        simple_format[question_id] = processed_value\n    return simple_format\n</code></pre>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.convert_from_simple_to_api_format","title":"convert_from_simple_to_api_format","text":"<pre><code>convert_from_simple_to_api_format(data: dict[str, Any]) -&gt; dict[str, list[Any]]\n</code></pre> <p>Convert data from simple format to API format.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Data in simple format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, list[Any]]</code> )          \u2013            <p>Data in API format.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def convert_from_simple_to_api_format(\n    self, data: dict[str, Any]\n) -&gt; dict[str, list[Any]]:\n    \"\"\"Convert data from simple format to API format.\n\n    Args:\n        data: Data in simple format.\n\n    Returns:\n        dict: Data in API format.\n\n    \"\"\"\n    api_format = {}\n    for question_id, value in data.items():\n        if value is None:\n            value = \"\"\n        if not isinstance(value, list):\n            value = [value]\n\n        api_format[question_id] = value\n    return api_format\n</code></pre>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.get_question_by_id","title":"get_question_by_id","text":"<pre><code>get_question_by_id(question_id: str | UUID) -&gt; TemplateItem | None\n</code></pre> <p>Get a template item by ID.</p> <p>Parameters:</p> <ul> <li> <code>question_id</code>               (<code>str | UUID</code>)           \u2013            <p>ID of the template item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TemplateItem | None</code>           \u2013            <p>TemplateItem | None: The template item, or None if not found.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def get_question_by_id(self, question_id: str | UUID) -&gt; TemplateItem | None:\n    \"\"\"Get a template item by ID.\n\n    Args:\n        question_id: ID of the template item.\n\n    Returns:\n        TemplateItem | None: The template item, or None if not found.\n\n    \"\"\"\n    for item in self.items:\n        if str(item.id) == str(question_id):\n            return item\n    return None\n</code></pre>"},{"location":"methods/simple/templates/#pupil_labs.realtime_api.models.Template.validate_answers","title":"validate_answers","text":"<pre><code>validate_answers(answers: dict[str, list[str]], template_format: TemplateDataFormat, raise_exception: bool = True) -&gt; list[ErrorDetails]\n</code></pre> <p>Validate answers for this Template.</p> <p>Parameters:</p> <ul> <li> <code>answers</code>               (<code>dict[str, list[str]]</code>)           \u2013            <p>Answers to validate.</p> </li> <li> <code>raise_exception</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to raise an exception on validation failure.</p> </li> <li> <code>template_format</code>               (<code>TemplateDataFormat</code>)           \u2013            <p>Format of the answers (\"simple\" or \"api\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[ErrorDetails]</code> )          \u2013            <p>List of validation errors, or empty list if validation succeeded.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>InvalidTemplateAnswersError</code>             \u2013            <p>If validation fails and raise_exception is</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/models.py</code> <pre><code>def validate_answers(\n    self,\n    answers: dict[str, list[str]],\n    template_format: TemplateDataFormat,\n    raise_exception: bool = True,\n) -&gt; list[ErrorDetails]:\n    \"\"\"Validate answers for this Template.\n\n    Args:\n        answers: Answers to validate.\n        raise_exception: Whether to raise an exception on validation failure.\n        template_format: Format of the answers (\"simple\" or \"api\").\n\n    Returns:\n        list: List of validation errors, or empty list if validation succeeded.\n\n    Raises:\n        InvalidTemplateAnswersError: If validation fails and raise_exception is\n        True.\n\n    \"\"\"\n    AnswerModel = self._create_answer_model(template_format=template_format)\n    errors = []\n    try:\n        AnswerModel(**answers)\n    except ValidationError as e:\n        errors = e.errors()\n\n    for error in errors:\n        question_id = error[\"loc\"][0]\n        question = self.get_question_by_id(str(question_id))\n        if question:\n            error[\"question\"] = asdict(question)  # type: ignore[typeddict-unknown-key]\n\n    if errors and raise_exception:\n        raise InvalidTemplateAnswersError(self, answers, errors)\n    return errors\n</code></pre>"},{"location":"methods/simple/templates/#get-template-data","title":"Get Template Data","text":"<p>Using the <code>device.get_template_data</code> method, you can receive the responses currently saved in the template.</p> <pre><code>data = device.get_template_data()\n</code></pre>"},{"location":"methods/simple/templates/#set-template-data","title":"Set Template Data","text":"<p>And using the <code>device.post_template_data</code> method, you can set the template responses remotely.</p> <pre><code>    device.post_template_data(questionnaire)\n</code></pre>"},{"location":"methods/simple/templates/#get-template-questions-validate-them","title":"Get Template Questions &amp; Validate them","text":"<p>You can also retrieve individual questions by their ID using the <code>template.get_question_by_id</code> method and check the validity of a response using the [<code>template.validate_answer</code>][pupil_labs.realtime_api.models.Template.validate_answer] method.</p>"},{"location":"methods/simple/templates/#see-it-in-action","title":"See it in action","text":"Check the whole example code here templates.py<pre><code>import os\n\nimport beaupy\n\nfrom pupil_labs.realtime_api.models import InvalidTemplateAnswersError, TemplateItem\nfrom pupil_labs.realtime_api.simple import discover_one_device\n\n# handle KeyboardInterrupts ourselves\nbeaupy.Config.raise_on_interrupt = True\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# Fetch current template definition\ntemplate = device.get_template()\n\n# Fetch data filled on the template\ndata = device.get_template_data()\n\nLINE = \"\\u2500\" * os.get_terminal_size().columns\nRED = \"\\033[31m\"\nRESET = \"\\033[0m\"\n\nprint(f\"[{template.name}] Data pre-filled:\")\nprint(LINE)\nprint(\"\\n\".join(f\"{k}\\t{v}\" for k, v in data.items()))\n\n\ndef prompt_checkbox_answer(item: TemplateItem, current_value):\n    ticked = []\n    for i, choice in enumerate(item.choices):\n        current_value: list\n        if choice in (current_value or []):\n            current_value.remove(choice)\n            ticked.append(i)\n    choices = beaupy.select_multiple(\n        item.choices,\n        ticked_indices=ticked,\n    )\n    return choices\n\n\ndef prompt_radio_answer(item: TemplateItem, current_value):\n    cursor_index = 0\n    if current_value and current_value[0] in item.choices:\n        cursor_index = item.choices.index(current_value[0])\n\n    choice = beaupy.select(item.choices, cursor_index=cursor_index)\n    template_input = []\n    if choice is not None:\n        template_input = [choice]\n    return template_input\n\n\ndef prompt_string_answer(item: TemplateItem, current_value):\n    placeholder = item.help_text if item.help_text and item.help_text != [\"\"] else None\n    current_value = (\n        placeholder if not current_value or current_value == [\"\"] else current_value\n    )\n    return beaupy.prompt(\n        f\"Enter value for '{item.title}': \",\n        initial_value=\"\" if current_value is None else str(current_value),\n    )\n\n\n# Filling a template\nquestionnaire = {}\nif template:\n    try:\n        for item in template.items:\n            if item.widget_type in (\"SECTION_HEADER\", \"PAGE_BREAK\"):\n                continue\n            print(LINE)\n            print(\n                f\"{'* ' if item.required else ''}\"\n                f\"ID: {item.id} - Title: {item.title} \"\n                f\"- Input Type: {item.input_type}\"\n            )\n            current_value = data.get(str(item.id))\n            while True:\n                question = template.get_question_by_id(item.id)\n                if item.widget_type == \"CHECKBOX_LIST\":\n                    template_input = prompt_checkbox_answer(item, current_value)\n                elif item.widget_type == \"RADIO_LIST\":\n                    template_input = prompt_radio_answer(item, current_value)\n                else:\n                    template_input = prompt_string_answer(item, current_value)\n\n                try:\n                    print(template_input)\n                    errors = question.validate_answer(template_input)\n                    if not errors:\n                        questionnaire[str(item.id)] = template_input\n                        break\n                    else:\n                        print(f\"Errors: {errors}\")\n                except InvalidTemplateAnswersError as e:\n                    print(f\"{RED}Validation failed for: {template_input}\")\n                    for error in e.errors:\n                        print(f\"    {error['msg']}\")\n                    print(LINE + RESET)\n    except KeyboardInterrupt:\n        print(\"\\nKeyboardInterrupt detected. Skipping the rest of the template\")\n\nprint(LINE)\n# Sending the template\nif questionnaire:\n    device.post_template_data(questionnaire)\n\n# Fetch new data filled on the template\ndata = device.get_template_data()\n\n# Iterate to check filled data\nprint(f\"[{template.name}] Data post:\")\nprint(LINE)\nprint(\"\\n\".join(f\"{k}\\t{v}\" for k, v in data.items()))\n\ndevice.close()\n</code></pre>"},{"location":"methods/simple/streaming/eye-cameras/","title":"Stream Eye Cameras","text":"<p>Neon +1.1.2</p> <p>You can receive eye camera video frames using the <code>receive_eyes_video_frame</code> method.</p> <pre><code>bgr_pixels, frame_datetime = device.receive_eyes_video_frame()\n</code></pre> SimpleVideoFrame Check the whole example code here stream_eyes_camera_video.py<pre><code>import cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api.simple import discover_one_device  # noqa: E402\n\n\ndef main():\n    # Look for devices. Returns as soon as it has found the first device.\n    print(\"Looking for the next best device...\")\n    device = discover_one_device(max_search_duration_seconds=10)\n    if device is None:\n        print(\"No device found.\")\n        raise SystemExit(-1)\n\n    print(f\"Connecting to {device}...\")\n\n    try:\n        while True:\n            bgr_pixels, frame_datetime = device.receive_eyes_video_frame()\n            draw_time(bgr_pixels, frame_datetime)\n            cv2.imshow(\"Eyes Camera - Press ESC to quit\", bgr_pixels)\n            if cv2.waitKey(1) &amp; 0xFF == 27:\n                break\n    except KeyboardInterrupt:\n        pass\n    finally:\n        print(\"Stopping...\")\n        device.close()  # explicitly stop auto-update\n\n\ndef draw_time(frame, time):\n    frame_txt_font_name = cv2.FONT_HERSHEY_SIMPLEX\n    frame_txt_font_scale = 1.0\n    frame_txt_thickness = 1\n\n    # first line: frame index\n    frame_txt = str(time)\n\n    cv2.putText(\n        frame,\n        frame_txt,\n        (20, 50),\n        frame_txt_font_name,\n        frame_txt_font_scale,\n        (255, 255, 255),\n        thickness=frame_txt_thickness,\n        lineType=cv2.LINE_8,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"methods/simple/streaming/eye-cameras/#pupil_labs.realtime_api.simple.SimpleVideoFrame","title":"SimpleVideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A simplified video frame representation.</p> <p>This class provides a simplified representation of a video frame with BGR pixel data and timestamp information.</p> <p>Attributes:</p> <ul> <li> <code>bgr_pixels</code>               (<code>BGRBuffer</code>)           \u2013            <p>BGR pixel data as a NumPy array.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/eye-cameras/#pupil_labs.realtime_api.simple.SimpleVideoFrame.bgr_pixels","title":"bgr_pixels  <code>instance-attribute</code>","text":"<pre><code>bgr_pixels: BGRBuffer\n</code></pre> <p>BGR pixel data as a NumPy array.</p>"},{"location":"methods/simple/streaming/eye-cameras/#pupil_labs.realtime_api.simple.SimpleVideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/","title":"Blinks, fixations &amp; saccades","text":"<p>Neon +2.9.0 +1.5.0</p> <p>Using the <code>device.receive_eye_events</code> method, you can receive eye events such as blinks, saccades or fixations. The data returned is either an instance of:</p> <p>Warning</p> <p>Requires the \"Compute fixations\" setting to be enabled in the Companion Device.</p>"},{"location":"methods/simple/streaming/eye-events/#fixationeventdata","title":"<code>FixationEventData</code>","text":"<p>Defines a complete fixation or saccade event. The data returned are structured as follows, with event_type being either <code>0</code> for saccades or <code>1</code> for fixations.</p> SaccadeFixation <pre><code>FixationEventData(\n    event_type=0,\n    start_time_ns=1744625900502677306,\n    end_time_ns=1744625900562676306,\n    start_gaze_x=768.2272338867188,\n    start_gaze_y=685.6964721679688,\n    end_gaze_x=716.1095581054688,\n    end_gaze_y=493.5322570800781,\n    mean_gaze_x=747.7811279296875,\n    mean_gaze_y=597.7672119140625,\n    amplitude_pixels=199.10633850097656,\n    amplitude_angle_deg=12.716423988342285,\n    mean_velocity=3318.313232421875,\n    max_velocity=7444.6396484375,\n    rtp_ts_unix_seconds=1744626471.955861\n)\n</code></pre> <pre><code>FixationEventData(\n    event_type=1,\n    start_time_ns=1744625967695094306,\n    end_time_ns=1744625968135465306,\n    start_gaze_x=870.0199584960938,\n    start_gaze_y=311.0625,\n    end_gaze_x=730.7664794921875,\n    end_gaze_y=264.4870300292969,\n    mean_gaze_x=839.43115234375,\n    mean_gaze_y=280.5098876953125,\n    amplitude_pixels=146.83596801757812,\n    amplitude_angle_deg=9.18490982055664,\n    mean_velocity=272.82110595703125,\n    max_velocity=1415.25048828125,\n    rtp_ts_unix_seconds=1744626539.528702\n)\n</code></pre> FixationEventData"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData","title":"FixationEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade event.</p> <p>Represents a completed fixation or saccade event with detailed information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a FixationEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>amplitude_angle_deg</code>               (<code>float</code>)           \u2013            <p>Amplitude in degrees.</p> </li> <li> <code>amplitude_pixels</code>               (<code>float</code>)           \u2013            <p>Amplitude in pixels.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_gaze_x</code>               (<code>float</code>)           \u2013            <p>End gaze x-coordinate in pixels.</p> </li> <li> <code>end_gaze_y</code>               (<code>float</code>)           \u2013            <p>End gaze y-coordinate in pixels.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the event in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (0 for saccade, 1 for fixation).</p> </li> <li> <code>max_velocity</code>               (<code>float</code>)           \u2013            <p>Maximum velocity in pixels per degree.</p> </li> <li> <code>mean_gaze_x</code>               (<code>float</code>)           \u2013            <p>Mean gaze x-coordinate in pixels.</p> </li> <li> <code>mean_gaze_y</code>               (<code>float</code>)           \u2013            <p>Mean gaze y-coordinate in pixels.</p> </li> <li> <code>mean_velocity</code>               (<code>float</code>)           \u2013            <p>Mean velocity in pixels per degree.</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_gaze_x</code>               (<code>float</code>)           \u2013            <p>Start gaze x-coordinate in pixels.</p> </li> <li> <code>start_gaze_y</code>               (<code>float</code>)           \u2013            <p>Start gaze y-coordinate in pixels.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_angle_deg","title":"amplitude_angle_deg  <code>instance-attribute</code>","text":"<pre><code>amplitude_angle_deg: float\n</code></pre> <p>Amplitude in degrees.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.amplitude_pixels","title":"amplitude_pixels  <code>instance-attribute</code>","text":"<pre><code>amplitude_pixels: float\n</code></pre> <p>Amplitude in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_x","title":"end_gaze_x  <code>instance-attribute</code>","text":"<pre><code>end_gaze_x: float\n</code></pre> <p>End gaze x-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_gaze_y","title":"end_gaze_y  <code>instance-attribute</code>","text":"<pre><code>end_gaze_y: float\n</code></pre> <p>End gaze y-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the event in nanoseconds.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (0 for saccade, 1 for fixation).</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.max_velocity","title":"max_velocity  <code>instance-attribute</code>","text":"<pre><code>max_velocity: float\n</code></pre> <p>Maximum velocity in pixels per degree.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_x","title":"mean_gaze_x  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_x: float\n</code></pre> <p>Mean gaze x-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_gaze_y","title":"mean_gaze_y  <code>instance-attribute</code>","text":"<pre><code>mean_gaze_y: float\n</code></pre> <p>Mean gaze y-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.mean_velocity","title":"mean_velocity  <code>instance-attribute</code>","text":"<pre><code>mean_velocity: float\n</code></pre> <p>Mean velocity in pixels per degree.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_x","title":"start_gaze_x  <code>instance-attribute</code>","text":"<pre><code>start_gaze_x: float\n</code></pre> <p>Start gaze x-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_gaze_y","title":"start_gaze_y  <code>instance-attribute</code>","text":"<pre><code>start_gaze_y: float\n</code></pre> <p>Start gaze y-coordinate in pixels.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; FixationEventData\n</code></pre> <p>Create a FixationEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"FixationEventData\":\n    \"\"\"Create a FixationEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n    ) = struct.unpack(\"!iqqffffffffff\", data.raw)\n    return cls(\n        event_type,\n        start_time_ns,\n        end_time_ns,\n        start_gaze_x,\n        start_gaze_y,\n        end_gaze_x,\n        end_gaze_y,\n        mean_gaze_x,\n        mean_gaze_y,\n        amplitude_pixels,\n        amplitude_angle_deg,\n        mean_velocity,\n        max_velocity,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"methods/simple/streaming/eye-events/#fixationonseteventdata","title":"<code>FixationOnsetEventData</code>","text":"<p>This defines a fixation or saccade onset event. The data returned are structured as follows:</p> Saccade OnsetFixation Onset <pre><code>FixationOnsetEventData(event_type=2, start_time_ns=1744626187872119306, rtp_ts_unix_seconds=1744626759.2655792)\n</code></pre> <pre><code>FixationOnsetEventData(event_type=3, start_time_ns=1744626187872119306, rtp_ts_unix_seconds=1744626759.2655792)\n</code></pre> FixationOnsetEventData"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData","title":"FixationOnsetEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a fixation or saccade onset event.</p> <p>Represents the beginning of a fixation or saccade event.</p> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (2 for saccade onset, 3 for fixation onset).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the event in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (2 for saccade onset, 3 for fixation onset).</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the event in nanoseconds.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.FixationOnsetEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#blinkeventdata","title":"<code>BlinkEventData</code>","text":"<p>Finally, BlinkEventData determines a blink event.</p> <pre><code>BlinkEventData(\n    event_type=4,\n    start_time_ns=1744626029708811306,\n    end_time_ns=1744626029919061306,\n    rtp_ts_unix_seconds=1744626601.1020627\n)\n</code></pre> BlinkEventData"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData","title":"BlinkEventData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data for a blink event.</p> <p>Represents a detected blink event with timing information.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a BlinkEventData instance from raw RTSP data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>end_time_ns</code>               (<code>int</code>)           \u2013            <p>End time of the blink in nanoseconds.</p> </li> <li> <code>event_type</code>               (<code>int</code>)           \u2013            <p>Type of event (4 -&gt; blink events).</p> </li> <li> <code>rtp_ts_unix_seconds</code>               (<code>float</code>)           \u2013            <p>RTP timestamp in seconds since Unix epoch.</p> </li> <li> <code>start_time_ns</code>               (<code>int</code>)           \u2013            <p>Start time of the blink in nanoseconds.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since the Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.end_time_ns","title":"end_time_ns  <code>instance-attribute</code>","text":"<pre><code>end_time_ns: int\n</code></pre> <p>End time of the blink in nanoseconds.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.event_type","title":"event_type  <code>instance-attribute</code>","text":"<pre><code>event_type: int\n</code></pre> <p>Type of event (4 -&gt; blink events).</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.rtp_ts_unix_seconds","title":"rtp_ts_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>rtp_ts_unix_seconds: float\n</code></pre> <p>RTP timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.start_time_ns","title":"start_time_ns  <code>instance-attribute</code>","text":"<pre><code>start_time_ns: int\n</code></pre> <p>Start time of the blink in nanoseconds.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since the Unix epoch.</p>"},{"location":"methods/simple/streaming/eye-events/#pupil_labs.realtime_api.streaming.eye_events.BlinkEventData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; BlinkEventData\n</code></pre> <p>Create a BlinkEventData instance from raw RTSP data.</p> Source code in <code>src/pupil_labs/realtime_api/streaming/eye_events.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"BlinkEventData\":\n    \"\"\"Create a BlinkEventData instance from raw RTSP data.\"\"\"\n    (\n        event_type,\n        start_time_ns,\n        end_time_ns,\n    ) = struct.unpack(\"!iqq\", data.raw)\n    return cls(event_type, start_time_ns, end_time_ns, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"methods/simple/streaming/eye-events/#example","title":"Example","text":"<p>If you run the example you will get an output like this:</p> <pre><code>[FIXATION] event with duration of 3.93 seconds.\n[SACCADE] event with 43\u00b0 amplitude.\n[BLINK] blinked at 10:35:07 UTC\n</code></pre> Check the whole example code here stream_eye_events<pre><code>from datetime import datetime, timezone\n\nfrom pupil_labs.realtime_api.simple import discover_one_device\nfrom pupil_labs.realtime_api.streaming.eye_events import (\n    BlinkEventData,\n    FixationEventData,\n)\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# device.streaming_start()  # optional, if not called, stream is started on-demand\n\ntry:\n    while True:\n        eye_event = device.receive_eye_events()\n        if isinstance(eye_event, BlinkEventData):\n            time_sec = eye_event.start_time_ns // 1e9\n            blink_time = datetime.fromtimestamp(time_sec, timezone.utc)\n            print(f\"[BLINK] blinked at {blink_time.strftime('%H:%M:%S')} UTC\")\n\n        elif isinstance(eye_event, FixationEventData) and eye_event.event_type == 0:\n            angle = eye_event.amplitude_angle_deg\n            print(f\"[SACCADE] event with {angle:.0f}\u00b0 amplitude.\")\n\n        elif isinstance(eye_event, FixationEventData) and eye_event.event_type == 1:\n            duration = (eye_event.end_time_ns - eye_event.start_time_ns) / 1e9\n            print(f\"[FIXATION] event with duration of {duration:.2f} seconds.\")\n\n        # print(eye_event) # This will print all the fields of the eye event\n\nexcept KeyboardInterrupt:\n    pass\n\nfinally:\n    print(\"Stopping...\")\n    # device.streaming_stop()  # optional, if not called, stream is stopped on close\n    device.close()  # explicitly stop auto-update\n</code></pre>"},{"location":"methods/simple/streaming/gaze/","title":"Streaming Gaze Data","text":"<p>Use <code>device.receive_gaze_datum</code> to receive gaze data.</p> <pre><code>gaze = device.receive_gaze_datum()\n</code></pre> <p>This function can return different types of gaze data (GazeDataType) depending on the device and its configuration:</p> <ul> <li>GazeData object for Pupil Invisible or Neon without \"Compute eye state\" enabled.</li> <li>EyestateGazeData or EyestateEyelidGazeData for Neon with \"Compute eye state\" enabled, depending on the version of the Neon Companion app.</li> </ul> <p>See below samples for each type of gaze data.</p> Gaze dataWith Eye StateWith Eye State &amp; Eye Lid <pre><code>GazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre> <p>This method exposes gaze data</p> GazeData <p>Neon +2.8.8 +1.2.0</p> <p><pre><code>EyestateGazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    pupil_diameter_left=4.306737899780273,\n    eyeball_center_left_x=-29.3125,\n    eyeball_center_left_y=11.6875,\n    eyeball_center_left_z=-42.15625,\n    optical_axis_left_x=0.09871648252010345,\n    optical_axis_left_y=0.15512824058532715,\n    optical_axis_left_z=0.9829498529434204,\n    pupil_diameter_right=3.2171919345855713,\n    eyeball_center_right_x=33.21875,\n    eyeball_center_right_y=12.84375,\n    eyeball_center_right_z=-45.34375,\n    optical_axis_right_x=-0.20461124181747437,\n    optical_axis_right_y=0.1512681096792221,\n    optical_axis_right_z=0.9670844078063965,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre>  This method exposes gaze data, pupil diameter, and eye poses.</p> EyestateGazeData <p>Neon +2.9.0 +1.3.6</p> <p><pre><code>EyestateEyelidGazeData(\n    x=784.0623779296875,\n    y=537.4524536132812,\n    worn=False,\n    pupil_diameter_left=4.306737899780273,\n    eyeball_center_left_x=-29.3125,\n    eyeball_center_left_y=11.6875,\n    eyeball_center_left_z=-42.15625,\n    optical_axis_left_x=0.09871648252010345,\n    optical_axis_left_y=0.15512824058532715,\n    optical_axis_left_z=0.9829498529434204,\n    pupil_diameter_right=3.2171919345855713,\n    eyeball_center_right_x=33.21875,\n    eyeball_center_right_y=12.84375,\n    eyeball_center_right_z=-45.34375,\n    optical_axis_right_x=-0.20461124181747437,\n    optical_axis_right_y=0.1512681096792221,\n    optical_axis_right_z=0.9670844078063965,\n    eyelid_angle_top_left=-1.1484375,\n    eyelid_angle_bottom_left=-1.2763671875,\n    eyelid_aperture_left=1.6408717632293701,\n    eyelid_angle_top_right=-0.6259765625,\n    eyelid_angle_bottom_right=-1.2216796875,\n    eyelid_aperture_right=7.2039408683776855,\n    timestamp_unix_seconds=1744294828.3579288\n)\n</code></pre> This method exposes gaze data, pupil diameter, eye poses, and eye openness data.</p> EyestateEyelidGazeData <p>You can learn more about the payload in Under the Hood.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData","title":"GazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Basic gaze data with position, timestamp and indicator of glasses worn status.</p> <p>Represents the 2D gaze point on the scene camera coordinates with a timestamp in nanoseconds unix epoch and an indicator of whether the glasses are being worn.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create a GazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>\"X coordinate of the gaze point</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>\"X coordinate of the gaze point</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.GazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; GazeData\n</code></pre> <p>Create a GazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GazeData</code> (              <code>GazeData</code> )          \u2013            <p>An instance of GazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"GazeData\":\n    \"\"\"Create a GazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        GazeData: An instance of GazeData with the parsed values.\n\n    \"\"\"\n    x, y, worn = struct.unpack(\"!ffB\", data.raw)\n    return cls(x, y, worn == 255, data.timestamp_unix_seconds)\n</code></pre>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData","title":"EyestateGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Gaze data with additional eye state information.</p> <p>Contains gaze point, pupil diameter, eyeball center coordinates, and optical axis coordinates for both left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create an EyestateGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>eyeball_center_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the right eye.</p> </li> <li> <code>optical_axis_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the right eye.</p> </li> <li> <code>pupil_diameter_left</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the left eye.</p> </li> <li> <code>pupil_diameter_right</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the gaze point.</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_x","title":"eyeball_center_left_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_x: float\n</code></pre> <p>X coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_y","title":"eyeball_center_left_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_left_z","title":"eyeball_center_left_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_x","title":"eyeball_center_right_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_x: float\n</code></pre> <p>X coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_y","title":"eyeball_center_right_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.eyeball_center_right_z","title":"eyeball_center_right_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_x","title":"optical_axis_left_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_x: float\n</code></pre> <p>X coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_y","title":"optical_axis_left_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_y: float\n</code></pre> <p>Y coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_left_z","title":"optical_axis_left_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_z: float\n</code></pre> <p>Z coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_x","title":"optical_axis_right_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_x: float\n</code></pre> <p>X coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_y","title":"optical_axis_right_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_y: float\n</code></pre> <p>Y coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.optical_axis_right_z","title":"optical_axis_right_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_z: float\n</code></pre> <p>Z coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.pupil_diameter_left","title":"pupil_diameter_left  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left: float\n</code></pre> <p>Pupil diameter for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.pupil_diameter_right","title":"pupil_diameter_right  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right: float\n</code></pre> <p>Pupil diameter for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>X coordinate of the gaze point.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; EyestateGazeData\n</code></pre> <p>Create an EyestateGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EyestateGazeData</code> (              <code>EyestateGazeData</code> )          \u2013            <p>An instance of EyestateGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"EyestateGazeData\":\n    \"\"\"Create an EyestateGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        EyestateGazeData: An instance of EyestateGazeData with the parsed values.\n\n    \"\"\"\n    (\n        x,\n        y,\n        worn,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n    ) = struct.unpack(\"!ffBffffffffffffff\", data.raw)\n    return cls(\n        x,\n        y,\n        worn == 255,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData","title":"EyestateEyelidGazeData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Gaze data with additional eyelid state information.</p> <p>Contains gaze point, pupil diameter, eyeball center coordinates, optical axis coordinates, as well as eyelid angles and aperture for both left and right eyes.</p> <p>Methods:</p> <ul> <li> <code>from_raw</code>             \u2013              <p>Create an EyestateEyelidGazeData instance from raw data.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get the timestamp as a datetime object.</p> </li> <li> <code>eyeball_center_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the left eye.</p> </li> <li> <code>eyeball_center_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyeball_center_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the eyeball center for the right eye.</p> </li> <li> <code>eyelid_angle_bottom_left</code>               (<code>float</code>)           \u2013            <p>Angle of the bottom eyelid for the left eye(rad).</p> </li> <li> <code>eyelid_angle_bottom_right</code>               (<code>float</code>)           \u2013            <p>Angle of the bottom eyelid for the right eye (rad).</p> </li> <li> <code>eyelid_angle_top_left</code>               (<code>float</code>)           \u2013            <p>Angle of the top eyelid for the left eye(rad).</p> </li> <li> <code>eyelid_angle_top_right</code>               (<code>float</code>)           \u2013            <p>Angle of the top eyelid for the right eye (rad).</p> </li> <li> <code>eyelid_aperture_left</code>               (<code>float</code>)           \u2013            <p>Aperture of the eyelid for the left eye (mm).</p> </li> <li> <code>eyelid_aperture_right</code>               (<code>float</code>)           \u2013            <p>Aperture of the eyelid for the right eye (mm).</p> </li> <li> <code>optical_axis_left_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_left_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the left eye.</p> </li> <li> <code>optical_axis_right_x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the optical axis for the right eye.</p> </li> <li> <code>optical_axis_right_z</code>               (<code>float</code>)           \u2013            <p>Z coordinate of the optical axis for the right eye.</p> </li> <li> <code>pupil_diameter_left</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the left eye.</p> </li> <li> <code>pupil_diameter_right</code>               (<code>float</code>)           \u2013            <p>Pupil diameter for the right eye.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get the timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> <li> <code>worn</code>               (<code>bool</code>)           \u2013            <p>Whether the glasses are being worn.</p> </li> <li> <code>x</code>               (<code>float</code>)           \u2013            <p>X coordinate of the gaze point.</p> </li> <li> <code>y</code>               (<code>float</code>)           \u2013            <p>Y coordinate of the gaze point.</p> </li> </ul>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get the timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_x","title":"eyeball_center_left_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_x: float\n</code></pre> <p>X coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_y","title":"eyeball_center_left_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_left_z","title":"eyeball_center_left_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_left_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_x","title":"eyeball_center_right_x  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_x: float\n</code></pre> <p>X coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_y","title":"eyeball_center_right_y  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_y: float\n</code></pre> <p>Y coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyeball_center_right_z","title":"eyeball_center_right_z  <code>instance-attribute</code>","text":"<pre><code>eyeball_center_right_z: float\n</code></pre> <p>Z coordinate of the eyeball center for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_bottom_left","title":"eyelid_angle_bottom_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_bottom_left: float\n</code></pre> <p>Angle of the bottom eyelid for the left eye(rad).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_bottom_right","title":"eyelid_angle_bottom_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_bottom_right: float\n</code></pre> <p>Angle of the bottom eyelid for the right eye (rad).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_top_left","title":"eyelid_angle_top_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_top_left: float\n</code></pre> <p>Angle of the top eyelid for the left eye(rad).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_angle_top_right","title":"eyelid_angle_top_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_angle_top_right: float\n</code></pre> <p>Angle of the top eyelid for the right eye (rad).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_aperture_left","title":"eyelid_aperture_left  <code>instance-attribute</code>","text":"<pre><code>eyelid_aperture_left: float\n</code></pre> <p>Aperture of the eyelid for the left eye (mm).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.eyelid_aperture_right","title":"eyelid_aperture_right  <code>instance-attribute</code>","text":"<pre><code>eyelid_aperture_right: float\n</code></pre> <p>Aperture of the eyelid for the right eye (mm).</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_x","title":"optical_axis_left_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_x: float\n</code></pre> <p>X coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_y","title":"optical_axis_left_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_y: float\n</code></pre> <p>Y coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_left_z","title":"optical_axis_left_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_left_z: float\n</code></pre> <p>Z coordinate of the optical axis for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_x","title":"optical_axis_right_x  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_x: float\n</code></pre> <p>X coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_y","title":"optical_axis_right_y  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_y: float\n</code></pre> <p>Y coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.optical_axis_right_z","title":"optical_axis_right_z  <code>instance-attribute</code>","text":"<pre><code>optical_axis_right_z: float\n</code></pre> <p>Z coordinate of the optical axis for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.pupil_diameter_left","title":"pupil_diameter_left  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_left: float\n</code></pre> <p>Pupil diameter for the left eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.pupil_diameter_right","title":"pupil_diameter_right  <code>instance-attribute</code>","text":"<pre><code>pupil_diameter_right: float\n</code></pre> <p>Pupil diameter for the right eye.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get the timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.worn","title":"worn  <code>instance-attribute</code>","text":"<pre><code>worn: bool\n</code></pre> <p>Whether the glasses are being worn.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: float\n</code></pre> <p>X coordinate of the gaze point.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: float\n</code></pre> <p>Y coordinate of the gaze point.</p>"},{"location":"methods/simple/streaming/gaze/#pupil_labs.realtime_api.streaming.gaze.EyestateEyelidGazeData.from_raw","title":"from_raw  <code>classmethod</code>","text":"<pre><code>from_raw(data: RTSPData) -&gt; EyestateEyelidGazeData\n</code></pre> <p>Create an EyestateEyelidGazeData instance from raw data.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>RTSPData</code>)           \u2013            <p>The raw data received from the RTSP stream.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EyestateEyelidGazeData</code> (              <code>EyestateEyelidGazeData</code> )          \u2013            <p>An instance of EyestateEyelidGazeData with the parsed values.</p> </li> </ul> Source code in <code>src/pupil_labs/realtime_api/streaming/gaze.py</code> <pre><code>@classmethod\ndef from_raw(cls, data: RTSPData) -&gt; \"EyestateEyelidGazeData\":\n    \"\"\"Create an EyestateEyelidGazeData instance from raw data.\n\n    Args:\n        data (RTSPData): The raw data received from the RTSP stream.\n\n    Returns:\n        EyestateEyelidGazeData: An instance of EyestateEyelidGazeData with the\n            parsed values.\n\n    \"\"\"\n    (\n        x,\n        y,\n        worn,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        eyelid_angle_top_left,\n        eyelid_angle_bottom_left,\n        eyelid_aperture_left,\n        eyelid_angle_top_right,\n        eyelid_angle_bottom_right,\n        eyelid_aperture_right,\n    ) = struct.unpack(\"!ffBffffffffffffffffffff\", data.raw)\n    return cls(\n        x,\n        y,\n        worn == 255,\n        pupil_diameter_left,\n        eyeball_center_left_x,\n        eyeball_center_left_y,\n        eyeball_center_left_z,\n        optical_axis_left_x,\n        optical_axis_left_y,\n        optical_axis_left_z,\n        pupil_diam_right,\n        eyeball_center_right_x,\n        eyeball_center_right_y,\n        eyeball_center_right_z,\n        optical_axis_right_x,\n        optical_axis_right_y,\n        optical_axis_right_z,\n        eyelid_angle_top_left,\n        eyelid_angle_bottom_left,\n        eyelid_aperture_left,\n        eyelid_angle_top_right,\n        eyelid_angle_bottom_right,\n        eyelid_aperture_right,\n        data.timestamp_unix_seconds,\n    )\n</code></pre>"},{"location":"methods/simple/streaming/gaze/#full-code-examples","title":"Full Code Examples","text":"Check the whole example code here stream_gaze.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# device.streaming_start()  # optional, if not called, stream is started on-demand\n\ntry:\n    while True:\n        print(device.receive_gaze_datum())\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print(\"Stopping...\")\n    # device.streaming_stop()  # optional, if not called, stream is stopped on close\n    device.close()  # explicitly stop auto-update\n</code></pre>"},{"location":"methods/simple/streaming/imu-data/","title":"Stream IMU Data","text":"<p>Neon +1.1.2</p> <p>Data generated by the IMU can be received using the <code>device.receive_imu_datum</code> method. It returns a UTC timestamp in seconds, the head pose as a quaternion, gyro data, and accelerometer data as follows.</p> <pre><code>IMUData(\n    gyro_data=Data3D(x=-0.1659393310546875, y=-0.1964569091796875, z=0.1735687255859375),\n    accel_data=Data3D(x=-0.02880859375, y=-0.224609375, z=1.0068359375),\n    quaternion=Quaternion(x=-0.06114135682582855, y=-0.090116947889328, z=0.8700147271156311, w=0.48084819316864014),\n    timestamp_unix_seconds=1744297102.1941311\n)\n</code></pre> IMUData Check the whole example code here stream_imu.py<pre><code>from pupil_labs.realtime_api.simple import discover_one_device\n\n# Look for devices. Returns as soon as it has found the first device.\nprint(\"Looking for the next best device...\")\ndevice = discover_one_device(max_search_duration_seconds=10)\nif device is None:\n    print(\"No device found.\")\n    raise SystemExit(-1)\n\n# device.streaming_start()  # optional, if not called, stream is started on-demand\ntry:\n    while True:\n        print(device.receive_imu_datum())\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print(\"Stopping...\")\n    # device.streaming_stop()  # optional, if not called, stream is stopped on close\n    device.close()  # explicitly stop auto-update\n</code></pre>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData","title":"IMUData","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data from the Inertial Measurement Unit (IMU).</p> <p>Contains gyroscope, accelerometer, and rotation data from the IMU sensor.</p> <p>Attributes:</p> <ul> <li> <code>accel_data</code>               (<code>Data3D</code>)           \u2013            <p>Accelerometer data in m/s\u00b2.</p> </li> <li> <code>datetime</code>               (<code>datetime</code>)           \u2013            <p>Get timestamp as a datetime object.</p> </li> <li> <code>gyro_data</code>               (<code>Data3D</code>)           \u2013            <p>Gyroscope data in deg/s.</p> </li> <li> <code>quaternion</code>               (<code>Quaternion</code>)           \u2013            <p>Rotation represented as a quaternion.</p> </li> <li> <code>timestamp_unix_nanoseconds</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_ns</code>               (<code>int</code>)           \u2013            <p>Get timestamp in nanoseconds since Unix epoch.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.accel_data","title":"accel_data  <code>instance-attribute</code>","text":"<pre><code>accel_data: Data3D\n</code></pre> <p>Accelerometer data in m/s\u00b2.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.datetime","title":"datetime  <code>property</code>","text":"<pre><code>datetime: datetime\n</code></pre> <p>Get timestamp as a datetime object.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.gyro_data","title":"gyro_data  <code>instance-attribute</code>","text":"<pre><code>gyro_data: Data3D\n</code></pre> <p>Gyroscope data in deg/s.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.quaternion","title":"quaternion  <code>instance-attribute</code>","text":"<pre><code>quaternion: Quaternion\n</code></pre> <p>Rotation represented as a quaternion.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_nanoseconds","title":"timestamp_unix_nanoseconds  <code>property</code>","text":"<pre><code>timestamp_unix_nanoseconds: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p> Deprecated <p>This class property is deprecated and will be removed in future versions. Use timestamp_unix_ns() instead.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_ns","title":"timestamp_unix_ns  <code>property</code>","text":"<pre><code>timestamp_unix_ns: int\n</code></pre> <p>Get timestamp in nanoseconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/imu-data/#pupil_labs.realtime_api.streaming.imu.IMUData.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/scene-camera/","title":"Scene Camera","text":""},{"location":"methods/simple/streaming/scene-camera/#scene-camera-video","title":"Scene Camera Video","text":"<p>It's very easy to stream the scene camera feed with timestamps for real-time monitoring. Simply use the <code>device.receive_scene_video_frame</code> method.</p> <pre><code>bgr_pixels, frame_datetime = device.receive_scene_video_frame()\n</code></pre> SimpleVideoFrame Check the whole example code here stream_scene_camera_video.py<pre><code>import cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api.simple import discover_one_device  # noqa: E402\n\n\ndef main():\n    # Look for devices. Returns as soon as it has found the first device.\n    print(\"Looking for the next best device...\")\n    device = discover_one_device(max_search_duration_seconds=10)\n    if device is None:\n        print(\"No device found.\")\n        raise SystemExit(-1)\n\n    print(f\"Connecting to {device}...\")\n\n    try:\n        while True:\n            bgr_pixels, frame_datetime = device.receive_scene_video_frame()\n            draw_time(bgr_pixels, frame_datetime)\n            cv2.imshow(\"Scene Camera - Press ESC to quit\", bgr_pixels)\n            if cv2.waitKey(1) &amp; 0xFF == 27:\n                break\n    except KeyboardInterrupt:\n        pass\n    finally:\n        print(\"Stopping...\")\n        device.close()  # explicitly stop auto-update\n\n\ndef draw_time(frame, time):\n    frame_txt_font_name = cv2.FONT_HERSHEY_SIMPLEX\n    frame_txt_font_scale = 1.0\n    frame_txt_thickness = 1\n\n    # first line: frame index\n    frame_txt = str(time)\n\n    cv2.putText(\n        frame,\n        frame_txt,\n        (20, 50),\n        frame_txt_font_name,\n        frame_txt_font_scale,\n        (255, 255, 255),\n        thickness=frame_txt_thickness,\n        lineType=cv2.LINE_8,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.SimpleVideoFrame","title":"SimpleVideoFrame","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A simplified video frame representation.</p> <p>This class provides a simplified representation of a video frame with BGR pixel data and timestamp information.</p> <p>Attributes:</p> <ul> <li> <code>bgr_pixels</code>               (<code>BGRBuffer</code>)           \u2013            <p>BGR pixel data as a NumPy array.</p> </li> <li> <code>timestamp_unix_seconds</code>               (<code>float</code>)           \u2013            <p>Timestamp in seconds since Unix epoch.</p> </li> </ul>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.SimpleVideoFrame.bgr_pixels","title":"bgr_pixels  <code>instance-attribute</code>","text":"<pre><code>bgr_pixels: BGRBuffer\n</code></pre> <p>BGR pixel data as a NumPy array.</p>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.SimpleVideoFrame.timestamp_unix_seconds","title":"timestamp_unix_seconds  <code>instance-attribute</code>","text":"<pre><code>timestamp_unix_seconds: float\n</code></pre> <p>Timestamp in seconds since Unix epoch.</p>"},{"location":"methods/simple/streaming/scene-camera/#scene-camera-video-with-overlaid-gaze","title":"Scene Camera Video with Overlaid Gaze","text":"<p>For additional context about where the wearer was gazing, it's useful to overlay gaze measurements onto the scene camera video stream. Since the scene camera and gaze signal can have different sampling rates, we need to be sure they are matched. For that, you can use (<code>device.receive_matched_scene_video_frame_and_gaze</code>). This receives a pair of scene camera video and gaze data already matched.</p> <pre><code>frame, gaze = device.receive_matched_scene_video_frame_and_gaze()\n</code></pre> MatchedItem Check the whole example code here stream_video_with_overlayed_gaze.py<pre><code>import cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api.simple import discover_one_device  # noqa: E402\n\n\ndef main():\n    # Look for devices. Returns as soon as it has found the first device.\n    print(\"Looking for the next best device...\")\n    device = discover_one_device(max_search_duration_seconds=10)\n    if device is None:\n        print(\"No device found.\")\n        raise SystemExit(-1)\n\n    print(f\"Connecting to {device}...\")\n\n    try:\n        while True:\n            frame, gaze = device.receive_matched_scene_video_frame_and_gaze()\n            cv2.circle(\n                frame.bgr_pixels,\n                (int(gaze.x), int(gaze.y)),\n                radius=80,\n                color=(0, 0, 255),\n                thickness=15,\n            )\n\n            cv2.imshow(\"Scene camera with gaze overlay\", frame.bgr_pixels)\n            if cv2.waitKey(1) &amp; 0xFF == 27:\n                break\n    except KeyboardInterrupt:\n        pass\n    finally:\n        print(\"Stopping...\")\n        device.close()  # explicitly stop auto-update\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedItem","title":"MatchedItem","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A matched pair of scene video frame and gaze data.</p> <p>This class represents a scene video frame and gaze data point that occurred at approximately the same time.</p> Note <p>The name MatchedItem is maintained for backward compatibility. It represents a matched pair of scene video frame and gaze data.</p> <p>Attributes:</p> <ul> <li> <code>frame</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Scene video frame.</p> </li> <li> <code>gaze</code>               (<code>GazeDataType</code>)           \u2013            <p>Corresponding gaze data.</p> </li> </ul>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedItem.frame","title":"frame  <code>instance-attribute</code>","text":"<pre><code>frame: SimpleVideoFrame\n</code></pre> <p>Scene video frame.</p>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedItem.gaze","title":"gaze  <code>instance-attribute</code>","text":"<pre><code>gaze: GazeDataType\n</code></pre> <p>Corresponding gaze data.</p>"},{"location":"methods/simple/streaming/scene-camera/#scene-camera-video-with-overlayed-eyes-video-and-gaze-circle","title":"Scene Camera Video with Overlayed Eyes Video and Gaze Circle","text":"<p>Neon</p> <p>It might also be useful to overlay the eye camera video frames, e.g. if you want to manually inspect the eye data or blinking behaviour. This can be achieved using the <code>device.receive_matched_scene_and_eyes_video_frames_and_gaze</code> method.</p> <pre><code>matched = device.receive_matched_scene_and_eyes_video_frames_and_gaze()\n</code></pre> MatchedGazeEyesSceneItem Check the whole example code here stream_scene_eyes_and_gaze.py<pre><code>import cv2\nimport numpy as np\n\n# Workaround for https://github.com/opencv/opencv/issues/21952\ncv2.imshow(\"cv/av bug\", np.zeros(1))\ncv2.destroyAllWindows()\n\nfrom pupil_labs.realtime_api.simple import discover_one_device  # noqa: E402\n\n\ndef main():\n    # Look for devices. Returns as soon as it has found the first device.\n    print(\"Looking for the next best device...\")\n    device = discover_one_device(max_search_duration_seconds=10)\n    if device is None:\n        print(\"No device found.\")\n        raise SystemExit(-1)\n\n    print(f\"Connecting to {device}...\")\n\n    try:\n        while True:\n            matched = device.receive_matched_scene_and_eyes_video_frames_and_gaze()\n            if not matched:\n                print(\n                    \"Not able to find a match! Note: Pupil Invisible does not support \"\n                    \"streaming eyes video\"\n                )\n                continue\n\n            cv2.circle(\n                matched.scene.bgr_pixels,\n                (int(matched.gaze.x), int(matched.gaze.y)),\n                radius=80,\n                color=(0, 0, 255),\n                thickness=15,\n            )\n\n            # Render eyes video into the scene video\n            height, width, _ = matched.eyes.bgr_pixels.shape\n            matched.scene.bgr_pixels[:height, :width, :] = matched.eyes.bgr_pixels\n\n            cv2.imshow(\n                \"Scene camera with eyes and gaze overlay\", matched.scene.bgr_pixels\n            )\n            if cv2.waitKey(1) &amp; 0xFF == 27:\n                break\n    except KeyboardInterrupt:\n        pass\n    finally:\n        print(\"Stopping...\")\n        device.close()  # explicitly stop auto-update\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem","title":"MatchedGazeEyesSceneItem","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A matched triplet of scene video frame, eye video frame, and gaze data.</p> <p>This class represents scene and eye video frames along with gaze data that occurred at approximately the same time.</p> <p>Attributes:</p> <ul> <li> <code>eyes</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Eye camera video frame.</p> </li> <li> <code>gaze</code>               (<code>GazeDataType</code>)           \u2013            <p>Corresponding gaze data.</p> </li> <li> <code>scene</code>               (<code>SimpleVideoFrame</code>)           \u2013            <p>Scene video frame.</p> </li> </ul>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.eyes","title":"eyes  <code>instance-attribute</code>","text":"<pre><code>eyes: SimpleVideoFrame\n</code></pre> <p>Eye camera video frame.</p>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.gaze","title":"gaze  <code>instance-attribute</code>","text":"<pre><code>gaze: GazeDataType\n</code></pre> <p>Corresponding gaze data.</p>"},{"location":"methods/simple/streaming/scene-camera/#pupil_labs.realtime_api.simple.MatchedGazeEyesSceneItem.scene","title":"scene  <code>instance-attribute</code>","text":"<pre><code>scene: SimpleVideoFrame\n</code></pre> <p>Scene video frame.</p>"},{"location":"coverage/","title":"Coverage","text":""}]}